{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c56feca-f3f1-4836-b6a2-e029635a2c92",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d631450-d119-445a-bad1-0b6ad00e51d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import built-in Python libs\n",
    "import pickle\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "# Import data science libs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import deep learning libs\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Import weights & bias\n",
    "import wandb\n",
    "\n",
    "# Import data preprocessing libs\n",
    "from tokenizers import Tokenizer, decoders, pre_tokenizers\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.normalizers import NFKC\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0336920f-8cbe-480f-8321-d8bc7a5b8e7f",
   "metadata": {},
   "source": [
    "# Download Datasets, Tokenizers, DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fa478c9-58c0-4f9e-9d35-5923b0016715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwindsuzu\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.27<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">rnn_attention</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/windsuzu/phonetic-translation\" target=\"_blank\">https://wandb.ai/windsuzu/phonetic-translation</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/windsuzu/phonetic-translation/runs/30i663cw\" target=\"_blank\">https://wandb.ai/windsuzu/phonetic-translation/runs/30i663cw</a><br/>\n",
       "                Run data is saved locally in <code>/home/ai2019/ne6081022/project/phonetics-in-chinese-japanese-machine-translation/experiments/main/wandb/run-20210427_085121-30i663cw</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project=\"phonetic-translation\",\n",
    "    entity=\"windsuzu\",\n",
    "    group=\"experiments\",\n",
    "    name=\"rnn_attention\",\n",
    "    job_type=\"baseline\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ffe06b-484a-45ee-b7ae-9b41ce130e3e",
   "metadata": {},
   "source": [
    "## Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c12b40e-53c2-42ee-b5af-11d3c0548530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact train:latest, 205.64MB. 2 files... Done. 0:0:0\n"
     ]
    }
   ],
   "source": [
    "train_data_art = run.use_artifact(\"train:latest\")\n",
    "train_data_dir = train_data_art.download()\n",
    "\n",
    "dev_data_art = run.use_artifact(\"dev:latest\")\n",
    "dev_data_dir = dev_data_art.download()\n",
    "\n",
    "test_data_art = run.use_artifact(\"test:latest\")\n",
    "test_data_dir = test_data_art.download()\n",
    "\n",
    "data_dir = {\n",
    "    \"train\": train_data_dir,\n",
    "    \"dev\": dev_data_dir,\n",
    "    \"test\": test_data_dir,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713a1bde-d82e-40c4-bbbc-8a736cbd6503",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9331de4f-da3e-44a3-a3dd-641f79e2b401",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencepiece_tokenizer_art = run.use_artifact(\"sentencepiece:latest\")\n",
    "sentencepiece_tokenizer_dir = sentencepiece_tokenizer_art.download()\n",
    "ch_tokenizer_dir = Path(sentencepiece_tokenizer_dir) / \"ch_tokenizer.json\"\n",
    "jp_tokenizer_dir = Path(sentencepiece_tokenizer_dir) / \"jp_tokenizer.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f61436-b58e-4db8-b9a2-7eec275d5a21",
   "metadata": {},
   "source": [
    "## DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9261e259-5232-4d71-b440-6bd04a9da803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SentencePieceDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir,\n",
    "        src_tokenizer_dir,\n",
    "        trg_tokenizer_dir,\n",
    "        batch_size=128,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.src_tokenizer_dir = src_tokenizer_dir\n",
    "        self.trg_tokenizer_dir = trg_tokenizer_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.src_tokenizer = self._load_tokenizer(self.src_tokenizer_dir)\n",
    "        self.trg_tokenizer = self._load_tokenizer(self.trg_tokenizer_dir)\n",
    "\n",
    "        if stage == \"fit\":\n",
    "            self.train_set = self._data_preprocess(self.data_dir[\"train\"])\n",
    "            self.val_set = self._data_preprocess(self.data_dir[\"dev\"])\n",
    "\n",
    "        if stage == \"test\":\n",
    "            self.test_set = self._data_preprocess(self.data_dir[\"test\"])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_set,\n",
    "            self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=self._data_batching_fn,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_set,\n",
    "            self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=self._data_batching_fn,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_set,\n",
    "            self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=self._data_batching_fn,\n",
    "        )\n",
    "\n",
    "    def _read_data_array(self, data_dir):\n",
    "        with open(data_dir, encoding=\"utf8\") as f:\n",
    "            arr = f.readlines()\n",
    "        return arr\n",
    "\n",
    "    def _load_tokenizer(self, tokenizer_dir):\n",
    "        return Tokenizer.from_file(str(tokenizer_dir))\n",
    "\n",
    "    def _data_preprocess(self, data_dir):\n",
    "        src_txt = self._read_data_array(Path(data_dir) / \"ch.txt\")\n",
    "        trg_txt = self._read_data_array(Path(data_dir) / \"jp.txt\")\n",
    "        parallel_txt = np.array(list(zip(src_txt, trg_txt)))\n",
    "        return parallel_txt\n",
    "\n",
    "    def _data_batching_fn(self, data_batch):\n",
    "        data_batch = np.array(data_batch)  # shape=(batch_size, 2=src+trg)\n",
    "\n",
    "        src_batch = data_batch[:, 0]  # shape=(batch_size, )\n",
    "        trg_batch = data_batch[:, 1]  # shape=(batch_size, )\n",
    "        \n",
    "        # src_batch=(batch_size, longest_sentence)\n",
    "        # trg_batch=(batch_size, longest_sentence)\n",
    "        src_batch = self.src_tokenizer.encode_batch(src_batch)  \n",
    "        trg_batch = self.trg_tokenizer.encode_batch(trg_batch)\n",
    "\n",
    "        # We have to sort the batch by their non-padded lengths in descending order,\n",
    "        # because the descending order can help in `nn.utils.rnn.pack_padded_sequence()`,\n",
    "        # which it will help us ignoring the <pad> in training rnn.\n",
    "        # https://meetonfriday.com/posts/4d6a906a\n",
    "        src_batch, trg_batch = zip(\n",
    "            *sorted(\n",
    "                zip(src_batch, trg_batch),\n",
    "                key=lambda x: sum(x[0].attention_mask),\n",
    "                reverse=True,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return src_batch, trg_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d7d4ac4-bfd6-486e-8b0b-faeb49fa4f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencepiece_dm_art = run.use_artifact(\"sentencepiece_dm:latest\")\n",
    "sentencepiece_dm_dir = sentencepiece_dm_art.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afa8036a-ca16-4ce3-b6b8-a9e4522d18dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dm = SentencePieceDataModule(data_dir, ch_tokenizer_dir, jp_tokenizer_dir, 128)\n",
    "with open(Path(sentencepiece_dm_dir) / \"sentencepiece_dm.pkl\", \"rb\") as f:\n",
    "    dm = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce56a9fb-f0c4-4251-b732-b730903391cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.batch_size = 16\n",
    "dm.num_workers = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c617a9-9850-41d4-be65-b3a29fb199d1",
   "metadata": {},
   "source": [
    "### Test DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55802c0c-afb3-4adc-8f65-d86c1f239074",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dm.setup(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad0b6f57-2e4e-426d-a70c-308bfaf21921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000 32000\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "input_dim = dm.src_tokenizer.get_vocab_size()\n",
    "output_dim = dm.trg_tokenizer.get_vocab_size()\n",
    "print(input_dim, output_dim)\n",
    "\n",
    "src_pad_idx = dm.src_tokenizer.token_to_id(\"[PAD]\")\n",
    "print(src_pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5284cdb6-cbdb-462b-bf59-f961677ba687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 Encoding(num_tokens=71, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "16 Encoding(num_tokens=64, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "for src, trg in dm.test_dataloader():\n",
    "    print(len(src), src[0])\n",
    "    print(len(trg), trg[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a82584-c3f8-4247-9154-1560c8c7a088",
   "metadata": {},
   "source": [
    "# Build Lightning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b14a37f-d343-4e7e-808b-84d29b3bc4b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Encoder\n",
    "\n",
    "![](../assets/bi_encoder.png)\n",
    "\n",
    "首先我們先輸入 embeded 過後的字來計算正向和反向的 hidden state:\n",
    "\n",
    "$$\n",
    "h_\\overrightarrow{t} = \\overrightarrow{\\text{EncoderGRU}}(\\text{emb}(x_\\overrightarrow{t}), h_\\overrightarrow{t-1})\n",
    "\\\\\n",
    "h_\\overleftarrow{t} = \\overleftarrow{\\text{EncoderGRU}}(\\text{emb}(x_\\overleftarrow{t}), h_\\overleftarrow{t-1})\n",
    "$$\n",
    "\n",
    "得到的 `outputs` 代表所有最後一層的 hidden states 的組合，我們會用 outputs 來計算 attention，也就是翻譯時要注意原句的哪些單字:\n",
    "\n",
    "$$\n",
    "h_1 = [h_\\overrightarrow{1}; h_\\overleftarrow{1}], h_2 = [h_\\overrightarrow{2}; h_\\overleftarrow{2}], \\\\\n",
    "\\text{outputs} = H = \\left\\{h_1, h_2, \\cdots, h_T\\right\\}\n",
    "$$\n",
    "\n",
    "得到的 `hidden` 代表每一層最後一個時間點的 hidden states 的疊加，我們會用 hidden 做為 decoder 初始的 context vector `s0`:\n",
    "\n",
    "$$\n",
    "\\overrightarrow{z} = h_\\overrightarrow{T} \\\\\n",
    "\\overleftarrow{z} = h_\\overleftarrow{T}\n",
    "$$\n",
    "\n",
    "因為 decoder 不是雙向，所以我們把 hidden 丟進一個 linear `g` 和 `tanh` 裡獲得濃縮後的 context vector `z`:\n",
    "\n",
    "$$\n",
    "z = \\tanh(g(\\text{cat}(\\overrightarrow{z}, \\overleftarrow{z}))) = s_0\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Inputs\n",
    "\n",
    "| variables   | use                                             | note                                              |\n",
    "| ----------- | ----------------------------------------------- | ------------------------------------------------- |\n",
    "| src         | 初始語言資料                                    | `shape=[batch_size, src_len]` (batch-first shape) |\n",
    "| src_len     | batch 中每個句子的真實長度                      | `shape=[batch_size]`                              |\n",
    "| input_dim   | 初始語言的 vocab_size                           | `src_tokenizer.get_vocab_size()`                  |\n",
    "| emb_dim     | embedding_size                                  ||\n",
    "| enc_hid_dim | Encoder 中 rnn 的 hidden_size                   ||\n",
    "| dec_hid_dim | Decoder 中 rnn 的 hidden_size                   ||\n",
    "| rnn         | 雙向 GRU, 吃 batch-first 的資料                 | `nn.GRU(emb_dim, enc_hid_dim, bidirectional = True, batch_first=True)`|\n",
    "| fc          | 將雙向 context vector 輸出成單個 context vector | `nn.Linear(enc_hid_dim * 2, dec_hid_dim)`|\n",
    "\n",
    "### Outputs\n",
    "\n",
    "| variables        | use                                            | note                                     |\n",
    "| ---------------- | ---------------------------------------------- | ---------------------------------------- |\n",
    "| packed_embedded  | 將 `[PAD]` 刪掉包裝成 packed 格式              | `PackedSequence`                         |\n",
    "| packed_outputs   | 沒有 `[PAD]` 的最後一層 hidden_states          | `PackedSequence`                         |\n",
    "| enc_outputs      | 有 `[PAD]` 的最後一層 hidden_states            | `shape=[batch_size, src_len, enc_hid_dim*2]` |\n",
    "| hidden           | 所有 layer 疊加的 context_vector               | `shape=[layer*2, batch_size, enc_hid_dim]`   |\n",
    "| hidden[:2, :, :] | 最上面 forward layer 的 hidden_state           | `shape=[batch_size, enc_hid_dim]`            |\n",
    "| hidden[:1, :, :] | 最上面 backward layer 的 hidden_state          | `shape=[batch_size, enc_hid_dim]`            |\n",
    "| last_hidden      | 透過 torch.cat 組合最後一層 forward + backward | `shape=[batch_size, enc_hid_dim*2]`          |\n",
    "| dec_hidden       | 經過 tanh + fc 得到的 context_vector           | `shape=[batch_size, dec_hid_dim]`\n",
    "\n",
    "> - **Terminology Alert**😪:\n",
    ">   - **outputs** 可以想成所有時間點的最後一層的 hidden_states 所組成\n",
    ">   - **hidden** 可以想成所有 layer (forward + backward) 在最後時間點的 hidden_states 堆疊而成的 context_vector\n",
    "\n",
    "問題一: 什麼是 packed_sequence?\n",
    "> - [[Pytorch]Pack the data to train variable length sequences](https://meetonfriday.com/posts/4d6a906a)\n",
    "\n",
    "問題二: outputs 和 hidden 差在哪?\n",
    "> - [学会区分 RNN 的 output 和 state](https://zhuanlan.zhihu.com/p/28919765)\n",
    "> - [LSTM/GRU中output和hidden的区别//其他问题](https://blog.csdn.net/yagreenhand/article/details/84893493)\n",
    "\n",
    "<img src=\"../assets/output_vs_hidden.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbdf3cb7-3b7c-46d4-a57e-620868022575",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_len):\n",
    "        # src     = [batch_size, src_len]\n",
    "        # src_len = [batch_size]\n",
    "\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded = [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to(\"cpu\"), batch_first=True)\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "        # packed_outputs is a packed sequence containing all hidden states\n",
    "        # hidden is now from the final non-padded element in the batch\n",
    "\n",
    "        enc_outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n",
    "        # enc_outputs is now a non-packed sequence\n",
    "\n",
    "        # enc_outputs = [batch_size, src_len, enc_hid_dim*num_directions]\n",
    "        #             = [forward_n + backward_n]\n",
    "        #             = [last layer]\n",
    "\n",
    "        # hidden  = [n_layers*num_directions, batch_size, enc_hid_dim]\n",
    "        #         = [forward_1, backward_1, forward_2, backword_2, ...]\n",
    "\n",
    "        # hidden[-2, :, : ] is the last of the forwards RNN\n",
    "        # hidden[-1, :, : ] is the last of the backwards RNN\n",
    "\n",
    "        last_hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        init_dec_hidden = torch.tanh(self.fc(last_hidden))\n",
    "\n",
    "        # enc_outputs     = [batch_size, src_len, enc_hid_dim*2]  (we only have 1 layer)\n",
    "        # init_dec_hidden = [batch_size, dec_hid_dim]\n",
    "\n",
    "        return enc_outputs, init_dec_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f688f0a0-d19b-4016-8df1-bc5cb1dc9786",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Attention\n",
    "\n",
    "![](../assets/seq2seq_encoder_attention.png)\n",
    "\n",
    "先說結論，每次 attention layer 會產生一個 `src_len` 長度的陣列，代表在預測下一個字 $\\hat{y}_{t+1}$ 的時間點時，對原句 `src` 中每一個 token 的專注度有多高。\n",
    "\n",
    "🤯 每次需要給 attention layer 什麼?\n",
    "\n",
    "1. decoder 前一個時間點的 hidden state $s_{t-1}$ (i.e., 第一個就是 encoder 的 `hidden` $z$ 也就是 $s_0$)\n",
    "2. encoder 的 `outputs` $H$\n",
    "\n",
    "而 attention layer 其實只是一個 linear layer，用來和 `tanh` 一起計算出一個能量值 $E_t$:\n",
    "\n",
    "$$\n",
    "E_t = \\tanh(\\text{attn}(s_{t-1}, H))\n",
    "$$\n",
    "\n",
    "因為 `enc_outputs` 的長度是 `src_len`，而 `hidden` 只是一個 scalar，所以我們必須把 `hidden` 拉到跟 `enc_outputs` 一樣長。 計算出來的 $E_t$ 可以想像成 `encoder_outputs` $H$ 和 `previous_decoder_hidden_state` $s_{t-1}$ 有多匹配。\n",
    "\n",
    "因為算出來的能量值 $E_t$ 形狀是 `[src_len, hid_dim]`，我們可以把他帶入一個形狀是 `[hid_dim, 1]` linear layer $v$。最終得到一個形狀是 `[src_len]` 的 `attention_sequence`:\n",
    "\n",
    "$$\n",
    "\\hat{a}_t = v(E_t)\n",
    "$$\n",
    "\n",
    "你可以想像 $v$ 裡面學習到的參數是一個權重，告訴我們能量值 $E_t$ 作用在 `encoder_outputs` 中每個 token 的權重有多少。\n",
    "\n",
    "最後的最後， attention_sequence 會通過 softmax 讓所有機率加總為 1，其中會把 `attention_sequence` 和 `mask` 結合，讓對應在 [PAD] index 的 hidden state 都變成 -1e10 (會讓他們在套入 softmax 後變成 0)。\n",
    "\n",
    "$$\n",
    "a_t = \\text{softmax}(\\hat{a}_t)\n",
    "$$\n",
    "\n",
    "這個 $a_t$ 正是告訴我們在 decode 的當下，要注視原句的哪些 token!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Inputs\n",
    "\n",
    "| variable        | use                                                   | note                                          |\n",
    "| --------------- | ----------------------------------------------------- | --------------------------------------------- |\n",
    "| hidden          | encoder_hidden $s_0$ 或是 decoder_hidden $s_{t-1}$     | `shape=[batch_size, dec_hid_dim]`              |\n",
    "| encoder_outputs | encoder_outputs $H$，也就是 encoder 最後一層的 hidden_states   | `shape=[batch_size, src_len, enc_hid_dim * 2]` |\n",
    "| mask            | 用來遮罩的 tensor，1 是真實的 token，0 是 [PAD]            | `shape=[batch_size, src_len]`      |\n",
    "| attn            | 用來匹配 `enc_outputs` 和 `hidden` 的 attention layer | `linear(enc_hid*2+dec_hid, dec_hid)` |\n",
    "| v               | 用來學習 `attention` 權重的 linear layer              | `linear(dec_hid, 1)`                 |\n",
    "\n",
    "### Outputs\n",
    "\n",
    "| variable              | use                                                                                                       | note                                       |\n",
    "| --------------------- | --------------------------------------------------------------------------------------------------------- | ------------------------------------------ |\n",
    "| energy                | 計算 attention sequence 的第一個產物，將 hidden+encoder_outputs 組合後，透過 attention_layer 和 tanh 算出 | `shape=[batch_size, src_len, dec_hid_dim]` |\n",
    "| attention             | 將能量值 `energy` 丟入 `v` 中學習權重後產生的 attention sequence，但還沒處理 padding                      | `shape=[batch_size, src_len]`              |\n",
    "| attention.masked_fill | 把 attention sequence 當中，index 是 [PAD] 的地方改成 -1e10，讓他們通過 softmax 都會變成 0    | `tensor.masked_fill(mask, dim=n)` |                                                                                                         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "340b0be0-315e-4edf-82b9-6aec15abdc79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "\n",
    "        # hidden = [batch_size, dec_hid_dim]\n",
    "        # encoder_outputs = [batch_size, src_len, enc_hid_dim * 2]\n",
    "\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        # hidden = [batch_size, 1, dec_hid_dim]        (unsqueeze 1)\n",
    "        #        = [batch_size, src_len, dec_hid_dim]  (repeat)\n",
    "        \n",
    "        stacked_hidden = torch.cat((hidden, encoder_outputs), dim=2)\n",
    "        # stacked_hidden = [batch_size, src_len, dec_hid_dim + enc_hid_dim * 2]\n",
    "\n",
    "        energy = torch.tanh(self.attn(stacked_hidden))\n",
    "        # energy = [batch_size, src_len, dec_hid_dim]\n",
    "\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        # attention = [batch_size, src_len, 1]   (v)\n",
    "        #           = [batch_size, src_len]      (squeeze)\n",
    "\n",
    "        attention = attention.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a00635-74e1-4304-a5e0-195e00c6bb33",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Decoder\n",
    "\n",
    "![](../assets/seq2seq_decoder_attention.png)\n",
    "\n",
    "Attention 機制會用前一個時間點 $t-1$ 的 `hidden` $s_{t-1}$ 和代表整個原句的 `encoder_outputs` $H$，計算出現在時間點 $t$ 的 attention vector $a_t$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E_t &= \\tanh(\\text{attn}(s_{t-1}, H)) \\\\\n",
    "\\hat{a}_t &= vE_t \\\\\n",
    "a_t &= \\text{softmax}(\\hat{a}_t)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "接著，我們再使用 $a_t$ 對 $H$ 進行 `matrix-matrix product`，找出真正能表達原句中，對每個 token 專注力的 `weighted_sum` $w_t$。\n",
    "\n",
    "$$\n",
    "w_t = a_tH\n",
    "$$\n",
    "\n",
    "接著就可以通過 `decoderGRU` 計算現在時間點 $t$ 的 `hidden` $s_t$:\n",
    "\n",
    "$$\n",
    "s_t = \\text{DecoderGRU}(d(y_t), w_t, s_{t-1})\n",
    "$$\n",
    "\n",
    "材料有:\n",
    "\n",
    "1. embedded token $d(y_t)$ (例圖中 decoder 的 `<sos>` 經過 embedding 的結果)\n",
    "2. 上面算出來的 weighted source vector $w_t$\n",
    "3. 前一個時間點的 decoder 的 `hidden` $s_{t-1}$\n",
    "\n",
    "預測下一個 token $\\hat{y}_{t+1}$ 就很簡單了，只要把東西都備齊，放進 linear layer `fc_out` 就好:\n",
    "\n",
    "$$\n",
    "\\hat{y}_{t+1} = f(d(y_t), w_t, s_t)\n",
    "$$\n",
    "\n",
    "### Inputs\n",
    "\n",
    "| variable        | use                                                | note                                                        |\n",
    "| --------------- | -------------------------------------------------- | ----------------------------------------------------------- |\n",
    "| inp           | 在現在時間點 $t$ 時輸入到 decoder 的 token         | `shape=[batch_size]`                                        |\n",
    "| hidden          | 前一個時間點 $t-1$ 的 hidden state                 | `shape=[batch_size, dec_hid_dim]`                           |\n",
    "| encoder_outputs | Encoder 最後一層的 hidden_states $H$               | `shape=[batch_size, src_len, enc_hid_dim*2]`                |\n",
    "| mask            | 給 attention 用來無視 `[PAD]` 的 0/1s              | `shape=[batch_size, src_len]`                               |\n",
    "| output_dim      | 目標語言的 `vocab_size`，用來當 embedding 輸出大小 | `trg_tokenizer.get_vocab_size()`                            |\n",
    "| rnn             | 單層且單向的 GRU，吃 batch_first 的資料            | `GRU(enc_hid_dim*2+emb_dim, dec_hid_dim, batch_first=True)` |\n",
    "| fc_out          | 預測下一個 token 的 linear layer                   | `Linear(enc_hid_dim*2+dec_hid_dim+emb_dim, output_dim)`     |\n",
    "\n",
    "### Outputs\n",
    "\n",
    "| variable   | use                                                  | note                                          |\n",
    "| ---------- | ---------------------------------------------------- | --------------------------------------------- |\n",
    "| a          | attention vector                                     | `shape=[batch_size, src_len]`                 |\n",
    "| embedded   | input token 經過 embedding 得到的結果                | `shape=[batch_size, emb_dim]`                 |\n",
    "| weighted   | attention 和 encoder_outputs 乘積得到的 weighted sum | `shape=[batch_size, src_len]`                 |\n",
    "| rnn_input  | embedded 和 weighted 堆疊                            | `shape=[batch_size, enc_hid_dim*2 + emb_dim]` |\n",
    "| output     | DecoderGRU 的 hidden state $s_t$                     | `shape=[batch_size, dec_hid_dim]`             |\n",
    "| hidden     | DecoderGRU 的 hidden state $s_t$                     | `shape=[batch_size, dec_hid_dim]`             |\n",
    "| prediction | 預測下一個 token 是字典中哪一個 token 的機率分布     | `shape=[batch_size, output_dim]`              |\n",
    "\n",
    "> 1. forward 中很多向量都擴充了一個維度，那是代表 seq_len=1\n",
    "> 2. 因為 decoderGRU 只有單層、單時間點，所以 output 和 hidden 是一樣的東西－都是 $s_t$\n",
    "> 3. `torch.bmm` 是簡單的矩陣相乘\n",
    ">     1. 一定要 3 維矩陣\n",
    ">     2. 公式是 $b\\times n\\times m @ b\\times m\\times p = b\\times n\\times p$\n",
    ">     3. `bmm((10, 3, 4), (10, 4, 5)) = (10, 3, 5)`\n",
    ">     4. [documentation](https://pytorch.org/docs/stable/generated/torch.bmm.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "784d865d-b849-42bf-895b-c6553db52b50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inp, hidden, encoder_outputs, mask):\n",
    "        # encoder_outputs = [batch_size, src_len, enc_hid_dim*2]\n",
    "        # hidden = [batch_size, dec_hid_dim]\n",
    "\n",
    "        inp = inp.unsqueeze(1)\n",
    "        # inp = [batch_size]\n",
    "        #     = [batch_size, 1]  (unsqueeze 1)\n",
    "\n",
    "        # embedded = [batch_size, 1, emb_dim]\n",
    "        embedded = self.dropout(self.embedding(inp))\n",
    "        \n",
    "        # a = [batch_size, src_len]\n",
    "        #   = [batch_size, 1, src_len]  (unsqueeze 1)\n",
    "        a = self.attention(hidden, encoder_outputs, mask)\n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        # weighted = [batch_size, 1, enc_hid_dim*2]\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "\n",
    "        # rnn_input = [batch_size, 1, emb_dim + enc_hid_dim*2]\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        \n",
    "        # hidden = [1, batch_size, dec_hid_dim]  (unsqueeze 0)\n",
    "        hidden = hidden.unsqueeze(0)\n",
    "        \n",
    "        # output = [batch_size, 1, dec_hid_dim]\n",
    "        # hidden = [1, batch_size, dec_hid_dim]\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        \n",
    "        # embedded = [batch_size, emb_dim]        (squeeze 0)\n",
    "        # output   = [batch_size, dec_hid_dim]    (squeeze 0)\n",
    "        # weighted = [batch_size, enc_hid_dim*2]  (squeeze 0)\n",
    "        # hidden = [batch_size, dec_hid_dim]      (squeeze 0)\n",
    "        embedded = embedded.squeeze(1)\n",
    "        output = output.squeeze(1)\n",
    "        weighted = weighted.squeeze(1)\n",
    "        hidden = hidden.squeeze(0)\n",
    "        \n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        predict_input = torch.cat((output, weighted, embedded), dim=1)\n",
    "\n",
    "        # prediction = [batch_size, output_dim]\n",
    "        prediction = self.fc_out(predict_input)\n",
    "\n",
    "        # a = [batch_size, src_len]  (squeeze 1)\n",
    "        a = a.squeeze(1)\n",
    "\n",
    "        return prediction, hidden, a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0b337c-0f6f-48e8-9528-24279c6d657f",
   "metadata": {},
   "source": [
    "## Full Seq2Seq Model\n",
    "\n",
    "我用 `pl.LightningModule` 來封裝所有 seq2seq 模型的 training 和 validation (使用自己的 `_forward()` 函式)、以及 test step (使用內建的 `forward()` 函式)。輸入的 `config` 為網路中所有可以被調整的超參數，可以用於執行 `wandb sweep` (hyperparameter tuning)。\n",
    "\n",
    "### Training\n",
    "\n",
    "在 seq2seq 中，訓練時 (`_forward()`) 首先從 encoder 獲得兩種 final_hidden_states (分別是 outputs 和 hidden):\n",
    "\n",
    "1. outputs: 由每個時間點的 final_linear 輸出的 hidden_states 疊加而成，作為 attention 用途\n",
    "2. hidden: 由最後一個時間點的所有 hidden_states 組合而成，作為初始的 decoder_hidden_states\n",
    "\n",
    "再來就是 decoder 訓練的部分:\n",
    "\n",
    "- `preds` 用來儲存所有預測 $\\hat{y}$ 的結果\n",
    "- 將所有 (一個 batch) 要放入 decoder 的 input_tokens 都設為 `[BOS]`\n",
    "- 在 loop 裡面進行 decode:\n",
    "    - 往 decoder 丟入 input_token $y_t$ 和前一個 hidden_state $s_{t-1}$ 及 encoder_outputs $H$\n",
    "    - 獲得預測值 $\\hat{y}_{t+1}$ 和新的 hidden_state $s_t$\n",
    "    - 機率性使用 `teacher_force`:\n",
    "        - 使用: 下一次的 input_token 是 ground_truth\n",
    "        - 不使用: 下一次的 input_token 就是本次預測 $\\hat{y}_{t+1}$\n",
    "\n",
    "decode 的順序是從 1 開始，這是為了讓 `preds` 能夠跟 target 對稱，當我們要計算 loss 時，再把 target 和 `preds` 的第一個砍掉就好:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{trg} &= \\begin{bmatrix} &\\text{[BOS]}, &y_1, &y_2, &y_3, &\\text{[EOS]} \\end{bmatrix} \\\\\n",
    "\\text{preds} &= \\begin{bmatrix} &&&0, &\\hat{y}_1, &\\hat{y}_2, &\\hat{y}_3, &\\text{[EOS]} \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "### Inference\n",
    "\n",
    "在做 inference (`forward()`) 的時候，除了不會用 `teacher_force` 外，我們的 decode loop 會從 1 跑到自定義的 `max_len`，讓 decode 執行到 `max_len` 結束為止。我會在全部 batch 都預測完成後，再來切掉任何句子出現 `[EOS]` 之後的 tokens。\n",
    "\n",
    "``` python\n",
    "eos_pos = dict((preds == self.trg_tokenizer.token_to_id(\"[EOS]\")).nonzero().tolist())\n",
    "\n",
    "real_sentence = sentence[:eos_pos.get(idx)+1 if eos_pos.get(idx) else None]\n",
    "real_attention = attention[:eos_pos.get(idx)+1 if eos_pos.get(idx) else None, :src_len]\n",
    "```\n",
    "\n",
    "另外在 inference 會同時記錄 attention_matrix 作為 case study 用途。執行完所有的預測後，會對每一個要預測的句子回傳四個物件 (`test_outputs`):\n",
    "\n",
    "|variable|shape|desc|\n",
    "|-|-|-|\n",
    "|pred_sentence | [trg_len] | 預測的句子 tokens |\n",
    "|attn_matrix   | [trg_len, src_len] | pred_sentence 和 src_sentence 的專注力矩陣 |\n",
    "|src_sentence  | [src_len] | 原句 tokens |\n",
    "|trg_sentence  | [trg_len] | 目標句 tokens |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6244b75b-8414-4aaa-abf6-e76fe742f6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel(pl.LightningModule):\n",
    "    def __init__(self, input_dim, output_dim, src_tokenizer, trg_tokenizer, config):\n",
    "        super().__init__()\n",
    "        self.src_pad_idx = src_tokenizer.token_to_id(\"[PAD]\")\n",
    "        self.trg_tokenizer = trg_tokenizer\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            input_dim,\n",
    "            config[\"enc_emb_dim\"],\n",
    "            config[\"enc_hid_dim\"],\n",
    "            config[\"dec_hid_dim\"],\n",
    "            config[\"enc_dropout\"],\n",
    "        )\n",
    "\n",
    "        attn = Attention(config[\"enc_hid_dim\"], config[\"dec_hid_dim\"])\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            output_dim,\n",
    "            config[\"dec_emb_dim\"],\n",
    "            config[\"enc_hid_dim\"],\n",
    "            config[\"dec_hid_dim\"],\n",
    "            config[\"dec_dropout\"],\n",
    "            attn,\n",
    "        )\n",
    "\n",
    "        self.lr = config[\"lr\"]\n",
    "        self.apply(self.init_weights)\n",
    "        self.save_hyperparameters()\n",
    "    \n",
    "    \n",
    "    def init_weights(self, m):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "            else:\n",
    "                nn.init.constant_(param.data, 0)\n",
    "    \n",
    "    \n",
    "    # Training\n",
    "    # Use only when training and validation\n",
    "    def _forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # teacher_forcing_ratio is probability to use teacher forcing\n",
    "        # e.g., if teacher_forcing_ratio is 0.5 we use teacher forcing 50% of the time\n",
    "\n",
    "        # src = list of Encoding([ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
    "        # trg = list of Encoding([ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
    "\n",
    "        # src_batch = [batch_size, src_len]\n",
    "        # src_mask  = [batch_size, src_len]\n",
    "        # src_len   = [batch_size]\n",
    "        src_batch = torch.tensor([e.ids for e in src], device=self.device)\n",
    "        src_mask = torch.tensor([e.attention_mask for e in src], device=self.device)\n",
    "        src_len = torch.sum(src_mask, axis=1)\n",
    "\n",
    "        # trg_batch = [batch_size, trg_len]\n",
    "        trg_batch = torch.tensor([e.ids for e in trg], device=self.device)\n",
    "\n",
    "        batch_size = src_batch.shape[0]\n",
    "        trg_len = trg_batch.shape[1]\n",
    "        trg_vocab_size = self.output_dim\n",
    "\n",
    "        # create a tensor for storing all decoder outputs\n",
    "        preds = torch.zeros(batch_size, trg_len, trg_vocab_size, device=self.device)\n",
    "\n",
    "        # encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        # hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src_batch, src_len)\n",
    "        \n",
    "        # first input to the decoder = [BOS] tokens\n",
    "        # inp = [batch_size]\n",
    "        inp = trg_batch[:, 0]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            # pred   = [batch_size, output_dim]\n",
    "            # hidden = [batch_size, dec_hid_dim]\n",
    "            pred, hidden, _ = self.decoder(inp, hidden, encoder_outputs, src_mask)\n",
    "\n",
    "            # store predictions in a tensor holding predictions for each token\n",
    "            preds[:, t, :] = pred\n",
    "            \n",
    "            # decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            # top1 = [batch_size]\n",
    "            # get the highest predicted token from our predictions\n",
    "            top1 = pred.argmax(1)\n",
    "\n",
    "            # inp = [batch_size]\n",
    "            # if teacher forcing, use actual next token as next input\n",
    "            # if not, use predicted token\n",
    "            inp = trg_batch[:, t] if teacher_force else top1\n",
    "        \n",
    "        return preds\n",
    "    \n",
    "    \n",
    "    # Inference\n",
    "    # * Let you use the pl model as a pytorch model.\n",
    "    # * \n",
    "    # * pl_model.eval()\n",
    "    # * pl_model(X)\n",
    "    # *\n",
    "    def forward(self, src, max_len=100):\n",
    "        src_batch = torch.tensor([e.ids for e in src], device=self.device)\n",
    "        src_mask = torch.tensor([e.attention_mask for e in src], device=self.device)\n",
    "        src_len = torch.sum(src_mask, axis=1)  # actual src_len without [PAD]\n",
    "        \n",
    "        batch_size = src_batch.shape[0]\n",
    "        src_size = src_batch.shape[1]  # src_len with [PAD]\n",
    "        trg_len = max_len\n",
    "        trg_vocab_size = self.output_dim\n",
    "        \n",
    "        preds = torch.zeros(batch_size, trg_len, trg_vocab_size, device=self.device)\n",
    "        encoder_outputs, hidden = self.encoder(src_batch, src_len)\n",
    "        \n",
    "        # create a tensor for storing all attention matrices\n",
    "        attns = torch.zeros(batch_size, trg_len, src_size, device=self.device)\n",
    "        \n",
    "        # first input to the decoder = [BOS] tokens\n",
    "        # inp = [batch_size]\n",
    "        inp = torch.tensor([self.trg_tokenizer.token_to_id(\"[BOS]\")], device=self.device).repeat(batch_size)\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            # attn = [batch_size, src_len]\n",
    "            pred, hidden, attn = self.decoder(inp, hidden, encoder_outputs, src_mask)\n",
    "            \n",
    "            preds[:, t, :] = pred\n",
    "            top1 = pred.argmax(1)\n",
    "            inp = top1\n",
    "            \n",
    "            # store attention sequences in a tensor holding attention value for each token\n",
    "            attns[:, t, :] = attn\n",
    "            \n",
    "        return preds, attns, src_len\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # both are lists of encodings\n",
    "        src, trg = batch\n",
    "        \n",
    "        # y    = [batch_size, trg_len]\n",
    "        # pred = [batch_size, trg_len, output_dim]\n",
    "        y = torch.tensor([e.ids for e in trg], device=self.device)\n",
    "        preds = self._forward(src, trg)\n",
    "        output_dim = preds.shape[-1]\n",
    "        \n",
    "        # y    = [batch_size * (trg_len-1)]\n",
    "        # pred = [batch_size * (trg_len-1), output_dim]\n",
    "        y = y[:, 1:].reshape(-1)\n",
    "        preds = preds[:, 1:, :].reshape(-1, output_dim)\n",
    "        \n",
    "        loss = F.cross_entropy(preds, y, ignore_index=src_pad_idx)\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        perplexity = torch.exp(loss)\n",
    "        self.log(\"train_ppl\", perplexity)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        src, trg = batch\n",
    "        y = torch.tensor([e.ids for e in trg], device=self.device)\n",
    "        preds = self._forward(src, trg)\n",
    "        \n",
    "        output_dim = preds.shape[-1]\n",
    "        y = y[:, 1:].reshape(-1)\n",
    "        preds = preds[:, 1:, :].reshape(-1, output_dim)\n",
    "        \n",
    "        loss = F.cross_entropy(preds, y, ignore_index=src_pad_idx)\n",
    "        self.log(\"valid_loss\", loss)\n",
    "        \n",
    "        perplexity = torch.exp(loss)\n",
    "        self.log(\"valid_ppl\", perplexity)\n",
    "        \n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        src, trg = batch\n",
    "        preds, attn_matrix, real_src_len = self(src)\n",
    "        \n",
    "        # attn_matrix = [batch_size, trg_len, src_len]\n",
    "        # preds       = [batch_size, trg_len, output_dim]\n",
    "        #             = [batch_size, trg_len]             (argmax 2)\n",
    "        preds = preds.argmax(2)\n",
    "        \n",
    "        # convert `preds` tensor to list of real sentences (tokens)\n",
    "        # meaning to cut the sentence by [EOS] and remove the [PAD] tokens\n",
    "        \n",
    "        # eos_pos = dict(sentence_idx: first_pad_position)\n",
    "        #\n",
    "        # e.g., {0: 32, 2: 55} \n",
    "        # Meaning that we have 32 tokens (include [EOS]) in the first predicted sentence\n",
    "        # and `max_len` tokens (no [EOS]) in the second predicted setence\n",
    "        # and 55 tokens (include [EOS]) in the third predicted sentence\n",
    "        eos_pos = dict((preds == self.trg_tokenizer.token_to_id(\"[EOS]\")).nonzero().tolist())\n",
    "        \n",
    "        pred_sentences, attn_matrices = [], []\n",
    "        for idx, (sentence, attention, src_len) in enumerate(zip(preds, attn_matrix, real_src_len)):\n",
    "            \n",
    "            # sentence  = [trg_len_with_pad]\n",
    "            #           = [real_trg_len]\n",
    "            pred_sentences.append(sentence[:eos_pos.get(idx)+1 if eos_pos.get(idx) else None])\n",
    "            \n",
    "            # attention = [trg_len_with_pad, src_len_with_pad]\n",
    "            #           = [real_trg_len, real_src_len]\n",
    "            attn_matrices.append(attention[:eos_pos.get(idx)+1 if eos_pos.get(idx) else None, :src_len])\n",
    "        \n",
    "        # source sentences for displaying attention matrix \n",
    "        src = [[token for token in e.tokens if token != \"[PAD]\"] for e in src]\n",
    "        \n",
    "        # target sentences for calculating BLEU scores\n",
    "        trg = [[token for token in e.tokens if token != \"[PAD]\"] for e in trg]\n",
    "        \n",
    "        return pred_sentences, attn_matrices, src, trg\n",
    "        \n",
    "    \n",
    "    def test_epoch_end(self, test_outputs):\n",
    "        outputs = []\n",
    "        for (pred_sent_list, attn_list, src_list, trg_list) in test_outputs:\n",
    "            for pred_sent, attn, src, trg in list(zip(pred_sent_list, attn_list, src_list, trg_list)):\n",
    "                pred_sent = list(map(self.trg_tokenizer.id_to_token, pred_sent))\n",
    "                outputs.append((pred_sent, attn, src, trg))\n",
    "        \n",
    "        # outputs = list of predictions of testsets, each has a tuple of (pred_sentence, attn_matrix, src_sentence, trg_sentence)\n",
    "        # pred_sentence = [trg_len]\n",
    "        # attn_matrix   = [trg_len, src_len]\n",
    "        # src_sentence  = [src_len]\n",
    "        # trg_sentence  = [trg_len]\n",
    "        self.test_outputs = outputs\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "    \n",
    "    \n",
    "    def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27ba8f6d-0a19-41fc-9b84-08a66d8e5d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = pl.loggers.WandbLogger()\n",
    "\n",
    "config = {\n",
    "    \"enc_emb_dim\": 128,\n",
    "    \"dec_emb_dim\": 128,\n",
    "    \"enc_hid_dim\": 128,\n",
    "    \"dec_hid_dim\": 128,\n",
    "    \"enc_dropout\": 0.3,\n",
    "    \"dec_dropout\": 0.3,\n",
    "    \"lr\": 1e-3,\n",
    "}\n",
    "\n",
    "model = Seq2SeqModel(\n",
    "    input_dim,\n",
    "    output_dim,\n",
    "    dm.src_tokenizer,\n",
    "    dm.trg_tokenizer,\n",
    "    config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "582203d1-ea7e-4700-a056-d79367252b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 25,085,824 trainable parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2SeqModel(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(32000, 128)\n",
       "    (rnn): GRU(128, 128, batch_first=True, bidirectional=True)\n",
       "    (fc): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=384, out_features=128, bias=True)\n",
       "      (v): Linear(in_features=128, out_features=1, bias=False)\n",
       "    )\n",
       "    (embedding): Embedding(32000, 128)\n",
       "    (rnn): GRU(384, 128, batch_first=True)\n",
       "    (fc_out): Linear(in_features=512, out_features=32000, bias=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f511c9-2fdd-4513-870b-3cb135561fcb",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f8da6e7-48f4-4864-90f5-2826fc1ac8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = Path(\"checkpoints\")\n",
    "\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(dirpath=ckpt_dir,  # path for saving checkpoints\n",
    "                                          filename=\"best_ckpt\",\n",
    "                                          monitor=\"valid_loss\",\n",
    "                                          mode=\"min\",\n",
    "                                          save_top_k=1,\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "863b5798-7b71-4d6e-a413-9ef8a1d92bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    logger=wandb_logger,\n",
    "    gpus=1,\n",
    "    max_epochs=10,\n",
    "    fast_dev_run=False,\n",
    "    gradient_clip_val=1,\n",
    "    resume_from_checkpoint=ckpt_dir / \"best_ckpt\",\n",
    "    callbacks=[checkpoint],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddce994a-6222-4edf-8f2c-1d452fdd6aa7",
   "metadata": {},
   "source": [
    "## Save Init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3e08911-f53f-459c-818e-48755f50414f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f854c515c2ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_setup_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_before_accelerator_backend_setup\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# note: this sets up self.lightning_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/gpu.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, trainer, model)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_nvidia_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, trainer, model)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mto\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \"\"\"\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_training_type_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_precision_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mconnect_training_type_plugin\u001b[0;34m(self, plugin, model)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \"\"\"\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0mplugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect_precision_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugin\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPrecisionPlugin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/single_device.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/single_device.py\u001b[0m in \u001b[0;36mmodel_to_device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/core/decorators.py\u001b[0m in \u001b[0;36minner_fn\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mpre_layer_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_post_move_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mpost_layer_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/device_dtype_mixin.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__update_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    669\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    670\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 671\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33d1269-f95d-4682-999d-2f857328999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(model.best_model_path)\n",
    "run._config = config\n",
    "\n",
    "model_artifact = wandb.Artifact(\n",
    "            \"rnn_attention\", type=\"model\",\n",
    "            description=\"Seq2Seq Model with RNN attention implemented\",\n",
    "            metadata=dict(config))\n",
    "\n",
    "model_artifact.add_file(model.best_model_path)\n",
    "wandb.save(model.best_model_path)\n",
    "\n",
    "run.log_artifact(model_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b8febf-1377-44e6-a39e-e90d69366c9e",
   "metadata": {},
   "source": [
    "# Testing (BLEU Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9922a0f-3d69-43b0-bb5f-59bec045bc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_from_checkpoint(checkpoint.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844e8b93-fd94-41e0-9104-e1becb175b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b603898a-8157-44ac-996a-2e94f400dc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_corpus_bleu(preds: List[str], refs: List[List[str]], n_gram=4):\n",
    "    # arg example:\n",
    "    # preds: [\"机器人行业在环境问题上的措施\", \"松下生产科技公司也以环境先进企业为目标\"]\n",
    "    # refs: [[\"机器人在环境上的改变\", \"對於机器人在环境上的措施\"],  [\"松下科技公司的首要目标是解决环境问题\"]]\n",
    "    preds = list(map(list, preds))\n",
    "    refs = [[list(sen) for sen in ref] for ref in refs]\n",
    "    return torchmetrics.functional.nlp.bleu_score(preds, refs, n_gram=n_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be6ba1b-dccc-4fe4-92ff-ae5f6e8e901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.test_outputs[0][0]\n",
    "pred = [dm.trg_tokenizer.decode(list(map(dm.trg_tokenizer.token_to_id, pred)))]\n",
    "ref = model.test_outputs[0][3]\n",
    "ref = [[dm.trg_tokenizer.decode(list(map(dm.trg_tokenizer.token_to_id, ref)))]]\n",
    "calculate_bleu(pred, ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a0b7ac-d318-4e84-85d9-f0272b17eaa3",
   "metadata": {},
   "source": [
    "# Case Study and Attention Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad74f903-8b5f-47e1-b538-c0e7383ab03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_study(pred_token, src_token, trg_token, attn_matrix):\n",
    "    print(dm.src_tokenizer.decode(list(map(dm.src_tokenizer.token_to_id, src_token))))\n",
    "    print(dm.trg_tokenizer.decode(list(map(dm.trg_tokenizer.token_to_id, trg_token))))\n",
    "    print(dm.trg_tokenizer.decode(list(map(dm.trg_tokenizer.token_to_id, pred_token))))\n",
    "    \n",
    "    plt.rcParams['font.sans-serif'] = ['Noto Sans CJK TC']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    plt.figure(figsize=(30, 30))\n",
    "    \n",
    "    sns.heatmap(attn_matrix, xticklabels=src_token, yticklabels=pred_token)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb386ff-5c59-4784-981f-3b6dc2bfafc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_study(model.test_outputs[0][0],\n",
    "           model.test_outputs[0][2],\n",
    "           model.test_outputs[0][3],\n",
    "           model.test_outputs[0][1].cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
