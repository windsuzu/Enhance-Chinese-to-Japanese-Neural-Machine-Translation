{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import built-in Python libs\n",
    "import pickle\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "# Import data science libs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import deep learning libs\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Import weights & bias\n",
    "import wandb\n",
    "\n",
    "# Import data preprocessing libs\n",
    "from tokenizers import Tokenizer, decoders, pre_tokenizers\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.normalizers import NFKC\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Datasets, Tokenizers, DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"enc_emb_dim\": 300,\n",
    "    \"dec_emb_dim\": 300,\n",
    "    \"enc_hid_dim\": 512,\n",
    "    \"dec_hid_dim\": 512,\n",
    "    \"enc_dropout\": 0.3,\n",
    "    \"dec_dropout\": 0.3,\n",
    "    \"lr\": 1e-3,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_workers\": 8,\n",
    "    \"precision\": 16,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwindsuzu\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": "\n                Tracking run with wandb version 0.10.28<br/>\n                Syncing run <strong style=\"color:#cdcd00\">wild-glitter-144</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/windsuzu/phonetic-translation\" target=\"_blank\">https://wandb.ai/windsuzu/phonetic-translation</a><br/>\n                Run page: <a href=\"https://wandb.ai/windsuzu/phonetic-translation/runs/25g867jv\" target=\"_blank\">https://wandb.ai/windsuzu/phonetic-translation/runs/25g867jv</a><br/>\n                Run data is saved locally in <code>/home/windsuzu/phonetics-in-chinese-japanese-machine-translation/experiments/main/wandb/run-20210502_105848-25g867jv</code><br/><br/>\n            ",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project=\"phonetic-translation\",\n",
    "    entity=\"windsuzu\",\n",
    "    group=\"experiments\",\n",
    "    job_type=\"baseline_rnn\",\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact filtered_train:latest, 99.47MB. 2 files... Done. 0:0:0\n"
     ]
    }
   ],
   "source": [
    "train_data_art = run.use_artifact(\"filtered_train:latest\")\r\n",
    "train_data_dir = train_data_art.download()\r\n",
    "\r\n",
    "dev_data_art = run.use_artifact(\"dev:latest\")\r\n",
    "dev_data_dir = dev_data_art.download()\r\n",
    "\r\n",
    "test_data_art = run.use_artifact(\"test:latest\")\r\n",
    "test_data_dir = test_data_art.download()\r\n",
    "\r\n",
    "data_dir = {\r\n",
    "    \"train\": train_data_dir,\r\n",
    "    \"dev\": dev_data_dir,\r\n",
    "    \"test\": test_data_dir,\r\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencepiece_tokenizer_art = run.use_artifact(\"sentencepiece:latest\")\n",
    "sentencepiece_tokenizer_dir = sentencepiece_tokenizer_art.download()\n",
    "ch_tokenizer_dir = Path(sentencepiece_tokenizer_dir) / \"ch_tokenizer.json\"\n",
    "jp_tokenizer_dir = Path(sentencepiece_tokenizer_dir) / \"jp_tokenizer.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SentencePieceDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir,\n",
    "        src_tokenizer_dir,\n",
    "        trg_tokenizer_dir,\n",
    "        batch_size=128,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.src_tokenizer_dir = src_tokenizer_dir\n",
    "        self.trg_tokenizer_dir = trg_tokenizer_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.src_tokenizer = self._load_tokenizer(self.src_tokenizer_dir)\n",
    "        self.trg_tokenizer = self._load_tokenizer(self.trg_tokenizer_dir)\n",
    "\n",
    "        if stage == \"fit\":\n",
    "            self.train_set = self._data_preprocess(self.data_dir[\"train\"])\n",
    "            self.val_set = self._data_preprocess(self.data_dir[\"dev\"])\n",
    "\n",
    "        if stage == \"test\":\n",
    "            self.test_set = self._data_preprocess(self.data_dir[\"test\"])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_set,\n",
    "            self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=self._data_batching_fn,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_set,\n",
    "            self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=self._data_batching_fn,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_set,\n",
    "            self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=self._data_batching_fn,\n",
    "        )\n",
    "\n",
    "    def _read_data_array(self, data_dir):\n",
    "        with open(data_dir, encoding=\"utf8\") as f:\n",
    "            arr = f.readlines()\n",
    "        return arr\n",
    "\n",
    "    def _load_tokenizer(self, tokenizer_dir):\n",
    "        return Tokenizer.from_file(str(tokenizer_dir))\n",
    "\n",
    "    def _data_preprocess(self, data_dir):\n",
    "        src_txt = self._read_data_array(Path(data_dir) / \"ch.txt\")\n",
    "        trg_txt = self._read_data_array(Path(data_dir) / \"jp.txt\")\n",
    "        parallel_txt = np.array(list(zip(src_txt, trg_txt)))\n",
    "        return parallel_txt\n",
    "\n",
    "    def _data_batching_fn(self, data_batch):\n",
    "        data_batch = np.array(data_batch)  # shape=(batch_size, 2=src+trg)\n",
    "\n",
    "        src_batch = data_batch[:, 0]  # shape=(batch_size, )\n",
    "        trg_batch = data_batch[:, 1]  # shape=(batch_size, )\n",
    "        \n",
    "        # src_batch=(batch_size, longest_sentence)\n",
    "        # trg_batch=(batch_size, longest_sentence)\n",
    "        src_batch = self.src_tokenizer.encode_batch(src_batch)  \n",
    "        trg_batch = self.trg_tokenizer.encode_batch(trg_batch)\n",
    "\n",
    "        # We have to sort the batch by their non-padded lengths in descending order,\n",
    "        # because the descending order can help in `nn.utils.rnn.pack_padded_sequence()`,\n",
    "        # which it will help us ignoring the <pad> in training rnn.\n",
    "        # https://meetonfriday.com/posts/4d6a906a\n",
    "        src_batch, trg_batch = zip(\n",
    "            *sorted(\n",
    "                zip(src_batch, trg_batch),\n",
    "                key=lambda x: sum(x[0].attention_mask),\n",
    "                reverse=True,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return src_batch, trg_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = SentencePieceDataModule(\r\n",
    "    data_dir,\r\n",
    "    ch_tokenizer_dir,\r\n",
    "    jp_tokenizer_dir,\r\n",
    "    config[\"batch_size\"],\r\n",
    "    config[\"num_workers\"],\r\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dm.setup(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000 32000\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "input_dim = dm.src_tokenizer.get_vocab_size()\n",
    "output_dim = dm.trg_tokenizer.get_vocab_size()\n",
    "print(input_dim, output_dim)\n",
    "\n",
    "src_pad_idx = dm.src_tokenizer.token_to_id(\"[PAD]\")\n",
    "print(src_pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 Encoding(num_tokens=105, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "64 Encoding(num_tokens=117, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "for src, trg in dm.test_dataloader():\r\n",
    "    print(len(src), src[0])\r\n",
    "    print(len(trg), trg[0])\r\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Lightning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "![](../assets/bi_encoder.png)\n",
    "\n",
    "首先我們先輸入 embeded 過後的字來計算正向和反向的 hidden state:\n",
    "\n",
    "$$\n",
    "h_\\overrightarrow{t} = \\overrightarrow{\\text{EncoderGRU}}(\\text{emb}(x_\\overrightarrow{t}), h_\\overrightarrow{t-1})\n",
    "\\\\\n",
    "h_\\overleftarrow{t} = \\overleftarrow{\\text{EncoderGRU}}(\\text{emb}(x_\\overleftarrow{t}), h_\\overleftarrow{t-1})\n",
    "$$\n",
    "\n",
    "得到的 `outputs` 代表所有最後一層的 hidden states 的組合，我們會用 outputs 來計算 attention，也就是翻譯時要注意原句的哪些單字:\n",
    "\n",
    "$$\n",
    "h_1 = [h_\\overrightarrow{1}; h_\\overleftarrow{1}], h_2 = [h_\\overrightarrow{2}; h_\\overleftarrow{2}], \\\\\n",
    "\\text{outputs} = H = \\left\\{h_1, h_2, \\cdots, h_T\\right\\}\n",
    "$$\n",
    "\n",
    "得到的 `hidden` 代表每一層最後一個時間點的 hidden states 的疊加，我們會用 hidden 做為 decoder 初始的 context vector `s0`:\n",
    "\n",
    "$$\n",
    "\\overrightarrow{z} = h_\\overrightarrow{T} \\\\\n",
    "\\overleftarrow{z} = h_\\overleftarrow{T}\n",
    "$$\n",
    "\n",
    "因為 decoder 不是雙向，所以我們把 hidden 丟進一個 linear `g` 和 `tanh` 裡獲得濃縮後的 context vector `z`:\n",
    "\n",
    "$$\n",
    "z = \\tanh(g(\\text{cat}(\\overrightarrow{z}, \\overleftarrow{z}))) = s_0\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Inputs\n",
    "\n",
    "| variables   | use                                             | note                                              |\n",
    "| ----------- | ----------------------------------------------- | ------------------------------------------------- |\n",
    "| src         | 初始語言資料                                    | `shape=[batch_size, src_len]` (batch-first shape) |\n",
    "| src_len     | batch 中每個句子的真實長度                      | `shape=[batch_size]`                              |\n",
    "| input_dim   | 初始語言的 vocab_size                           | `src_tokenizer.get_vocab_size()`                  |\n",
    "| emb_dim     | embedding_size                                  ||\n",
    "| enc_hid_dim | Encoder 中 rnn 的 hidden_size                   ||\n",
    "| dec_hid_dim | Decoder 中 rnn 的 hidden_size                   ||\n",
    "| rnn         | 雙向 GRU, 吃 batch-first 的資料                 | `nn.GRU(emb_dim, enc_hid_dim, bidirectional = True, batch_first=True)`|\n",
    "| fc          | 將雙向 context vector 輸出成單個 context vector | `nn.Linear(enc_hid_dim * 2, dec_hid_dim)`|\n",
    "\n",
    "### Outputs\n",
    "\n",
    "| variables        | use                                            | note                                     |\n",
    "| ---------------- | ---------------------------------------------- | ---------------------------------------- |\n",
    "| packed_embedded  | 將 `[PAD]` 刪掉包裝成 packed 格式              | `PackedSequence`                         |\n",
    "| packed_outputs   | 沒有 `[PAD]` 的最後一層 hidden_states          | `PackedSequence`                         |\n",
    "| enc_outputs      | 有 `[PAD]` 的最後一層 hidden_states            | `shape=[batch_size, src_len, enc_hid_dim*2]` |\n",
    "| hidden           | 所有 layer 疊加的 context_vector               | `shape=[layer*2, batch_size, enc_hid_dim]`   |\n",
    "| hidden[:2, :, :] | 最上面 forward layer 的 hidden_state           | `shape=[batch_size, enc_hid_dim]`            |\n",
    "| hidden[:1, :, :] | 最上面 backward layer 的 hidden_state          | `shape=[batch_size, enc_hid_dim]`            |\n",
    "| last_hidden      | 透過 torch.cat 組合最後一層 forward + backward | `shape=[batch_size, enc_hid_dim*2]`          |\n",
    "| dec_hidden       | 經過 tanh + fc 得到的 context_vector           | `shape=[batch_size, dec_hid_dim]`\n",
    "\n",
    "> - **Terminology Alert**😪:\n",
    ">   - **outputs** 可以想成所有時間點的最後一層的 hidden_states 所組成\n",
    ">   - **hidden** 可以想成所有 layer (forward + backward) 在最後時間點的 hidden_states 堆疊而成的 context_vector\n",
    "\n",
    "問題一: 什麼是 packed_sequence?\n",
    "> - [[Pytorch]Pack the data to train variable length sequences](https://meetonfriday.com/posts/4d6a906a)\n",
    "\n",
    "問題二: outputs 和 hidden 差在哪?\n",
    "> - [学会区分 RNN 的 output 和 state](https://zhuanlan.zhihu.com/p/28919765)\n",
    "> - [LSTM/GRU中output和hidden的区别//其他问题](https://blog.csdn.net/yagreenhand/article/details/84893493)\n",
    "\n",
    "<img src=\"../assets/output_vs_hidden.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_len):\n",
    "        # src     = [batch_size, src_len]\n",
    "        # src_len = [batch_size]\n",
    "\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded = [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to(\"cpu\"), batch_first=True)\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "        # packed_outputs is a packed sequence containing all hidden states\n",
    "        # hidden is now from the final non-padded element in the batch\n",
    "\n",
    "        enc_outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n",
    "        # enc_outputs is now a non-packed sequence\n",
    "\n",
    "        # enc_outputs = [batch_size, src_len, enc_hid_dim*num_directions]\n",
    "        #             = [forward_n + backward_n]\n",
    "        #             = [last layer]\n",
    "\n",
    "        # hidden  = [n_layers*num_directions, batch_size, enc_hid_dim]\n",
    "        #         = [forward_1, backward_1, forward_2, backword_2, ...]\n",
    "\n",
    "        # hidden[-2, :, : ] is the last of the forwards RNN\n",
    "        # hidden[-1, :, : ] is the last of the backwards RNN\n",
    "\n",
    "        last_hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        init_dec_hidden = torch.tanh(self.fc(last_hidden))\n",
    "\n",
    "        # enc_outputs     = [batch_size, src_len, enc_hid_dim*2]  (we only have 1 layer)\n",
    "        # init_dec_hidden = [batch_size, dec_hid_dim]\n",
    "\n",
    "        return enc_outputs, init_dec_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](../assets/seq2seq_encoder_attention.png)\n",
    "\n",
    "先說結論，每次 attention layer 會產生一個 `src_len` 長度的陣列，代表在預測下一個字 $\\hat{y}_{t+1}$ 的時間點時，對原句 `src` 中每一個 token 的專注度有多高。\n",
    "\n",
    "🤯 每次需要給 attention layer 什麼?\n",
    "\n",
    "1. decoder 前一個時間點的 hidden state $s_{t-1}$ (i.e., 第一個就是 encoder 的 `hidden` $z$ 也就是 $s_0$)\n",
    "2. encoder 的 `outputs` $H$\n",
    "\n",
    "而 attention layer 其實只是一個 linear layer，用來和 `tanh` 一起計算出一個能量值 $E_t$:\n",
    "\n",
    "$$\n",
    "E_t = \\tanh(\\text{attn}(s_{t-1}, H))\n",
    "$$\n",
    "\n",
    "因為 `enc_outputs` 的長度是 `src_len`，而 `hidden` 只是一個 scalar，所以我們必須把 `hidden` 拉到跟 `enc_outputs` 一樣長。 計算出來的 $E_t$ 可以想像成 `encoder_outputs` $H$ 和 `previous_decoder_hidden_state` $s_{t-1}$ 有多匹配。\n",
    "\n",
    "因為算出來的能量值 $E_t$ 形狀是 `[src_len, hid_dim]`，我們可以把他帶入一個形狀是 `[hid_dim, 1]` linear layer $v$。最終得到一個形狀是 `[src_len]` 的 `attention_sequence`:\n",
    "\n",
    "$$\n",
    "\\hat{a}_t = v(E_t)\n",
    "$$\n",
    "\n",
    "你可以想像 $v$ 裡面學習到的參數是一個權重，告訴我們能量值 $E_t$ 作用在 `encoder_outputs` 中每個 token 的權重有多少。\n",
    "\n",
    "最後的最後， attention_sequence 會通過 softmax 讓所有機率加總為 1，其中會把 `attention_sequence` 和 `mask` 結合，讓對應在 [PAD] index 的 hidden state 都變成 -1e10 (會讓他們在套入 softmax 後變成 0)。\n",
    "\n",
    "$$\n",
    "a_t = \\text{softmax}(\\hat{a}_t)\n",
    "$$\n",
    "\n",
    "這個 $a_t$ 正是告訴我們在 decode 的當下，要注視原句的哪些 token!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Inputs\n",
    "\n",
    "| variable        | use                                                   | note                                          |\n",
    "| --------------- | ----------------------------------------------------- | --------------------------------------------- |\n",
    "| hidden          | encoder_hidden $s_0$ 或是 decoder_hidden $s_{t-1}$     | `shape=[batch_size, dec_hid_dim]`              |\n",
    "| encoder_outputs | encoder_outputs $H$，也就是 encoder 最後一層的 hidden_states   | `shape=[batch_size, src_len, enc_hid_dim * 2]` |\n",
    "| mask            | 用來遮罩的 tensor，1 是真實的 token，0 是 [PAD]            | `shape=[batch_size, src_len]`      |\n",
    "| attn            | 用來匹配 `enc_outputs` 和 `hidden` 的 attention layer | `linear(enc_hid*2+dec_hid, dec_hid)` |\n",
    "| v               | 用來學習 `attention` 權重的 linear layer              | `linear(dec_hid, 1)`                 |\n",
    "\n",
    "### Outputs\n",
    "\n",
    "| variable              | use                                                                                                       | note                                       |\n",
    "| --------------------- | --------------------------------------------------------------------------------------------------------- | ------------------------------------------ |\n",
    "| energy                | 計算 attention sequence 的第一個產物，將 hidden+encoder_outputs 組合後，透過 attention_layer 和 tanh 算出 | `shape=[batch_size, src_len, dec_hid_dim]` |\n",
    "| attention             | 將能量值 `energy` 丟入 `v` 中學習權重後產生的 attention sequence，但還沒處理 padding                      | `shape=[batch_size, src_len]`              |\n",
    "| attention.masked_fill | 把 attention sequence 當中，index 是 [PAD] 的地方改成 -1e10，讓他們通過 softmax 都會變成 0    | `tensor.masked_fill(mask, dim=n)` |                                                                                                         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "\n",
    "        # hidden = [batch_size, dec_hid_dim]\n",
    "        # encoder_outputs = [batch_size, src_len, enc_hid_dim * 2]\n",
    "\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        # hidden = [batch_size, 1, dec_hid_dim]        (unsqueeze 1)\n",
    "        #        = [batch_size, src_len, dec_hid_dim]  (repeat)\n",
    "        \n",
    "        stacked_hidden = torch.cat((hidden, encoder_outputs), dim=2)\n",
    "        # stacked_hidden = [batch_size, src_len, dec_hid_dim + enc_hid_dim * 2]\n",
    "\n",
    "        energy = torch.tanh(self.attn(stacked_hidden))\n",
    "        # energy = [batch_size, src_len, dec_hid_dim]\n",
    "\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        # attention = [batch_size, src_len, 1]   (v)\n",
    "        #           = [batch_size, src_len]      (squeeze)\n",
    "\n",
    "        attention = attention.masked_fill(mask == 0, -(2**15))\n",
    "\n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "![](../assets/seq2seq_decoder_attention.png)\n",
    "\n",
    "Attention 機制會用前一個時間點 $t-1$ 的 `hidden` $s_{t-1}$ 和代表整個原句的 `encoder_outputs` $H$，計算出現在時間點 $t$ 的 attention vector $a_t$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E_t &= \\tanh(\\text{attn}(s_{t-1}, H)) \\\\\n",
    "\\hat{a}_t &= vE_t \\\\\n",
    "a_t &= \\text{softmax}(\\hat{a}_t)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "接著，我們再使用 $a_t$ 對 $H$ 進行 `matrix-matrix product`，找出真正能表達原句中，對每個 token 專注力的 `weighted_sum` $w_t$。\n",
    "\n",
    "$$\n",
    "w_t = a_tH\n",
    "$$\n",
    "\n",
    "接著就可以通過 `decoderGRU` 計算現在時間點 $t$ 的 `hidden` $s_t$:\n",
    "\n",
    "$$\n",
    "s_t = \\text{DecoderGRU}(d(y_t), w_t, s_{t-1})\n",
    "$$\n",
    "\n",
    "材料有:\n",
    "\n",
    "1. embedded token $d(y_t)$ (例圖中 decoder 的 `<sos>` 經過 embedding 的結果)\n",
    "2. 上面算出來的 weighted source vector $w_t$\n",
    "3. 前一個時間點的 decoder 的 `hidden` $s_{t-1}$\n",
    "\n",
    "預測下一個 token $\\hat{y}_{t+1}$ 就很簡單了，只要把東西都備齊，放進 linear layer `fc_out` 就好:\n",
    "\n",
    "$$\n",
    "\\hat{y}_{t+1} = f(d(y_t), w_t, s_t)\n",
    "$$\n",
    "\n",
    "### Inputs\n",
    "\n",
    "| variable        | use                                                | note                                                        |\n",
    "| --------------- | -------------------------------------------------- | ----------------------------------------------------------- |\n",
    "| inp           | 在現在時間點 $t$ 時輸入到 decoder 的 token         | `shape=[batch_size]`                                        |\n",
    "| hidden          | 前一個時間點 $t-1$ 的 hidden state                 | `shape=[batch_size, dec_hid_dim]`                           |\n",
    "| encoder_outputs | Encoder 最後一層的 hidden_states $H$               | `shape=[batch_size, src_len, enc_hid_dim*2]`                |\n",
    "| mask            | 給 attention 用來無視 `[PAD]` 的 0/1s              | `shape=[batch_size, src_len]`                               |\n",
    "| output_dim      | 目標語言的 `vocab_size`，用來當 embedding 輸出大小 | `trg_tokenizer.get_vocab_size()`                            |\n",
    "| rnn             | 單層且單向的 GRU，吃 batch_first 的資料            | `GRU(enc_hid_dim*2+emb_dim, dec_hid_dim, batch_first=True)` |\n",
    "| fc_out          | 預測下一個 token 的 linear layer                   | `Linear(enc_hid_dim*2+dec_hid_dim+emb_dim, output_dim)`     |\n",
    "\n",
    "### Outputs\n",
    "\n",
    "| variable   | use                                                  | note                                          |\n",
    "| ---------- | ---------------------------------------------------- | --------------------------------------------- |\n",
    "| a          | attention vector                                     | `shape=[batch_size, src_len]`                 |\n",
    "| embedded   | input token 經過 embedding 得到的結果                | `shape=[batch_size, emb_dim]`                 |\n",
    "| weighted   | attention 和 encoder_outputs 乘積得到的 weighted sum | `shape=[batch_size, src_len]`                 |\n",
    "| rnn_input  | embedded 和 weighted 堆疊                            | `shape=[batch_size, enc_hid_dim*2 + emb_dim]` |\n",
    "| output     | DecoderGRU 的 hidden state $s_t$                     | `shape=[batch_size, dec_hid_dim]`             |\n",
    "| hidden     | DecoderGRU 的 hidden state $s_t$                     | `shape=[batch_size, dec_hid_dim]`             |\n",
    "| prediction | 預測下一個 token 是字典中哪一個 token 的機率分布     | `shape=[batch_size, output_dim]`              |\n",
    "\n",
    "> 1. forward 中很多向量都擴充了一個維度，那是代表 seq_len=1\n",
    "> 2. 因為 decoderGRU 只有單層、單時間點，所以 output 和 hidden 是一樣的東西－都是 $s_t$\n",
    "> 3. `torch.bmm` 是簡單的矩陣相乘\n",
    ">     1. 一定要 3 維矩陣\n",
    ">     2. 公式是 $b\\times n\\times m @ b\\times m\\times p = b\\times n\\times p$\n",
    ">     3. `bmm((10, 3, 4), (10, 4, 5)) = (10, 3, 5)`\n",
    ">     4. [documentation](https://pytorch.org/docs/stable/generated/torch.bmm.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inp, hidden, encoder_outputs, mask):\n",
    "        # encoder_outputs = [batch_size, src_len, enc_hid_dim*2]\n",
    "        # hidden = [batch_size, dec_hid_dim]\n",
    "\n",
    "        inp = inp.unsqueeze(1)\n",
    "        # inp = [batch_size]\n",
    "        #     = [batch_size, 1]  (unsqueeze 1)\n",
    "\n",
    "        # embedded = [batch_size, 1, emb_dim]\n",
    "        embedded = self.dropout(self.embedding(inp))\n",
    "        \n",
    "        # a = [batch_size, src_len]\n",
    "        #   = [batch_size, 1, src_len]  (unsqueeze 1)\n",
    "        a = self.attention(hidden, encoder_outputs, mask)\n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        # weighted = [batch_size, 1, enc_hid_dim*2]\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "\n",
    "        # rnn_input = [batch_size, 1, emb_dim + enc_hid_dim*2]\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        \n",
    "        # hidden = [1, batch_size, dec_hid_dim]  (unsqueeze 0)\n",
    "        hidden = hidden.unsqueeze(0)\n",
    "        \n",
    "        # output = [batch_size, 1, dec_hid_dim]\n",
    "        # hidden = [1, batch_size, dec_hid_dim]\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        \n",
    "        # embedded = [batch_size, emb_dim]        (squeeze 0)\n",
    "        # output   = [batch_size, dec_hid_dim]    (squeeze 0)\n",
    "        # weighted = [batch_size, enc_hid_dim*2]  (squeeze 0)\n",
    "        # hidden = [batch_size, dec_hid_dim]      (squeeze 0)\n",
    "        embedded = embedded.squeeze(1)\n",
    "        output = output.squeeze(1)\n",
    "        weighted = weighted.squeeze(1)\n",
    "        hidden = hidden.squeeze(0)\n",
    "        \n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        predict_input = torch.cat((output, weighted, embedded), dim=1)\n",
    "\n",
    "        # prediction = [batch_size, output_dim]\n",
    "        prediction = self.fc_out(predict_input)\n",
    "\n",
    "        # a = [batch_size, src_len]  (squeeze 1)\n",
    "        a = a.squeeze(1)\n",
    "\n",
    "        return prediction, hidden, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "我用 `pl.LightningModule` 來封裝所有 seq2seq 模型的 training 和 validation (使用自己的 `_forward()` 函式)、以及 test step (使用內建的 `forward()` 函式)。輸入的 `config` 為網路中所有可以被調整的超參數，可以用於執行 `wandb sweep` (hyperparameter tuning)。\n",
    "\n",
    "### Training\n",
    "\n",
    "在 seq2seq 中，訓練時 (`_forward()`) 首先從 encoder 獲得兩種 final_hidden_states (分別是 outputs 和 hidden):\n",
    "\n",
    "1. outputs: 由每個時間點的 final_linear 輸出的 hidden_states 疊加而成，作為 attention 用途\n",
    "2. hidden: 由最後一個時間點的所有 hidden_states 組合而成，作為初始的 decoder_hidden_states\n",
    "\n",
    "再來就是 decoder 訓練的部分:\n",
    "\n",
    "- `preds` 用來儲存所有預測 $\\hat{y}$ 的結果\n",
    "- 將所有 (一個 batch) 要放入 decoder 的 input_tokens 都設為 `[BOS]`\n",
    "- 在 loop 裡面進行 decode:\n",
    "    - 往 decoder 丟入 input_token $y_t$ 和前一個 hidden_state $s_{t-1}$ 及 encoder_outputs $H$\n",
    "    - 獲得預測值 $\\hat{y}_{t+1}$ 和新的 hidden_state $s_t$\n",
    "    - 機率性使用 `teacher_force`:\n",
    "        - 使用: 下一次的 input_token 是 ground_truth\n",
    "        - 不使用: 下一次的 input_token 就是本次預測 $\\hat{y}_{t+1}$\n",
    "\n",
    "decode 的順序是從 1 開始，這是為了讓 `preds` 能夠跟 target 對稱，當我們要計算 loss 時，再把 target 和 `preds` 的第一個砍掉就好:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{trg} &= \\begin{bmatrix} &\\text{[BOS]}, &y_1, &y_2, &y_3, &\\text{[EOS]} \\end{bmatrix} \\\\\n",
    "\\text{preds} &= \\begin{bmatrix} &&&0, &\\hat{y}_1, &\\hat{y}_2, &\\hat{y}_3, &\\text{[EOS]} \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "### Inference\n",
    "\n",
    "在做 inference (`forward()`) 的時候，除了不會用 `teacher_force` 外，我們的 decode loop 會從 1 跑到自定義的 `max_len`，讓 decode 執行到 `max_len` 結束為止。我會在全部 batch 都預測完成後，再來切掉任何句子出現 `[EOS]` 之後的 tokens。\n",
    "\n",
    "``` python\n",
    "eos_pos = dict((preds == self.trg_tokenizer.token_to_id(\"[EOS]\")).nonzero().tolist())\n",
    "\n",
    "real_sentence = sentence[:eos_pos.get(idx)+1 if eos_pos.get(idx) else None]\n",
    "real_attention = attention[:eos_pos.get(idx)+1 if eos_pos.get(idx) else None, :src_len]\n",
    "```\n",
    "\n",
    "另外在 inference 會同時記錄 attention_matrix 作為 case study 用途。執行完所有的預測後，會對每一個要預測的句子回傳四個物件 (`test_outputs`):\n",
    "\n",
    "|variable|shape|desc|\n",
    "|-|-|-|\n",
    "|pred_sentence | [trg_len] | 預測的句子 tokens |\n",
    "|attn_matrix   | [trg_len, src_len] | pred_sentence 和 src_sentence 的專注力矩陣 |\n",
    "|src_sentence  | [src_len] | 原句 tokens |\n",
    "|trg_sentence  | [trg_len] | 目標句 tokens |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Seq2SeqModel(pl.LightningModule):\r\n",
    "    def __init__(self, input_dim, output_dim, src_tokenizer, trg_tokenizer, config):\r\n",
    "        super().__init__()\r\n",
    "        self.src_pad_idx = src_tokenizer.token_to_id(\"[PAD]\")\r\n",
    "        self.trg_tokenizer = trg_tokenizer\r\n",
    "        self.input_dim = input_dim\r\n",
    "        self.output_dim = output_dim\r\n",
    "\r\n",
    "        self.encoder = Encoder(\r\n",
    "            input_dim,\r\n",
    "            config[\"enc_emb_dim\"],\r\n",
    "            config[\"enc_hid_dim\"],\r\n",
    "            config[\"dec_hid_dim\"],\r\n",
    "            config[\"enc_dropout\"],\r\n",
    "        )\r\n",
    "\r\n",
    "        attn = Attention(config[\"enc_hid_dim\"], config[\"dec_hid_dim\"])\r\n",
    "\r\n",
    "        self.decoder = Decoder(\r\n",
    "            output_dim,\r\n",
    "            config[\"dec_emb_dim\"],\r\n",
    "            config[\"enc_hid_dim\"],\r\n",
    "            config[\"dec_hid_dim\"],\r\n",
    "            config[\"dec_dropout\"],\r\n",
    "            attn,\r\n",
    "        )\r\n",
    "\r\n",
    "        self.lr = config[\"lr\"]\r\n",
    "        self.apply(self.init_weights)\r\n",
    "        self.save_hyperparameters()\r\n",
    "    \r\n",
    "    \r\n",
    "    def init_weights(self, m):\r\n",
    "        for name, param in m.named_parameters():\r\n",
    "            if 'weight' in name:\r\n",
    "                nn.init.normal_(param.data, mean=0, std=0.01)\r\n",
    "            else:\r\n",
    "                nn.init.constant_(param.data, 0)\r\n",
    "    \r\n",
    "    \r\n",
    "    # Training\r\n",
    "    # Use only when training and validation\r\n",
    "    def _forward(self, src, trg, teacher_forcing_ratio=0.5):\r\n",
    "        # teacher_forcing_ratio is probability to use teacher forcing\r\n",
    "        # e.g., if teacher_forcing_ratio is 0.5 we use teacher forcing 50% of the time\r\n",
    "\r\n",
    "        # src = list of Encoding([ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\r\n",
    "        # trg = list of Encoding([ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\r\n",
    "\r\n",
    "        # src_batch = [batch_size, src_len]\r\n",
    "        # src_mask  = [batch_size, src_len]\r\n",
    "        # src_len   = [batch_size]\r\n",
    "        src_batch = torch.tensor([e.ids for e in src], device=self.device)\r\n",
    "        src_mask = torch.tensor([e.attention_mask for e in src], device=self.device)\r\n",
    "        src_len = torch.sum(src_mask, axis=1)\r\n",
    "\r\n",
    "        # trg_batch = [batch_size, trg_len]\r\n",
    "        trg_batch = torch.tensor([e.ids for e in trg], device=self.device)\r\n",
    "\r\n",
    "        batch_size = src_batch.shape[0]\r\n",
    "        trg_len = trg_batch.shape[1]\r\n",
    "        trg_vocab_size = self.output_dim\r\n",
    "\r\n",
    "        # create a tensor for storing all decoder outputs\r\n",
    "        preds = torch.zeros(batch_size, trg_len, trg_vocab_size, device=self.device)\r\n",
    "\r\n",
    "        # encoder_outputs is all hidden states of the input sequence, back and forwards\r\n",
    "        # hidden is the final forward and backward hidden states, passed through a linear layer\r\n",
    "        encoder_outputs, hidden = self.encoder(src_batch, src_len)\r\n",
    "        \r\n",
    "        # first input to the decoder = [BOS] tokens\r\n",
    "        # inp = [batch_size]\r\n",
    "        inp = trg_batch[:, 0]\r\n",
    "\r\n",
    "        for t in range(1, trg_len):\r\n",
    "            # pred   = [batch_size, output_dim]\r\n",
    "            # hidden = [batch_size, dec_hid_dim]\r\n",
    "            pred, hidden, _ = self.decoder(inp, hidden, encoder_outputs, src_mask)\r\n",
    "\r\n",
    "            # store predictions in a tensor holding predictions for each token\r\n",
    "            preds[:, t, :] = pred\r\n",
    "            \r\n",
    "            # decide if we are going to use teacher forcing or not\r\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\r\n",
    "\r\n",
    "            # top1 = [batch_size]\r\n",
    "            # get the highest predicted token from our predictions\r\n",
    "            top1 = pred.argmax(1)\r\n",
    "\r\n",
    "            # inp = [batch_size]\r\n",
    "            # if teacher forcing, use actual next token as next input\r\n",
    "            # if not, use predicted token\r\n",
    "            inp = trg_batch[:, t] if teacher_force else top1\r\n",
    "        \r\n",
    "        return preds\r\n",
    "    \r\n",
    "    \r\n",
    "    # Inference\r\n",
    "    # * Let you use the pl model as a pytorch model.\r\n",
    "    # * \r\n",
    "    # * pl_model.eval()\r\n",
    "    # * pl_model(X)\r\n",
    "    # *\r\n",
    "    def forward(self, src, max_len=100):\r\n",
    "        src_batch = torch.tensor([e.ids for e in src], device=self.device)\r\n",
    "        src_mask = torch.tensor([e.attention_mask for e in src], device=self.device)\r\n",
    "        src_len = torch.sum(src_mask, axis=1)  # actual src_len without [PAD]\r\n",
    "        \r\n",
    "        batch_size = src_batch.shape[0]\r\n",
    "        src_size = src_batch.shape[1]  # src_len with [PAD]\r\n",
    "        trg_len = max_len\r\n",
    "        trg_vocab_size = self.output_dim\r\n",
    "        \r\n",
    "        preds = torch.zeros(batch_size, trg_len, trg_vocab_size, device=self.device)\r\n",
    "        encoder_outputs, hidden = self.encoder(src_batch, src_len)\r\n",
    "        \r\n",
    "        # create a tensor for storing all attention matrices\r\n",
    "        attns = torch.zeros(batch_size, trg_len, src_size, device=self.device)\r\n",
    "        \r\n",
    "        # first input to the decoder = [BOS] tokens\r\n",
    "        # inp = [batch_size]\r\n",
    "        inp = torch.tensor([self.trg_tokenizer.token_to_id(\"[BOS]\")], device=self.device).repeat(batch_size)\r\n",
    "        \r\n",
    "        for t in range(1, trg_len):\r\n",
    "            \r\n",
    "            # attn = [batch_size, src_len]\r\n",
    "            pred, hidden, attn = self.decoder(inp, hidden, encoder_outputs, src_mask)\r\n",
    "            \r\n",
    "            preds[:, t, :] = pred\r\n",
    "            top1 = pred.argmax(1)\r\n",
    "            inp = top1\r\n",
    "            \r\n",
    "            # store attention sequences in a tensor holding attention value for each token\r\n",
    "            attns[:, t, :] = attn\r\n",
    "            \r\n",
    "        return preds, attns, src_len\r\n",
    "\r\n",
    "\r\n",
    "    def training_step(self, batch, batch_idx):\r\n",
    "        # both are lists of encodings\r\n",
    "        src, trg = batch\r\n",
    "        \r\n",
    "        # y    = [batch_size, trg_len]\r\n",
    "        # pred = [batch_size, trg_len, output_dim]\r\n",
    "        y = torch.tensor([e.ids for e in trg], device=self.device)\r\n",
    "        preds = self._forward(src, trg)\r\n",
    "        output_dim = preds.shape[-1]\r\n",
    "        \r\n",
    "        # y    = [batch_size * (trg_len-1)]\r\n",
    "        # pred = [batch_size * (trg_len-1), output_dim]\r\n",
    "        y = y[:, 1:].reshape(-1)\r\n",
    "        preds = preds[:, 1:, :].reshape(-1, output_dim)\r\n",
    "        \r\n",
    "        loss = F.cross_entropy(preds, y, ignore_index=src_pad_idx)\r\n",
    "        self.log(\"train_loss\", loss)\r\n",
    "\r\n",
    "        perplexity = torch.exp(loss)\r\n",
    "        self.log(\"train_ppl\", perplexity)\r\n",
    "        \r\n",
    "        if self.global_step % 50 == 0:\r\n",
    "            torch.cuda.empty_cache()\r\n",
    "            \r\n",
    "        return loss\r\n",
    "    \r\n",
    "    \r\n",
    "    def validation_step(self, batch, batch_idx):\r\n",
    "        src, trg = batch\r\n",
    "        y = torch.tensor([e.ids for e in trg], device=self.device)\r\n",
    "        preds = self._forward(src, trg)\r\n",
    "        \r\n",
    "        output_dim = preds.shape[-1]\r\n",
    "        y = y[:, 1:].reshape(-1)\r\n",
    "        preds = preds[:, 1:, :].reshape(-1, output_dim)\r\n",
    "        \r\n",
    "        loss = F.cross_entropy(preds, y, ignore_index=src_pad_idx)\r\n",
    "        self.log(\"valid_loss\", loss, sync_dist=True)\r\n",
    "        \r\n",
    "        perplexity = torch.exp(loss)\r\n",
    "        self.log(\"valid_ppl\", perplexity, sync_dist=True)\r\n",
    "        \r\n",
    "        \r\n",
    "    def test_step(self, batch, batch_idx):\r\n",
    "        src, trg = batch\r\n",
    "        preds, attn_matrix, real_src_len = self(src)\r\n",
    "        \r\n",
    "        # attn_matrix = [batch_size, trg_len, src_len]\r\n",
    "        # preds       = [batch_size, trg_len, output_dim]\r\n",
    "        #             = [batch_size, trg_len]             (argmax 2)\r\n",
    "        preds = preds.argmax(2)\r\n",
    "        \r\n",
    "        # convert `preds` tensor to list of real sentences (tokens)\r\n",
    "        # meaning to cut the sentence by [EOS] and remove the [PAD] tokens\r\n",
    "        \r\n",
    "        # eos_pos = dict(sentence_idx: first_pad_position)\r\n",
    "        #\r\n",
    "        # e.g., {0: 32, 2: 55} \r\n",
    "        # Meaning that we have 32 tokens (include [EOS]) in the first predicted sentence\r\n",
    "        # and `max_len` tokens (no [EOS]) in the second predicted setence\r\n",
    "        # and 55 tokens (include [EOS]) in the third predicted sentence\r\n",
    "        eos_pos = dict(reversed((preds == self.trg_tokenizer.token_to_id(\"[EOS]\")).nonzero().tolist()))\r\n",
    "    \r\n",
    "        pred_sentences, attn_matrices = [], []\r\n",
    "        for idx, (sentence, attention, src_len) in enumerate(zip(preds, attn_matrix, real_src_len)):\r\n",
    "            \r\n",
    "            # sentence  = [trg_len_with_pad]\r\n",
    "            #           = [real_trg_len]\r\n",
    "            pred_sentences.append(sentence[:eos_pos.get(idx)+1 if eos_pos.get(idx) else None])\r\n",
    "            \r\n",
    "            # attention = [trg_len_with_pad, src_len_with_pad]\r\n",
    "            #           = [real_trg_len, real_src_len]\r\n",
    "            attn_matrices.append(attention[:eos_pos.get(idx)+1 if eos_pos.get(idx) else None, :src_len])\r\n",
    "        \r\n",
    "        # source sentences for displaying attention matrix \r\n",
    "        src = [[token for token in e.tokens if token != \"[PAD]\"] for e in src]\r\n",
    "        \r\n",
    "        # target sentences for calculating BLEU scores\r\n",
    "        trg = [[token for token in e.tokens if token != \"[PAD]\"] for e in trg]\r\n",
    "        \r\n",
    "        return pred_sentences, attn_matrices, src, trg\r\n",
    "        \r\n",
    "    \r\n",
    "    def test_epoch_end(self, test_outputs):\r\n",
    "        outputs = []\r\n",
    "        for (pred_sent_list, attn_list, src_list, trg_list) in test_outputs:\r\n",
    "            for pred_sent, attn, src, trg in list(zip(pred_sent_list, attn_list, src_list, trg_list)):\r\n",
    "                pred_sent = list(map(self.trg_tokenizer.id_to_token, pred_sent))\r\n",
    "                outputs.append((pred_sent, attn, src, trg))\r\n",
    "        \r\n",
    "        # outputs = list of predictions of testsets, each has a tuple of (pred_sentence, attn_matrix, src_sentence, trg_sentence)\r\n",
    "        # pred_sentence = [trg_len]\r\n",
    "        # attn_matrix   = [trg_len, src_len]\r\n",
    "        # src_sentence  = [src_len]\r\n",
    "        # trg_sentence  = [trg_len]\r\n",
    "        self.test_outputs = outputs\r\n",
    "        \r\n",
    "    def configure_optimizers(self):\r\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\r\n",
    "    \r\n",
    "    \r\n",
    "    def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):\r\n",
    "        optimizer.zero_grad(set_to_none=True)\r\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = pl.loggers.WandbLogger()\n",
    "\n",
    "model = Seq2SeqModel(\n",
    "    input_dim,\n",
    "    output_dim,\n",
    "    dm.src_tokenizer,\n",
    "    dm.trg_tokenizer,\n",
    "    config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 84,620,032 trainable parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": "Seq2SeqModel(\n  (encoder): Encoder(\n    (embedding): Embedding(32000, 300)\n    (rnn): GRU(300, 512, batch_first=True, bidirectional=True)\n    (fc): Linear(in_features=1024, out_features=512, bias=True)\n    (dropout): Dropout(p=0.3, inplace=False)\n  )\n  (decoder): Decoder(\n    (attention): Attention(\n      (attn): Linear(in_features=1536, out_features=512, bias=True)\n      (v): Linear(in_features=512, out_features=1, bias=False)\n    )\n    (embedding): Embedding(32000, 300)\n    (rnn): GRU(1324, 512, batch_first=True)\n    (fc_out): Linear(in_features=1836, out_features=32000, bias=True)\n    (dropout): Dropout(p=0.3, inplace=False)\n  )\n)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/windsuzu/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: Checkpoint directory checkpoints exists and is not empty.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = Path(\"checkpoints\")\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(dirpath=ckpt_dir,  # path for saving checkpoints\n",
    "                                          filename=\"{epoch}-{valid_loss:.3f}\",\n",
    "                                          monitor=\"valid_loss\",\n",
    "                                          mode=\"min\",\n",
    "                                          save_top_k=3,\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    fast_dev_run=False,\n",
    "    logger=wandb_logger,\n",
    "    gpus=1,\n",
    "    max_epochs=5,\n",
    "    gradient_clip_val=1,\n",
    "    precision=config[\"precision\"],\n",
    "    callbacks=[checkpoint],\n",
    "    # resume_from_checkpoint=ckpt_dir / \"epoch=2-valid_loss=3.883.ckpt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_checkpoint(checkpoint.best_model_path)\n",
    "\n",
    "# model_artifact = wandb.Artifact(\n",
    "#             \"rnn_attention\", type=\"model\",\n",
    "#             description=\"Seq2Seq Model with RNN attention implemented\",\n",
    "#             metadata=dict(config))\n",
    "\n",
    "# model_artifact.add_file(checkpoint.best_model_path)\n",
    "# wandb.save(checkpoint.best_model_path)\n",
    "\n",
    "# run.log_artifact(model_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing (BLEU Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqModel.load_from_checkpoint(ckpt_dir / \"epoch=2-valid_loss=3.883.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35eda1e4e2e04260be2ab204b6c0df15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(HTML(value='Testing'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{}]"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_corpus_bleu(preds: List[str], refs: List[List[str]], n_gram=4):\n",
    "    # arg example:\n",
    "    # preds: [\"机器人行业在环境问题上的措施\", \"松下生产科技公司也以环境先进企业为目标\"]\n",
    "    # refs: [[\"机器人在环境上的改变\", \"對於机器人在环境上的措施\"],  [\"松下科技公司的首要目标是解决环境问题\"]]\n",
    "    preds = list(map(list, preds))\n",
    "    refs = [[list(sen) for sen in ref] for ref in refs]\n",
    "    return torchmetrics.functional.nlp.bleu_score(preds, refs, n_gram=n_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [dm.trg_tokenizer.decode(list(map(dm.trg_tokenizer.token_to_id, output[0]))) for output in model.test_outputs]\n",
    "refs = [[dm.trg_tokenizer.decode(list(map(dm.trg_tokenizer.token_to_id, output[3])))] for output in model.test_outputs]\n",
    "\n",
    "bleu_score = calculate_corpus_bleu(preds, refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.4170)"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study and Attention Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.sans-serif'] = ['Noto Sans CJK TC']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def case_study(pred_token, src_token, trg_token, attn_matrix):\n",
    "    \n",
    "    src  = dm.src_tokenizer.decode(list(map(dm.src_tokenizer.token_to_id, src_token)))\n",
    "    trg  = dm.trg_tokenizer.decode(list(map(dm.trg_tokenizer.token_to_id, trg_token)))\n",
    "    pred = dm.trg_tokenizer.decode(list(map(dm.trg_tokenizer.token_to_id, pred_token)))\n",
    "    \n",
    "    print(f\"SOURCE: \\n{src}\\n {'-'*100}\")\n",
    "    print(f\"TARGET: \\n{trg}\\n {'-'*100}\")\n",
    "    print(f\"PREDICTION: \\n{pred}\\n {'-'*100}\")\n",
    "    \n",
    "    print(f\"BLEU SCORE: {calculate_corpus_bleu([trg], [[pred]])}\")\n",
    "    \n",
    "    plt.figure(figsize=(30, 30))\n",
    "    ax = sns.heatmap(attn_matrix, xticklabels=src_token, yticklabels=pred_token)\n",
    "    ax.xaxis.set_ticks_position('top')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: \n",
      "平成 17 年度日本财团助成事业 - 关于 “ 研究成果报告会 ( 油及HNS泄漏事故预防处理技术研究开发 ) ” 的报告 之 2 关于 “ HNS泄漏事故善后处理 ”\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "TARGET: \n",
      "平成 17 年度日本財団助成事業 「 研究成果報告会 ( 油及びHNS流出事故対応のための防除技術の研究開発 ) 」 の報告 その 2 「 HNS流出事故への対応 」 について\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "PREDICTION: \n",
      "平成 17 年度日本財団事業事業 − 「 研究成果報告会 ( 油およびHNS事故事故防止研究開発 ) 」報告 」 の報告 2 「 「 HNS漏洩事故後処置後処理\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "BLEU SCORE: 0.5346561074256897\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": "<Figure size 2160x2160 with 2 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i=6\n",
    "case_study(model.test_outputs[i][0],\n",
    "           model.test_outputs[i][2],\n",
    "           model.test_outputs[i][3],\n",
    "           model.test_outputs[i][1].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python385jvsc74a57bd079b9df49ac430b00ede86aa066ef9244a5bf89bf4dad1c74865db678bf985bb6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}