{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "0a2b194d-b681-46b8-a3a8-443d313cbb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import gensim\n",
    "from pathlib import Path\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from tokenizers import Tokenizer, decoders, pre_tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "08fcb108-1551-4129-a6cd-b4f064f7bd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ch = Path(\"../dataset/ASPEC-JC/train/filtered_ch.txt\")\n",
    "train_jp = Path(\"../dataset/ASPEC-JC/train/filtered_jp.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "14dbd678-ba8a-4a3b-8871-86af1ba0eb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_ch = Path(\"../tokenizer/ch_tokenizer.json\")\n",
    "tokenizer_jp = Path(\"../tokenizer/jp_tokenizer.json\")\n",
    "\n",
    "tokenizer_ch = Tokenizer.from_file(str(tokenizer_ch))\n",
    "tokenizer_jp = Tokenizer.from_file(str(tokenizer_jp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "462c08a5-85bd-4786-8d30-195e41137e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n",
      "32000\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_ch.get_vocab_size())\n",
    "print(tokenizer_jp.get_vocab_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaf4c7b-f76e-4b49-9e59-36a5e0bb05ef",
   "metadata": {},
   "source": [
    "# Tokenize raw sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "c6914fb1-3842-4dd4-a68d-0b5cd2278430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(train_ch) as f:\n",
    "#     ch_texts = [tokenizer_ch.encode(line).tokens[1:-1] for line in f.readlines()]\n",
    "\n",
    "# with open(train_jp) as f:\n",
    "#     jp_texts = [tokenizer_jp.encode(line).tokens[1:-1] for line in f.readlines()]\n",
    "\n",
    "# with open(\"tokenized_sentences/ch.txt\", \"wb\") as f:\n",
    "#     pickle.dump(ch_texts, f)\n",
    "\n",
    "# with open(\"tokenized_sentences/jp.txt\", \"wb\") as f:\n",
    "#     pickle.dump(jp_texts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "f8f8a448-c67f-4ec3-a120-b993d04b59fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tokenized_sentences/ch.txt\", \"rb\") as f:\n",
    "    ch_texts = pickle.load(f)\n",
    "\n",
    "with open(\"tokenized_sentences/jp.txt\", \"rb\") as f:\n",
    "    jp_texts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503c8141-5f11-4066-9ece-136b84c2935e",
   "metadata": {},
   "source": [
    "# Helper Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "9d6eaccd-7986-426a-9fe2-cf0421965d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_coverage(vocab, embedding):\n",
    "    \"\"\"\n",
    "    vocab = vocabulary from tokenizer, input=[(voc, index)]\n",
    "    embedding = word2vec embedding\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for word in vocab.keys():\n",
    "        try:\n",
    "            if embedding[word] is not None:\n",
    "                count += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    print(f\"{count / len(vocab):.0%} ({count}/{len(vocab)}) is covered.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "9de65373-ec20-4cc6-b396-2dd8bb257512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_matrix(vocab, embedding):\n",
    "    \"\"\"\n",
    "    vocab = vocabulary from tokenizer, input=[(voc, index)]\n",
    "    embedding = word2vec embedding\n",
    "    \"\"\"\n",
    "    embed_matrix = np.zeros((len(vocab), 300))\n",
    "    \n",
    "    for word, i in vocab.items():\n",
    "        if word in embedding:\n",
    "            embed_matrix[i] = embedding[word]\n",
    "            \n",
    "    return embed_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22badf0-6ed5-482a-940a-fbccf7e539aa",
   "metadata": {},
   "source": [
    "# Train Semantic Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "730d365c-f75a-4bf9-8d49-ccd56323dae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# if any chinese + japanese\n",
    "reg = re.compile(r'[\\u3040-\\u30ff\\u3400-\\u4dbf\\u4e00-\\u9fff\\uf900-\\ufaff\\uff66-\\uff9f]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "629f5172-103d-4ce0-9e24-ec3fb6cecac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_rule(word, count, min_count):\n",
    "    if reg.search(word):\n",
    "        return gensim.utils.RULE_DEFAULT\n",
    "    else:\n",
    "        return gensim.utils.RULE_DISCARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "f248f750-68fe-40b9-9cf3-d6ff8084ef8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word2vec(tokenized_corpus, vector_size=300, max_vocab_size=32000):\n",
    "    return Word2Vec(\n",
    "        tokenized_corpus,\n",
    "        vector_size=vector_size,\n",
    "        max_vocab_size=max_vocab_size,\n",
    "        sg=1,\n",
    "        hs=0,\n",
    "        negative=5,\n",
    "        workers=32,\n",
    "        min_count=5,\n",
    "        trim_rule=my_rule,\n",
    "        epochs=5,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37c9bc4-f447-4ad4-a0e2-5c13d100c578",
   "metadata": {},
   "source": [
    "## Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "f68ff4b4-37d8-4c7f-a38d-6fe8e7716b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_model = build_word2vec(ch_texts)\n",
    "\n",
    "ch_model.save(\"word2vec/semantic/ch_word2vec\")\n",
    "ch_model = Word2Vec.load(\"word2vec/semantic/ch_word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "af1a24d1-5625-4cd4-a4db-e7fc73e71e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27382, 300)\n"
     ]
    }
   ],
   "source": [
    "print(ch_model.wv.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "e26c1e8f-517d-42e3-b54b-138cb464a255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86% (27382/32000) is covered.\n"
     ]
    }
   ],
   "source": [
    "check_coverage(tokenizer_ch.get_vocab(), ch_model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "9ea67933-80f2-4409-8379-2738fbd9fd39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32000, 300)"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch_semantic_embedding = build_embedding_matrix(tokenizer_ch.get_vocab(), ch_model.wv)\n",
    "np.save(\"word2vec/semantic/ch_embedding.npy\", ch_semantic_embedding)\n",
    "ch_semantic_embedding = np.load(\"word2vec/semantic/ch_embedding.npy\")\n",
    "ch_semantic_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faafb9ec-72ef-45a5-ac83-ca491b3688e9",
   "metadata": {},
   "source": [
    "## Japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "bd9e33aa-9df9-44a1-b5c7-16640cdbb540",
   "metadata": {},
   "outputs": [],
   "source": [
    "jp_model = build_word2vec(jp_texts)\n",
    "\n",
    "jp_model.save(\"word2vec/semantic/jp_word2vec\")\n",
    "jp_model = Word2Vec.load(\"word2vec/semantic/jp_word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "8a478828-e3e1-4415-a21a-4c28f51be22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28313, 300)\n"
     ]
    }
   ],
   "source": [
    "print(jp_model.wv.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "d2776bbd-90d5-4b7a-8f09-237669abb6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88% (28313/32000) is covered.\n"
     ]
    }
   ],
   "source": [
    "check_coverage(tokenizer_jp.get_vocab(), jp_model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "647691ae-db13-40b9-8a03-ed7748894827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32000, 300)"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jp_semantic_embedding = build_embedding_matrix(tokenizer_jp.get_vocab(), jp_model.wv)\n",
    "np.save(\"word2vec/semantic/jp_embedding.npy\", jp_semantic_embedding)\n",
    "jp_semantic_embedding = np.load(\"word2vec/semantic/jp_embedding.npy\")\n",
    "jp_semantic_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672b3ea7-fd37-4e8e-9f1d-426a90ede646",
   "metadata": {},
   "source": [
    "# Train Phonetic Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4516c2ee-9a09-4347-8a51-ed3f3ae77d59",
   "metadata": {},
   "source": [
    "## Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "5404ea84-ac2c-46ba-b651-80b8702b974b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_ㄋㄨㄥˊ ㄧㄝˋ', '_农业'),\n",
       " ('ㄗㄨㄛˋ ㄨㄟˊ', '作为'),\n",
       " ('ㄅㄟˇ ㄏㄞˇ ㄉㄠˋ', '北海道'),\n",
       " ('ㄉㄜ˙ ㄐㄧ ㄔㄨˇ', '的基础'),\n",
       " ('ㄔㄢˇ ㄧㄝˋ', '产业'),\n",
       " ('_,', '_,'),\n",
       " ('_ㄊㄚ ㄉㄜ˙', '_它的'),\n",
       " ('ㄏㄨㄛˊ ㄒㄧㄥˋ ㄏㄨㄚˋ', '活性化'),\n",
       " ('ㄉㄨㄟˋ ㄩˊ', '对于'),\n",
       " ('ㄉㄧˋ ㄑㄩ', '地区'),\n",
       " ('ㄐㄧㄥ ㄐㄧˋ ㄉㄜ˙', '经济的'),\n",
       " ('ㄏㄨㄛˊ ㄒㄧㄥˋ ㄏㄨㄚˋ', '活性化'),\n",
       " ('ㄌㄞˊ ㄕㄨㄛ', '来说'),\n",
       " ('ㄕˋ ㄅㄧˋ ㄧㄠˋ ㄉㄜ˙', '是必要的'),\n",
       " ('_,', '_,'),\n",
       " ('_ㄉㄢˋ ㄕˋ', '_但是'),\n",
       " ('ㄓㄜˋ ㄧ', '这一'),\n",
       " ('ㄌㄧㄥˇ ㄩˋ ㄉㄜ˙', '领域的'),\n",
       " ('ㄔㄢˇ', '产'),\n",
       " ('ㄒㄩㄝˊ', '学'),\n",
       " ('ㄌㄧㄢˊ ㄏㄜˊ', '联合'),\n",
       " ('ㄘㄨㄣˊ ㄗㄞˋ ㄓㄜ˙', '存在着'),\n",
       " ('ㄐㄧˇ ㄍㄜ˙', '几个'),\n",
       " ('ㄊㄜˋ ㄕㄨ', '特殊'),\n",
       " ('ㄨㄣˋ ㄊㄧˊ', '问题'),\n",
       " ('_。', '_。')]"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dragonmapper import hanzi\n",
    "[(hanzi.to_zhuyin(c), c) for c in ch_texts[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c6161a-5db7-4bd7-9d36-ec507eb2d7cd",
   "metadata": {},
   "source": [
    "## Japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "2f086c3b-4c85-4bfe-bac5-1afa6fcb536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pykakasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "9501461c-efbd-437f-9ec3-b8a27578ae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "kks = pykakasi.kakasi()\n",
    "def to_hira(kanji):\n",
    "    return \"\".join([item[\"hira\"] for item in kks.convert(kanji)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "591e09cb-1598-4046-8989-aeb5e5e4bcbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_い', '_異'),\n",
       " ('ぎょう', '業'),\n",
       " ('たね', '種'),\n",
       " ('ねっとわーく', 'ネットワーク'),\n",
       " ('からの', 'からの'),\n",
       " ('ちいき', '地域'),\n",
       " ('ぶ', 'ブ'),\n",
       " ('らんど', 'ランド'),\n",
       " ('か', '化'),\n",
       " ('_−', '_−'),\n",
       " ('_はいきぶつ', '_廃棄物'),\n",
       " ('じゅんかん', '循環'),\n",
       " ('かた', '型'),\n",
       " ('のうぎょう', '農業'),\n",
       " ('の', 'の'),\n",
       " ('じぎょう', '事業'),\n",
       " ('か', '化'),\n",
       " ('せんりゃく', '戦略'),\n",
       " ('_−', '_−')]"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(to_hira(c), c) for c in jp_texts[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349b21a2-190d-4e20-b120-da16e7900ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
