{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 23653,
     "status": "ok",
     "timestamp": 1618318484460,
     "user": {
      "displayName": "NE6081022王士杰",
      "photoUrl": "",
      "userId": "16413260345102275028"
     },
     "user_tz": -480
    },
    "id": "IDV7wZWMnqBZ",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import built-in Python libs\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "import unicodedata\n",
    "\n",
    "# Import data science libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import weights & bias\n",
    "import wandb\n",
    "\n",
    "# Import deep learning libs\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Import data preprocessing libs\n",
    "from tokenizers import Tokenizer, normalizers, pre_tokenizers, decoders, NormalizedString, PreTokenizedString\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.normalizers import NFKC\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import fasttext\n",
    "import jieba\n",
    "from janome.tokenizer import Tokenizer as jTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jLPQX3InOdG",
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# Split Raw Dataset to Source and Target datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 733,
     "status": "ok",
     "timestamp": 1618303481864,
     "user": {
      "displayName": "NE6081022王士杰",
      "photoUrl": "",
      "userId": "16413260345102275028"
     },
     "user_tz": -480
    },
    "id": "Tm-NwXepumOs",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_dir = Path.cwd().parent\n",
    "data_dir = root_dir / \"dataset\" / \"ASPEC-JC\"\n",
    "\n",
    "def extract_jp_ch_datasets(txt_arr):\n",
    "    p = np.array([list(map(str.strip, text.split(\"|||\")[1:])) for text in txt_arr])\n",
    "    jp = p[:, 0]\n",
    "    ch = p[:, 1]\n",
    "    return ch, jp\n",
    "\n",
    "\n",
    "def split_raw_text(txt_path):\n",
    "    with txt_path.open() as f:\n",
    "        ch, jp = extract_jp_ch_datasets(f.readlines())\n",
    "        \n",
    "        np.savetxt(txt_path.parent / \"ch.txt\", ch, fmt=\"%s\")\n",
    "        np.savetxt(txt_path.parent / \"jp.txt\", jp, fmt=\"%s\")\n",
    "\n",
    "\n",
    "dev_txt = data_dir / \"dev\" / \"dev.txt\"\n",
    "devtest_txt = data_dir / \"devtest\" / \"devtest.txt\"\n",
    "test_txt = data_dir / \"test\" / \"test.txt\"\n",
    "train_txt = data_dir / \"train\" / \"train.txt\"\n",
    "\n",
    "# uncomment to run the split program\n",
    "# split_raw_text(dev_txt)\n",
    "# split_raw_text(devtest_txt)\n",
    "# split_raw_text(test_txt)\n",
    "# split_raw_text(train_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zg9yMxFMnsjH"
   },
   "source": [
    "## Log Raw Data Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 7977,
     "status": "ok",
     "timestamp": 1618303499456,
     "user": {
      "displayName": "NE6081022王士杰",
      "photoUrl": "",
      "userId": "16413260345102275028"
     },
     "user_tz": -480
    },
    "id": "I_GlWuyLnWtu",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "bfcae0ec-7029-4f10-a406-95717557a7bc"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.25<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">raw_data</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/windsuzu/phonetic-translation\" target=\"_blank\">https://wandb.ai/windsuzu/phonetic-translation</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/windsuzu/phonetic-translation/runs/2x0aragc\" target=\"_blank\">https://wandb.ai/windsuzu/phonetic-translation/runs/2x0aragc</a><br/>\n",
       "                Run data is saved locally in <code>/content/wandb/run-20210413_084456-2x0aragc</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project='phonetic-translation', \n",
    "                 entity='windsuzu',\n",
    "                 group=\"dataset\",\n",
    "                 name=\"raw_data\",\n",
    "                 job_type=\"data_upload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 10893,
     "status": "ok",
     "timestamp": 1618303525088,
     "user": {
      "displayName": "NE6081022王士杰",
      "photoUrl": "",
      "userId": "16413260345102275028"
     },
     "user_tz": -480
    },
    "id": "B5i2IKDro6dA",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# save raw txt file to artifact\n",
    "#\n",
    "# |-- train\n",
    "#     |-- ch & jp\n",
    "# |-- dev\n",
    "#     |-- ch & jp\n",
    "# |-- devtest\n",
    "#     |-- ch & jp\n",
    "# |-- test\n",
    "#     |-- ch & jp\n",
    "\n",
    "root_dir = Path.cwd().parent\n",
    "data_dir = root_dir / \"dataset\" / \"ASPEC-JC\"\n",
    "raw_data_types = [\"train\", \"dev\", \"devtest\", \"test\"]\n",
    "\n",
    "artifacts = {}\n",
    "\n",
    "for data_type in raw_data_types:\n",
    "    artifacts[data_type] = wandb.Artifact(data_type, \"raw_data\")\n",
    "    artifacts[data_type].add_file(data_dir / data_type / \"ch.txt\", \"ch.txt\")\n",
    "    artifacts[data_type].add_file(data_dir / data_type / \"jp.txt\", \"jp.txt\")\n",
    "\n",
    "for data_type, artifact in artifacts.items():\n",
    "    run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Corpus Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwindsuzu\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">honest-lion-135</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/windsuzu/phonetic-translation\" target=\"_blank\">https://wandb.ai/windsuzu/phonetic-translation</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/windsuzu/phonetic-translation/runs/2gpnjpav\" target=\"_blank\">https://wandb.ai/windsuzu/phonetic-translation/runs/2gpnjpav</a><br/>\n",
       "                Run data is saved locally in <code>/home/windsuzu/phonetics-in-chinese-japanese-machine-translation/experiments/main/wandb/run-20210501_171419-2gpnjpav</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project='phonetic-translation', \n",
    "                 entity='windsuzu',\n",
    "                 group=\"dataset\",\n",
    "                 job_type=\"corpus_filtering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Downloading large artifact train:latest, 205.64MB. 2 files... Done. 0:0:0\n"
     ]
    }
   ],
   "source": [
    "# Download Raw Data\n",
    "train_data_art = run.use_artifact(\"train:latest\")\n",
    "train_data_dir = train_data_art.download()\n",
    "\n",
    "# Open Raw Data\n",
    "with open(Path(train_data_dir) / \"ch.txt\", encoding=\"utf8\") as f:\n",
    "    ch_docs = f.readlines()\n",
    "\n",
    "with open(Path(train_data_dir) / \"jp.txt\", encoding=\"utf8\") as f:\n",
    "    jp_docs = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "672315"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_parallel_docs = list(zip(ch_docs, jp_docs))\n",
    "len(original_parallel_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Filtering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "PRETRAINED_MODEL_PATH = '../fasttext/lid.176.bin'\n",
    "model = fasttext.load_model(PRETRAINED_MODEL_PATH)\n",
    "\n",
    "\n",
    "def pass_length_ratio(ch, jp):\n",
    "    l_r = len(ch) / len(jp)\n",
    "    return l_r > 0.5 and l_r < 1.4\n",
    "\n",
    "\n",
    "def pass_length(ch, jp):\n",
    "    return len(ch) > 5 and len(ch) < 80 and len(jp) > 10 and len(jp) < 100\n",
    "\n",
    "\n",
    "def pass_same_sentence(ch, jp):\n",
    "    return ch != jp\n",
    "\n",
    "\n",
    "def pass_language_identification(ch, jp):\n",
    "    return model.predict(ch.strip())[0][0] == \"__label__zh\" and model.predict(jp.strip())[0][0] == \"__label__ja\"\n",
    "\n",
    "\n",
    "def pass_imbalance_word(text):\n",
    "    count = 0\n",
    "    for c in text.strip():\n",
    "        try:\n",
    "            # find the difference between (ch+jp) and (en+num+space)\n",
    "            if c.encode(\"ascii\").isalpha() or c.isdigit() or c.isspace():\n",
    "                count += 1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    actual = len(text) - count\n",
    "    return actual > count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Corpus Filtering\n",
    "\n",
    "1. 刪除長度比例過大的句子\n",
    "2. 刪除過短或過長的句子\n",
    "3. 刪除一樣的句子\n",
    "4. 刪除不能被 fasttext identification model 認出語言的句子\n",
    "5. 刪除英文、數字符號多於中日文的句子\n",
    "6. 刪除有多種對應的翻譯的句子\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "557685"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered = []\n",
    "overlap = {}\n",
    "\n",
    "for ch, jp in original_parallel_docs:\n",
    "    # change full-width character to half-width\n",
    "    ch = unicodedata.normalize(\"NFKC\", ch)\n",
    "    jp = unicodedata.normalize(\"NFKC\", jp)\n",
    "    \n",
    "    if not pass_length_ratio(ch, jp):\n",
    "        continue\n",
    "    \n",
    "    if not pass_length(ch, jp):\n",
    "        continue\n",
    "    \n",
    "    if not pass_same_sentence(ch, jp):\n",
    "        continue\n",
    "    \n",
    "    if not pass_language_identification(ch, jp):\n",
    "        continue\n",
    "    \n",
    "    if not pass_imbalance_word(ch) or not pass_imbalance_word(jp):\n",
    "        continue\n",
    "    \n",
    "    if overlap.get(ch) or overlap.get(jp):\n",
    "        continue\n",
    "    else:\n",
    "        overlap[ch] = jp\n",
    "        overlap[jp] = ch\n",
    "        \n",
    "    filtered.append((ch, jp))\n",
    "\n",
    "len(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Alignment\n",
    "\n",
    "### Step 1\n",
    "\n",
    "把所有的中文和日文句子都用 jieba 和 janome 進行 tokenization，並組合成要丟進 fast_align 模型的格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "janome = ja_tokenizer()\n",
    "corpus = []\n",
    "\n",
    "for ch, jp in filtered:\n",
    "    ch_str_tokenized = \" \".join(jieba.cut(ch[:-1]))\n",
    "    jp_str_tokenized = \" \".join(janome.tokenize(jp[:-1], wakati=True))\n",
    "    line = f\"{ch_str_tokenized} ||| {jp_str_tokenized}\\n\"\n",
    "    corpus.append(line)\n",
    "\n",
    "with open(\"corpus.zh-ja\", 'w', encoding=\"utf8\") as f:\n",
    "    f.writelines(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "用 fast_align 跑出 `forward.align` 和 `reverse.align` 計算每一句的 word_alignment 分數。\n",
    "\n",
    "`align` 文件的每一行格式為: `alignment ||| score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ./fast_align -i corpus/corpus.zh-ja -d -o -v -s > corpus/forward.align\n",
    "# ./fast_align -i corpus/corpus.zh-ja -d -o -v -s -r > corpus/reverse.align\n",
    "\n",
    "with open(\"../fast_align/corpus/forward.align\") as f:\n",
    "    forward = list(map(lambda x: float(x.split(\" ||| \")[1].strip()), f.readlines()))\n",
    "\n",
    "with open(\"../fast_align/corpus/reverse.align\") as f:\n",
    "    reverse = list(map(lambda x: float(x.split(\" ||| \")[1].strip()), f.readlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "\n",
    "刪除機率小於 -160 的句子 (共 96703 句)，目的主要是降低資料集大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.90000e+01, 3.54000e+03, 9.31340e+04, 2.87882e+05, 1.73100e+05]),\n",
       " array([-395.466   , -317.093915, -238.72183 , -160.349745,  -81.97766 ,\n",
       "          -3.605575]),\n",
       " <BarContainer object of 5 artists>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alignment_scores = [sum(score) / 2 for score in zip(forward, reverse)]\n",
    "plt.hist(alignment_scores, bins=5)\n",
    "\n",
    "# -3 ~ -81     173100\n",
    "# -81 ~ -160   287882\n",
    "#\n",
    "# -160 ~ -238  93134\n",
    "# -238 ~ -317  3540\n",
    "# -317 ~ -395  29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "final_filtered = []\n",
    "for i, score in enumerate(alignment_scores):\n",
    "    if score > -161:\n",
    "        final_filtered.append(filtered[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "462582"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "root_dir = Path.cwd().parent\n",
    "data_dir = root_dir / \"dataset\" / \"ASPEC-JC\"\n",
    "\n",
    "ch = np.array(final_filtered)[:, 0]\n",
    "jp = np.array(final_filtered)[:, 1]\n",
    "\n",
    "np.savetxt(data_dir / \"train\" / \"filtered_ch.txt\", ch, fmt=\"%s\", newline=\"\", encoding=\"utf8\")\n",
    "np.savetxt(data_dir / \"train\" / \"filtered_jp.txt\", jp, fmt=\"%s\", newline=\"\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Filtered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_artifacts.Artifact at 0x7fd4560da730>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_artifact = wandb.Artifact(\"filtered_train\", \"filtered_data\")\n",
    "\n",
    "filtered_artifact.add_file(data_dir / \"train\" / \"filtered_ch.txt\", \"ch.txt\")\n",
    "filtered_artifact.add_file(data_dir / \"train\" / \"filtered_jp.txt\", \"jp.txt\")\n",
    "\n",
    "run.log_artifact(filtered_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Tokenization Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_tokenizer(\n",
    "    tokenizer,\n",
    "    files,\n",
    "    unk_token=\"[UNK]\",\n",
    "    vocab_size=32000,\n",
    "    min_frequency=2,\n",
    "):\n",
    "    trainer = BpeTrainer(\n",
    "        special_tokens=[unk_token, \"[BOS]\", \"[EOS]\", \"[PAD]\"],\n",
    "        vocab_size=vocab_size,\n",
    "        show_prorgess=True,\n",
    "        min_frequency=min_frequency,\n",
    "    )\n",
    "\n",
    "    if isinstance(files, str):\n",
    "        files = [files]\n",
    "\n",
    "    tokenizer.train(files, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_tokenizer(\n",
    "    tokenizer,\n",
    "    train_text_path: Path,\n",
    "    min_frequency=2,\n",
    "    padded=True,\n",
    "    post_process=True,\n",
    "):\n",
    "    assert train_text_path.exists(), \"Training Raw Text does not exist.\"\n",
    "\n",
    "    # Train tokenizer\n",
    "    train_tokenizer(tokenizer, str(train_text_path), min_frequency=min_frequency)\n",
    "\n",
    "    # Enable Padding\n",
    "    if padded:\n",
    "        tokenizer.enable_padding(\n",
    "            pad_id=tokenizer.token_to_id(\"[PAD]\"), pad_token=\"[PAD]\"\n",
    "        )\n",
    "\n",
    "    if post_process:\n",
    "        # Encode => BOS + sentence + EOS\n",
    "        tokenizer.post_processor = TemplateProcessing(\n",
    "            single=\"[BOS] $A [EOS]\",\n",
    "            special_tokens=[\n",
    "                (\"[BOS]\", tokenizer.token_to_id(\"[BOS]\")),\n",
    "                (\"[EOS]\", tokenizer.token_to_id(\"[EOS]\")),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_tokenzier(tokenizer, save_path, has_custom_norm=False, has_custom_pretok=False, has_custom_dec=False):\n",
    "    if has_custom_norm:\n",
    "        tokenizer.normalizer = normalizers.NFKC()\n",
    "    \n",
    "    if has_custom_pretok:\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    \n",
    "    if has_custom_dec:\n",
    "        tokenizer.decoder = decoders.WordPiece()\n",
    "    \n",
    "    tokenizer.save(str(save_path))\n",
    "    \n",
    "    \n",
    "def load_tokenizer(tokenizer_path, custom_norm=None, custom_pretok=None, custom_dec=None):\n",
    "    tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    if custom_norm:\n",
    "        tokenizer.normalizer = custom_norm\n",
    "    \n",
    "    if custom_pretok:\n",
    "        tokenizer.pre_tokenizer = custom_pretok\n",
    "    \n",
    "    if custom_dec:\n",
    "        tokenizer.decoder = custom_dec\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUpMP-eOpnLv",
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# Google Sentencepiece Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306,
     "referenced_widgets": [
      "68b8a22147e642cb9ff32404a6f5df73",
      "ba8723ef855e46a9a489b50225bd1acb",
      "c14f1062d03c447a93a6cf85d944e093",
      "e87be027a35141a590e8173b3c2895f0",
      "af90fbddd11b43518d019073b56cb6d9",
      "1c164ba062484ac795b19c71632a6652",
      "3a34f0eeb71c4197b4c232afe2bf3162",
      "5890ac78442d4427bf7e74b26fa68b3f"
     ]
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 6097,
     "status": "ok",
     "timestamp": 1618305901658,
     "user": {
      "displayName": "NE6081022王士杰",
      "photoUrl": "",
      "userId": "16413260345102275028"
     },
     "user_tz": -480
    },
    "id": "yjlnnAp1qBEJ",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "5c670c8f-c4ca-4ba5-ca74-93295f4eb977",
    "tags": []
   },
   "outputs": [],
   "source": [
    "run = wandb.init(project='phonetic-translation', \n",
    "                 entity='windsuzu',\n",
    "                 group=\"tokenizer\",\n",
    "                 name=\"sentence_piece\",\n",
    "                 job_type=\"build_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1438,
     "status": "ok",
     "timestamp": 1618305904606,
     "user": {
      "displayName": "NE6081022王士杰",
      "photoUrl": "",
      "userId": "16413260345102275028"
     },
     "user_tz": -480
    },
    "id": "lpd_nM0khGat",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "9bc5ffb5-82e3-4762-9db9-274835071347",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download Raw Data\n",
    "train_data_art = run.use_artifact(\"filtered_train:latest\")\n",
    "train_data_dir = train_data_art.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 930,
     "status": "ok",
     "timestamp": 1618305906923,
     "user": {
      "displayName": "NE6081022王士杰",
      "photoUrl": "",
      "userId": "16413260345102275028"
     },
     "user_tz": -480
    },
    "id": "M19hCES8qU3l",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sentence_piece_tokenizer(unk_token=\"[UNK]\", dropout: float = None):\n",
    "    tokenizer = Tokenizer(BPE(dropout=dropout, unk_token=unk_token))\n",
    "    tokenizer.normalizer = NFKC()\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n",
    "        [\n",
    "            pre_tokenizers.Whitespace(),\n",
    "            pre_tokenizers.Punctuation(),\n",
    "            pre_tokenizers.Digits(),\n",
    "            pre_tokenizers.Metaspace(replacement=\"_\", add_prefix_space=True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    tokenizer.decoder = decoders.Metaspace(replacement=\"_\", add_prefix_space=True)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 896,
     "status": "ok",
     "timestamp": 1618305907270,
     "user": {
      "displayName": "NE6081022王士杰",
      "photoUrl": "",
      "userId": "16413260345102275028"
     },
     "user_tz": -480
    },
    "id": "3Im8LFxeqy98",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get Tokenizer.json File Path\n",
    "\n",
    "root_dir = Path.cwd().parent\n",
    "tokenizer_dir = root_dir / \"tokenizer\"\n",
    "tokenizer_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ch_tokenizer_dir = tokenizer_dir / \"tokenizer_sentencepiece_ch.json\"\n",
    "jp_tokenizer_dir = tokenizer_dir / \"tokenizer_sentencepiece_jp.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 680,
     "status": "ok",
     "timestamp": 1618305907906,
     "user": {
      "displayName": "NE6081022王士杰",
      "photoUrl": "",
      "userId": "16413260345102275028"
     },
     "user_tz": -480
    },
    "id": "SlGsMPhe-sao",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ch_tokenizer = build_tokenizer(\n",
    "    sentence_piece_tokenizer(),\n",
    "    Path(train_data_dir) / \"ch.txt\",\n",
    "    min_frequency=2,\n",
    ")\n",
    "\n",
    "jp_tokenizer = build_tokenizer(\n",
    "    sentence_piece_tokenizer(),\n",
    "    Path(train_data_dir) / \"jp.txt\",\n",
    "    min_frequency=2,\n",
    ")\n",
    "\n",
    "save_tokenzier(ch_tokenizer, ch_tokenizer_dir)\n",
    "save_tokenzier(jp_tokenizer, jp_tokenizer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ch_tokenizer = load_tokenizer(ch_tokenizer_dir)\n",
    "jp_tokenizer = load_tokenizer(jp_tokenizer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 673,
     "status": "ok",
     "timestamp": 1618305908555,
     "user": {
      "displayName": "NE6081022王士杰",
      "photoUrl": "",
      "userId": "16413260345102275028"
     },
     "user_tz": -480
    },
    "id": "meA6U3Gxh30s",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "dfa83d74-9df6-4469-98d4-f71109b76ef0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('_简易', 31689), ('AQ', 20138), ('DB', 9488), ('有可能成为', 25035), ('枝条', 27143), ('_的人', 23383), ('幻', 1599), ('进行定量', 14436), ('疲', 3075), ('在某种程度上', 22295)]\n",
      "32000\n",
      "\n",
      "[1, 7269, 3929, 895, 5379, 5674, 733, 4130, 3492, 3473, 6611, 13409, 11633, 1458, 5376, 5418, 5392, 5385, 7493, 6038, 5392, 5385, 7543, 5454, 5376, 5383, 5674, 733, 4130, 8674, 6887, 1276, 5376, 5418, 5392, 5385, 9376, 6038, 5392, 5385, 6655, 5454, 5377, 66, 0, 2]\n",
      "['[BOS]', '_主', '茎', '及', '_1', '_次', '分', '蘖', '精', '米', '蛋白质', '含量的', '标准偏差', '小', '_,', '_为', '_0', '_.', '_28', '_~', '_0', '_.', '_35', '_%', '_,', '_2', '_次', '分', '蘖', '的标准', '偏差', '大', '_,', '_为', '_0', '_.', '_44', '_~', '_0', '_.', '_60', '_%', '_。', '_', '[UNK]', '[EOS]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'主茎及 1 次分蘖精米蛋白质含量的标准偏差小 , 为 0 . 28 ~ 0 . 35 % , 2 次分蘖的标准偏差大 , 为 0 . 44 ~ 0 . 60 % 。 '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print first ten vocab\n",
    "print([(key, val) for key, val in ch_tokenizer.get_vocab().items()][:10])\n",
    "print(ch_tokenizer.get_vocab_size())\n",
    "print()\n",
    "\n",
    "# Encode and Decode Testing\n",
    "encoded = ch_tokenizer.encode(\"主茎及1次分蘖精米蛋白质含量的标准偏差小,为0.28~0.35%,2次分蘖的标准偏差大,为0.44~0.60%。😀\")\n",
    "\n",
    "print(encoded.ids)\n",
    "print(encoded.tokens)\n",
    "ch_tokenizer.decode(encoded.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 624,
     "status": "ok",
     "timestamp": 1618305909068,
     "user": {
      "displayName": "NE6081022王士杰",
      "photoUrl": "",
      "userId": "16413260345102275028"
     },
     "user_tz": -480
    },
    "id": "sFQlvvVIDyqK",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "477601f8-0222-48be-9be8-f96b74afd80d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('モバイル', 7803), ('等価な', 26186), ('サーバへ', 23005), ('渤', 2050), ('諏', 3189), ('療法と', 26419), ('性の向上', 17113), ('主眼', 26284), ('可能性が高い', 9746), ('度や', 26300)]\n",
      "32000\n",
      "\n",
      "[1, 3961, 6067, 4144, 4073, 5229, 26146, 716, 2, 3, 3, 3, 3]\n",
      "['[BOS]', '_C', '_&', '_D', '管理', '施設', 'の高度', '化', '[EOS]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "C & D管理施設の高度化\n",
      "\n",
      "[1, 63, 0, 9811, 1836, 2511, 4118, 4290, 4720, 361, 8074, 716, 2]\n",
      "['[BOS]', '_', '[UNK]', '_異', '業', '種', 'ネットワーク', 'からの', '地域', 'ブ', 'ランド', '化', '[EOS]']\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " 異業種ネットワークからの地域ブランド化\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print first ten vocab\n",
    "print([(key, val) for key, val in jp_tokenizer.get_vocab().items()][:10])\n",
    "print(jp_tokenizer.get_vocab_size())\n",
    "print()\n",
    "\n",
    "# Encode and Decode Testing\n",
    "encoded = jp_tokenizer.encode_batch([\"Ｃ＆Ｄ管理施設の高度化\", \"😀異業種ネットワークからの地域ブランド化\"])\n",
    "\n",
    "for i in range(2):\n",
    "    print(encoded[i].ids)\n",
    "    print(encoded[i].tokens)\n",
    "    print(encoded[i].attention_mask)\n",
    "    print(jp_tokenizer.decode(encoded[i].ids))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHvXQUoBRmDi"
   },
   "source": [
    "## Log Tokenizer Artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1399,
     "status": "ok",
     "timestamp": 1618305911515,
     "user": {
      "displayName": "NE6081022王士杰",
      "photoUrl": "",
      "userId": "16413260345102275028"
     },
     "user_tz": -480
    },
    "id": "zo-Y3hXjRbSC",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "6de0d4c6-e51e-4a9b-83f9-e2c87dcce7b7",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_artifacts.Artifact at 0x1e01c171d90>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifact = wandb.Artifact(\"sentencepiece\", \n",
    "                          type=\"tokenizer\",\n",
    "                          metadata={\"vocab\": 32000, \n",
    "                                    \"method\": \"SentencePiece\",\n",
    "                                    \"min_frequency\": 2})\n",
    "\n",
    "artifact.add_file(ch_tokenizer_dir, \"ch_tokenizer.json\")\n",
    "artifact.add_file(jp_tokenizer_dir, \"jp_tokenizer.json\")\n",
    "run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Specific Tokenization (Jieba & Janome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwindsuzu\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.29<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">language_specific</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/windsuzu/phonetic-translation\" target=\"_blank\">https://wandb.ai/windsuzu/phonetic-translation</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/windsuzu/phonetic-translation/runs/v34zo9pr\" target=\"_blank\">https://wandb.ai/windsuzu/phonetic-translation/runs/v34zo9pr</a><br/>\n",
       "                Run data is saved locally in <code>/home/windsuzu/phonetics-in-chinese-japanese-machine-translation/experiments/main/wandb/run-20210507_142703-v34zo9pr</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project='phonetic-translation', \n",
    "                 entity='windsuzu',\n",
    "                 group=\"tokenizer\",\n",
    "                 name=\"language_specific\",\n",
    "                 job_type=\"build_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact filtered_train:latest, 99.47MB. 2 files... Done. 0:0:0\n"
     ]
    }
   ],
   "source": [
    "# Download Raw Data\n",
    "train_data_art = run.use_artifact(\"filtered_train:latest\")\n",
    "train_data_dir = train_data_art.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get Tokenizer.json File Path\n",
    "\n",
    "root_dir = Path.cwd().parent\n",
    "tokenizer_dir = root_dir / \"tokenizer\"\n",
    "tokenizer_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "jieba_tokenizer_dir = tokenizer_dir / \"tokenizer_jieba.json\"\n",
    "janome_tokenizer_dir = tokenizer_dir / \"tokenizer_janome.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class JiebaPreTokenizer:\n",
    "    def jieba_split(self, i: int, normalized_string: NormalizedString) -> List[NormalizedString]:\n",
    "        splits = []\n",
    "        for token, start, stop in jieba.tokenize(str(normalized_string)):\n",
    "            splits.append(normalized_string[start:stop])\n",
    "        return splits\n",
    "    \n",
    "    def pre_tokenize(self, pretok: PreTokenizedString):\n",
    "         pretok.split(self.jieba_split)\n",
    "            \n",
    "            \n",
    "class JiebaDecoder:\n",
    "    def decode(self, tokens: List[str]) -> str:\n",
    "        return \"\".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_jieba_tokenizer(unk_token=\"[UNK]\", dropout: float = None):\n",
    "    tokenizer = Tokenizer(BPE(dropout=dropout, unk_token=unk_token))\n",
    "    tokenizer.normalizer = NFKC()\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.PreTokenizer.custom(JiebaPreTokenizer())\n",
    "    tokenizer.decoder = decoders.Decoder.custom(JiebaDecoder())\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "jieba_tokenizer = build_tokenizer(\n",
    "    create_jieba_tokenizer(),\n",
    "    Path(train_data_dir) / \"ch.txt\",\n",
    "    min_frequency=2,\n",
    ")\n",
    "\n",
    "save_tokenzier(jieba_tokenizer, jieba_tokenizer_dir, has_custom_pretok=True, has_custom_dec=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "jieba_tokenizer = load_tokenizer(jieba_tokenizer_dir,\n",
    "                                 custom_pretok=pre_tokenizers.PreTokenizer.custom(JiebaPreTokenizer()),\n",
    "                                 custom_dec=decoders.Decoder.custom(JiebaDecoder()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('联合会', 29267), ('生儿', 26442), ('永久', 13214), ('容许', 8355), ('J', 48), ('俄', 581), ('圏', 1158), ('微量元素', 12468), ('颢', 5118), ('百分之几', 28546)]\n",
      "32000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.592 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 17068, 898, 23, 2461, 20284, 3495, 3476, 6290, 6307, 3151, 8524, 1461, 18, 0, 2, 3, 3, 3, 3, 3, 3]\n",
      "['[BOS]', '主茎', '及', '1', '次', '分蘖', '精', '米', '蛋白质', '含量', '的', '标准偏差', '小', ',', '[UNK]', '[EOS]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "主茎及1次分蘖精米蛋白质含量的标准偏差小,\n",
      "\n",
      "[1, 422, 30199, 100, 9866, 6213, 18, 24, 2461, 20284, 3151, 8524, 1279, 18, 422, 5543, 8890, 100, 5543, 8901, 228, 2]\n",
      "['[BOS]', '为', '0.28', '~', '0.3', '5%', ',', '2', '次', '分蘖', '的', '标准偏差', '大', ',', '为', '0.', '44', '~', '0.', '60%', '。', '[EOS]']\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "为0.28~0.35%,2次分蘖的标准偏差大,为0.44~0.60%。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print first ten vocab\n",
    "print([(key, val) for key, val in jieba_tokenizer.get_vocab().items()][:10])\n",
    "print(jieba_tokenizer.get_vocab_size())\n",
    "print()\n",
    "\n",
    "# Encode and Decode Testing\n",
    "encoded = jieba_tokenizer.encode_batch([\"主茎及1次分蘖精米蛋白质含量的标准偏差小,😀\", \"为0.28~0.35%,2次分蘖的标准偏差大,为0.44~0.60%。\"])\n",
    "\n",
    "for i in range(2):\n",
    "    print(encoded[i].ids)\n",
    "    print(encoded[i].tokens)\n",
    "    print(encoded[i].attention_mask)\n",
    "    print(jieba_tokenizer.decode(encoded[i].ids))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Janome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ja_tokenizer = jTokenizer()\n",
    "class JanomePreTokenizer:\n",
    "    def janome_split(self, i: int, normalized_string: NormalizedString) -> List[NormalizedString]:\n",
    "        splits = []\n",
    "        i = 0\n",
    "        for token in ja_tokenizer.tokenize(str(normalized_string), wakati=True):\n",
    "            splits.append(normalized_string[i: i+len(token)])\n",
    "            i += len(token)\n",
    "        return splits\n",
    "    \n",
    "    def pre_tokenize(self, pretok: PreTokenizedString):\n",
    "         pretok.split(self.janome_split)\n",
    "            \n",
    "            \n",
    "class JanomeDecoder:\n",
    "    def decode(self, tokens: List[str]) -> str:\n",
    "        return \"\".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_janome_tokenizer(unk_token=\"[UNK]\", dropout: float = None):\n",
    "    tokenizer = Tokenizer(BPE(dropout=dropout, unk_token=unk_token))\n",
    "    tokenizer.normalizer = NFKC()\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.PreTokenizer.custom(JanomePreTokenizer())\n",
    "    tokenizer.decoder = decoders.Decoder.custom(JanomeDecoder())\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Exception: KeyError: b'\\xe7\\x89\\x88'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-db220c5d8389>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m janome_tokenizer = build_tokenizer(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mcreate_janome_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"jp.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmin_frequency\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-3-36c36dbe0245>\u001b[0m in \u001b[0;36mbuild_tokenizer\u001b[0;34m(tokenizer, train_text_path, min_frequency, padded, post_process)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Train tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_text_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_frequency\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Enable Padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-575aa73d65cd>\u001b[0m in \u001b[0;36mtrain_tokenizer\u001b[0;34m(tokenizer, files, unk_token, vocab_size, min_frequency)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mException\u001b[0m: Exception: KeyError: b'\\xe7\\x89\\x88'"
     ]
    }
   ],
   "source": [
    "janome_tokenizer = build_tokenizer(\n",
    "    create_janome_tokenizer(),\n",
    "    Path(train_data_dir) / \"jp.txt\",\n",
    "    min_frequency=2,\n",
    ")\n",
    "\n",
    "save_tokenzier(janome_tokenizer, janome_tokenizer_dir, has_custom_pretok=True, has_custom_dec=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "janome_tokenizer = load_tokenizer(janome_tokenizer_dir,\n",
    "                                 custom_pretok=pre_tokenizers.PreTokenizer.custom(JanomePreTokenizer()),\n",
    "                                 custom_dec=decoders.Decoder.custom(JanomeDecoder()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print first ten vocab\n",
    "print([(key, val) for key, val in janome_tokenizer.get_vocab().items()][:10])\n",
    "print(janome_tokenizer.get_vocab_size())\n",
    "print()\n",
    "\n",
    "# Encode and Decode Testing\n",
    "encoded = janome_tokenizer.encode_batch([\"標準偏差は,主茎および1次分げつで0.28〜0.35%と小さく😀\", \"2次分げつで0.44〜0.60%と大きかった。\"])\n",
    "\n",
    "for i in range(2):\n",
    "    print(encoded[i].ids)\n",
    "    print(encoded[i].tokens)\n",
    "    print(encoded[i].attention_mask)\n",
    "    print(janome_tokenizer.decode(encoded[i].ids))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNfjeD/O41ga4aGmZW502GJ",
   "collapsed_sections": [],
   "mount_file_id": "1L02Qk-Dm-jqIakxj9MCrbk4fmL4qkjaN",
   "name": "ASPEC_JC_preprocess.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1c164ba062484ac795b19c71632a6652": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a34f0eeb71c4197b4c232afe2bf3162": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5890ac78442d4427bf7e74b26fa68b3f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68b8a22147e642cb9ff32404a6f5df73": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c14f1062d03c447a93a6cf85d944e093",
       "IPY_MODEL_e87be027a35141a590e8173b3c2895f0"
      ],
      "layout": "IPY_MODEL_ba8723ef855e46a9a489b50225bd1acb"
     }
    },
    "af90fbddd11b43518d019073b56cb6d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ba8723ef855e46a9a489b50225bd1acb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c14f1062d03c447a93a6cf85d944e093": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c164ba062484ac795b19c71632a6652",
      "placeholder": "​",
      "style": "IPY_MODEL_af90fbddd11b43518d019073b56cb6d9",
      "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r"
     }
    },
    "e87be027a35141a590e8173b3c2895f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5890ac78442d4427bf7e74b26fa68b3f",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3a34f0eeb71c4197b4c232afe2bf3162",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
