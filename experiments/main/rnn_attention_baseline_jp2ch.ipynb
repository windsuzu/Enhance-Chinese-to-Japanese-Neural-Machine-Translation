{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6162407c",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b5f9392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import built-in Python libs\n",
    "import pickle\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "# Import data science libs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import deep learning libs\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Import weights & bias\n",
    "import wandb\n",
    "\n",
    "# Import data preprocessing libs\n",
    "from tokenizers import Tokenizer, decoders, pre_tokenizers\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.normalizers import NFKC\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99455ace",
   "metadata": {},
   "source": [
    "# Download Datasets, Tokenizers, DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a99230aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"enc_emb_dim\": 300,\n",
    "    \"dec_emb_dim\": 300,\n",
    "    \"enc_hid_dim\": 512,\n",
    "    \"dec_hid_dim\": 512,\n",
    "    \"enc_dropout\": 0.3,\n",
    "    \"dec_dropout\": 0.3,\n",
    "    \"lr\": 1e-3,\n",
    "    \"batch_size\": 42,\n",
    "    \"num_workers\": 12,\n",
    "    \"precision\": 16,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "119f7bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwindsuzu\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">volcanic-snow-163</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/windsuzu/phonetic-translation\" target=\"_blank\">https://wandb.ai/windsuzu/phonetic-translation</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/windsuzu/phonetic-translation/runs/zyz6znt4\" target=\"_blank\">https://wandb.ai/windsuzu/phonetic-translation/runs/zyz6znt4</a><br/>\n",
       "                Run data is saved locally in <code>/home/windsuzu/phonetics-in-chinese-japanese-machine-translation/experiments/main/wandb/run-20210503_213056-zyz6znt4</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project=\"phonetic-translation\",\n",
    "    entity=\"windsuzu\",\n",
    "    group=\"experiments\",\n",
    "    job_type=\"baseline_rnn_jp2ch\",\n",
    "    config=config,\n",
    "    reinit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5880ac5",
   "metadata": {},
   "source": [
    "## Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "201bff14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact filtered_train:latest, 99.47MB. 2 files... Done. 0:0:0\n"
     ]
    }
   ],
   "source": [
    "train_data_art = run.use_artifact(\"filtered_train:latest\")\r\n",
    "train_data_dir = train_data_art.download()\r\n",
    "\r\n",
    "dev_data_art = run.use_artifact(\"dev:latest\")\r\n",
    "dev_data_dir = dev_data_art.download()\r\n",
    "\r\n",
    "test_data_art = run.use_artifact(\"test:latest\")\r\n",
    "test_data_dir = test_data_art.download()\r\n",
    "\r\n",
    "data_dir = {\r\n",
    "    \"train\": train_data_dir,\r\n",
    "    \"dev\": dev_data_dir,\r\n",
    "    \"test\": test_data_dir,\r\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553be76f",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b62aa1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencepiece_tokenizer_art = run.use_artifact(\"sentencepiece:latest\")\n",
    "sentencepiece_tokenizer_dir = sentencepiece_tokenizer_art.download()\n",
    "jp_tokenizer_dir = Path(sentencepiece_tokenizer_dir) / \"jp_tokenizer.json\"\n",
    "ch_tokenizer_dir = Path(sentencepiece_tokenizer_dir) / \"ch_tokenizer.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee993c67",
   "metadata": {},
   "source": [
    "## DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "567f3797",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SentencePieceDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir,\n",
    "        src_tokenizer_dir,\n",
    "        trg_tokenizer_dir,\n",
    "        batch_size=128,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "        partition=0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.src_tokenizer_dir = src_tokenizer_dir\n",
    "        self.trg_tokenizer_dir = trg_tokenizer_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "        self.partition = partition\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.src_tokenizer = self._load_tokenizer(self.src_tokenizer_dir)\n",
    "        self.trg_tokenizer = self._load_tokenizer(self.trg_tokenizer_dir)\n",
    "\n",
    "        if stage == \"fit\":\n",
    "            self.train_set = self._data_preprocess(self.data_dir[\"train\"])\n",
    "            self.val_set = self._data_preprocess(self.data_dir[\"dev\"])\n",
    "\n",
    "        if stage == \"test\":\n",
    "            self.test_set = self._data_preprocess(self.data_dir[\"test\"])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_set,\n",
    "            self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=self._data_batching_fn,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_set,\n",
    "            self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=self._data_batching_fn,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_set,\n",
    "            self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=self._data_batching_fn,\n",
    "        )\n",
    "\n",
    "    def _read_data_array(self, data_dir):\n",
    "        with open(data_dir, encoding=\"utf8\") as f:\n",
    "            arr = f.readlines()\n",
    "        return arr\n",
    "\n",
    "    def _load_tokenizer(self, tokenizer_dir):\n",
    "        return Tokenizer.from_file(str(tokenizer_dir))\n",
    "\n",
    "    def _data_preprocess(self, data_dir):\n",
    "        src_txt = self._read_data_array(Path(data_dir) / \"jp.txt\")\n",
    "        trg_txt = self._read_data_array(Path(data_dir) / \"ch.txt\")\n",
    "        parallel_txt = np.array(list(zip(src_txt, trg_txt)))\n",
    "        return parallel_txt\n",
    "\n",
    "    def _data_batching_fn(self, data_batch):\n",
    "        data_batch = np.array(data_batch)  # shape=(batch_size, 2=src+trg)\n",
    "\n",
    "        src_batch = data_batch[:, 0]  # shape=(batch_size, )\n",
    "        trg_batch = data_batch[:, 1]  # shape=(batch_size, )\n",
    "\n",
    "        # src_batch=(batch_size, longest_sentence)\n",
    "        # trg_batch=(batch_size, longest_sentence)\n",
    "        src_batch = self.src_tokenizer.encode_batch(src_batch)\n",
    "        trg_batch = self.trg_tokenizer.encode_batch(trg_batch)\n",
    "\n",
    "        # We have to sort the batch by their non-padded lengths in descending order,\n",
    "        # because the descending order can help in `nn.utils.rnn.pack_padded_sequence()`,\n",
    "        # which it will help us ignoring the <pad> in training rnn.\n",
    "        # https://meetonfriday.com/posts/4d6a906a\n",
    "        src_batch, trg_batch = zip(\n",
    "            *sorted(\n",
    "                zip(src_batch, trg_batch),\n",
    "                key=lambda x: sum(x[0].attention_mask),\n",
    "                reverse=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return src_batch, trg_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38949f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = SentencePieceDataModule(\n",
    "    data_dir,\n",
    "    jp_tokenizer_dir,\n",
    "    ch_tokenizer_dir,\n",
    "    config[\"batch_size\"],\n",
    "    config[\"num_workers\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b313c3",
   "metadata": {},
   "source": [
    "### Test DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25fca246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dm.setup(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9df3fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000 32000\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "input_dim = dm.src_tokenizer.get_vocab_size()\n",
    "output_dim = dm.trg_tokenizer.get_vocab_size()\n",
    "print(input_dim, output_dim)\n",
    "\n",
    "src_pad_idx = dm.src_tokenizer.token_to_id(\"[PAD]\")\n",
    "print(src_pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6b178fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 Encoding(num_tokens=117, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "42 Encoding(num_tokens=105, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "for src, trg in dm.test_dataloader():\n",
    "    print(len(src), src[0])\n",
    "    print(len(trg), trg[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d248c95",
   "metadata": {},
   "source": [
    "# Build Lightning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46930c9",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4463fdf3",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "![](../assets/bi_encoder.png)\n",
    "\n",
    "首先我們先輸入 embeded 過後的字來計算正向和反向的 hidden state:\n",
    "\n",
    "$$\n",
    "h_\\overrightarrow{t} = \\overrightarrow{\\text{EncoderGRU}}(\\text{emb}(x_\\overrightarrow{t}), h_\\overrightarrow{t-1})\n",
    "\\\\\n",
    "h_\\overleftarrow{t} = \\overleftarrow{\\text{EncoderGRU}}(\\text{emb}(x_\\overleftarrow{t}), h_\\overleftarrow{t-1})\n",
    "$$\n",
    "\n",
    "得到的 `outputs` 代表所有最後一層的 hidden states 的組合，我們會用 outputs 來計算 attention，也就是翻譯時要注意原句的哪些單字:\n",
    "\n",
    "$$\n",
    "h_1 = [h_\\overrightarrow{1}; h_\\overleftarrow{1}], h_2 = [h_\\overrightarrow{2}; h_\\overleftarrow{2}], \\\\\n",
    "\\text{outputs} = H = \\left\\{h_1, h_2, \\cdots, h_T\\right\\}\n",
    "$$\n",
    "\n",
    "得到的 `hidden` 代表每一層最後一個時間點的 hidden states 的疊加，我們會用 hidden 做為 decoder 初始的 context vector `s0`:\n",
    "\n",
    "$$\n",
    "\\overrightarrow{z} = h_\\overrightarrow{T} \\\\\n",
    "\\overleftarrow{z} = h_\\overleftarrow{T}\n",
    "$$\n",
    "\n",
    "因為 decoder 不是雙向，所以我們把 hidden 丟進一個 linear `g` 和 `tanh` 裡獲得濃縮後的 context vector `z`:\n",
    "\n",
    "$$\n",
    "z = \\tanh(g(\\text{cat}(\\overrightarrow{z}, \\overleftarrow{z}))) = s_0\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Inputs\n",
    "\n",
    "| variables   | use                                             | note                                              |\n",
    "| ----------- | ----------------------------------------------- | ------------------------------------------------- |\n",
    "| src         | 初始語言資料                                    | `shape=[batch_size, src_len]` (batch-first shape) |\n",
    "| src_len     | batch 中每個句子的真實長度                      | `shape=[batch_size]`                              |\n",
    "| input_dim   | 初始語言的 vocab_size                           | `src_tokenizer.get_vocab_size()`                  |\n",
    "| emb_dim     | embedding_size                                  ||\n",
    "| enc_hid_dim | Encoder 中 rnn 的 hidden_size                   ||\n",
    "| dec_hid_dim | Decoder 中 rnn 的 hidden_size                   ||\n",
    "| rnn         | 雙向 GRU, 吃 batch-first 的資料                 | `nn.GRU(emb_dim, enc_hid_dim, bidirectional = True, batch_first=True)`|\n",
    "| fc          | 將雙向 context vector 輸出成單個 context vector | `nn.Linear(enc_hid_dim * 2, dec_hid_dim)`|\n",
    "\n",
    "### Outputs\n",
    "\n",
    "| variables        | use                                            | note                                     |\n",
    "| ---------------- | ---------------------------------------------- | ---------------------------------------- |\n",
    "| packed_embedded  | 將 `[PAD]` 刪掉包裝成 packed 格式              | `PackedSequence`                         |\n",
    "| packed_outputs   | 沒有 `[PAD]` 的最後一層 hidden_states          | `PackedSequence`                         |\n",
    "| enc_outputs      | 有 `[PAD]` 的最後一層 hidden_states            | `shape=[batch_size, src_len, enc_hid_dim*2]` |\n",
    "| hidden           | 所有 layer 疊加的 context_vector               | `shape=[layer*2, batch_size, enc_hid_dim]`   |\n",
    "| hidden[:2, :, :] | 最上面 forward layer 的 hidden_state           | `shape=[batch_size, enc_hid_dim]`            |\n",
    "| hidden[:1, :, :] | 最上面 backward layer 的 hidden_state          | `shape=[batch_size, enc_hid_dim]`            |\n",
    "| last_hidden      | 透過 torch.cat 組合最後一層 forward + backward | `shape=[batch_size, enc_hid_dim*2]`          |\n",
    "| dec_hidden       | 經過 tanh + fc 得到的 context_vector           | `shape=[batch_size, dec_hid_dim]`\n",
    "\n",
    "> - **Terminology Alert**😪:\n",
    ">   - **outputs** 可以想成所有時間點的最後一層的 hidden_states 所組成\n",
    ">   - **hidden** 可以想成所有 layer (forward + backward) 在最後時間點的 hidden_states 堆疊而成的 context_vector\n",
    "\n",
    "問題一: 什麼是 packed_sequence?\n",
    "> - [[Pytorch]Pack the data to train variable length sequences](https://meetonfriday.com/posts/4d6a906a)\n",
    "\n",
    "問題二: outputs 和 hidden 差在哪?\n",
    "> - [学会区分 RNN 的 output 和 state](https://zhuanlan.zhihu.com/p/28919765)\n",
    "> - [LSTM/GRU中output和hidden的区别//其他问题](https://blog.csdn.net/yagreenhand/article/details/84893493)\n",
    "\n",
    "<img src=\"../assets/output_vs_hidden.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6705a69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_len):\n",
    "        # src     = [batch_size, src_len]\n",
    "        # src_len = [batch_size]\n",
    "\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded = [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to(\"cpu\"), batch_first=True)\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "        # packed_outputs is a packed sequence containing all hidden states\n",
    "        # hidden is now from the final non-padded element in the batch\n",
    "\n",
    "        enc_outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n",
    "        # enc_outputs is now a non-packed sequence\n",
    "\n",
    "        # enc_outputs = [batch_size, src_len, enc_hid_dim*num_directions]\n",
    "        #             = [forward_n + backward_n]\n",
    "        #             = [last layer]\n",
    "\n",
    "        # hidden  = [n_layers*num_directions, batch_size, enc_hid_dim]\n",
    "        #         = [forward_1, backward_1, forward_2, backword_2, ...]\n",
    "\n",
    "        # hidden[-2, :, : ] is the last of the forwards RNN\n",
    "        # hidden[-1, :, : ] is the last of the backwards RNN\n",
    "\n",
    "        last_hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        init_dec_hidden = torch.tanh(self.fc(last_hidden))\n",
    "\n",
    "        # enc_outputs     = [batch_size, src_len, enc_hid_dim*2]  (we only have 1 layer)\n",
    "        # init_dec_hidden = [batch_size, dec_hid_dim]\n",
    "\n",
    "        return enc_outputs, init_dec_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10cce12",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282f2728",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](../assets/seq2seq_encoder_attention.png)\n",
    "\n",
    "先說結論，每次 attention layer 會產生一個 `src_len` 長度的陣列，代表在預測下一個字 $\\hat{y}_{t+1}$ 的時間點時，對原句 `src` 中每一個 token 的專注度有多高。\n",
    "\n",
    "🤯 每次需要給 attention layer 什麼?\n",
    "\n",
    "1. decoder 前一個時間點的 hidden state $s_{t-1}$ (i.e., 第一個就是 encoder 的 `hidden` $z$ 也就是 $s_0$)\n",
    "2. encoder 的 `outputs` $H$\n",
    "\n",
    "而 attention layer 其實只是一個 linear layer，用來和 `tanh` 一起計算出一個能量值 $E_t$:\n",
    "\n",
    "$$\n",
    "E_t = \\tanh(\\text{attn}(s_{t-1}, H))\n",
    "$$\n",
    "\n",
    "因為 `enc_outputs` 的長度是 `src_len`，而 `hidden` 只是一個 scalar，所以我們必須把 `hidden` 拉到跟 `enc_outputs` 一樣長。 計算出來的 $E_t$ 可以想像成 `encoder_outputs` $H$ 和 `previous_decoder_hidden_state` $s_{t-1}$ 有多匹配。\n",
    "\n",
    "因為算出來的能量值 $E_t$ 形狀是 `[src_len, hid_dim]`，我們可以把他帶入一個形狀是 `[hid_dim, 1]` linear layer $v$。最終得到一個形狀是 `[src_len]` 的 `attention_sequence`:\n",
    "\n",
    "$$\n",
    "\\hat{a}_t = v(E_t)\n",
    "$$\n",
    "\n",
    "你可以想像 $v$ 裡面學習到的參數是一個權重，告訴我們能量值 $E_t$ 作用在 `encoder_outputs` 中每個 token 的權重有多少。\n",
    "\n",
    "最後的最後， attention_sequence 會通過 softmax 讓所有機率加總為 1，其中會把 `attention_sequence` 和 `mask` 結合，讓對應在 [PAD] index 的 hidden state 都變成 -1e10 (會讓他們在套入 softmax 後變成 0)。\n",
    "\n",
    "$$\n",
    "a_t = \\text{softmax}(\\hat{a}_t)\n",
    "$$\n",
    "\n",
    "這個 $a_t$ 正是告訴我們在 decode 的當下，要注視原句的哪些 token!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Inputs\n",
    "\n",
    "| variable        | use                                                   | note                                          |\n",
    "| --------------- | ----------------------------------------------------- | --------------------------------------------- |\n",
    "| hidden          | encoder_hidden $s_0$ 或是 decoder_hidden $s_{t-1}$     | `shape=[batch_size, dec_hid_dim]`              |\n",
    "| encoder_outputs | encoder_outputs $H$，也就是 encoder 最後一層的 hidden_states   | `shape=[batch_size, src_len, enc_hid_dim * 2]` |\n",
    "| mask            | 用來遮罩的 tensor，1 是真實的 token，0 是 [PAD]            | `shape=[batch_size, src_len]`      |\n",
    "| attn            | 用來匹配 `enc_outputs` 和 `hidden` 的 attention layer | `linear(enc_hid*2+dec_hid, dec_hid)` |\n",
    "| v               | 用來學習 `attention` 權重的 linear layer              | `linear(dec_hid, 1)`                 |\n",
    "\n",
    "### Outputs\n",
    "\n",
    "| variable              | use                                                                                                       | note                                       |\n",
    "| --------------------- | --------------------------------------------------------------------------------------------------------- | ------------------------------------------ |\n",
    "| energy                | 計算 attention sequence 的第一個產物，將 hidden+encoder_outputs 組合後，透過 attention_layer 和 tanh 算出 | `shape=[batch_size, src_len, dec_hid_dim]` |\n",
    "| attention             | 將能量值 `energy` 丟入 `v` 中學習權重後產生的 attention sequence，但還沒處理 padding                      | `shape=[batch_size, src_len]`              |\n",
    "| attention.masked_fill | 把 attention sequence 當中，index 是 [PAD] 的地方改成 -1e10，讓他們通過 softmax 都會變成 0    | `tensor.masked_fill(mask, dim=n)` |                                                                                                         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebf2a757",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "\n",
    "        # hidden = [batch_size, dec_hid_dim]\n",
    "        # encoder_outputs = [batch_size, src_len, enc_hid_dim * 2]\n",
    "\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        # hidden = [batch_size, 1, dec_hid_dim]        (unsqueeze 1)\n",
    "        #        = [batch_size, src_len, dec_hid_dim]  (repeat)\n",
    "        \n",
    "        stacked_hidden = torch.cat((hidden, encoder_outputs), dim=2)\n",
    "        # stacked_hidden = [batch_size, src_len, dec_hid_dim + enc_hid_dim * 2]\n",
    "\n",
    "        energy = torch.tanh(self.attn(stacked_hidden))\n",
    "        # energy = [batch_size, src_len, dec_hid_dim]\n",
    "\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        # attention = [batch_size, src_len, 1]   (v)\n",
    "        #           = [batch_size, src_len]      (squeeze)\n",
    "\n",
    "        attention = attention.masked_fill(mask == 0, -(2**15))\n",
    "\n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c32683",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0b85d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "![](../assets/seq2seq_decoder_attention.png)\n",
    "\n",
    "Attention 機制會用前一個時間點 $t-1$ 的 `hidden` $s_{t-1}$ 和代表整個原句的 `encoder_outputs` $H$，計算出現在時間點 $t$ 的 attention vector $a_t$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E_t &= \\tanh(\\text{attn}(s_{t-1}, H)) \\\\\n",
    "\\hat{a}_t &= vE_t \\\\\n",
    "a_t &= \\text{softmax}(\\hat{a}_t)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "接著，我們再使用 $a_t$ 對 $H$ 進行 `matrix-matrix product`，找出真正能表達原句中，對每個 token 專注力的 `weighted_sum` $w_t$。\n",
    "\n",
    "$$\n",
    "w_t = a_tH\n",
    "$$\n",
    "\n",
    "接著就可以通過 `decoderGRU` 計算現在時間點 $t$ 的 `hidden` $s_t$:\n",
    "\n",
    "$$\n",
    "s_t = \\text{DecoderGRU}(d(y_t), w_t, s_{t-1})\n",
    "$$\n",
    "\n",
    "材料有:\n",
    "\n",
    "1. embedded token $d(y_t)$ (例圖中 decoder 的 `<sos>` 經過 embedding 的結果)\n",
    "2. 上面算出來的 weighted source vector $w_t$\n",
    "3. 前一個時間點的 decoder 的 `hidden` $s_{t-1}$\n",
    "\n",
    "預測下一個 token $\\hat{y}_{t+1}$ 就很簡單了，只要把東西都備齊，放進 linear layer `fc_out` 就好:\n",
    "\n",
    "$$\n",
    "\\hat{y}_{t+1} = f(d(y_t), w_t, s_t)\n",
    "$$\n",
    "\n",
    "### Inputs\n",
    "\n",
    "| variable        | use                                                | note                                                        |\n",
    "| --------------- | -------------------------------------------------- | ----------------------------------------------------------- |\n",
    "| inp           | 在現在時間點 $t$ 時輸入到 decoder 的 token         | `shape=[batch_size]`                                        |\n",
    "| hidden          | 前一個時間點 $t-1$ 的 hidden state                 | `shape=[batch_size, dec_hid_dim]`                           |\n",
    "| encoder_outputs | Encoder 最後一層的 hidden_states $H$               | `shape=[batch_size, src_len, enc_hid_dim*2]`                |\n",
    "| mask            | 給 attention 用來無視 `[PAD]` 的 0/1s              | `shape=[batch_size, src_len]`                               |\n",
    "| output_dim      | 目標語言的 `vocab_size`，用來當 embedding 輸出大小 | `trg_tokenizer.get_vocab_size()`                            |\n",
    "| rnn             | 單層且單向的 GRU，吃 batch_first 的資料            | `GRU(enc_hid_dim*2+emb_dim, dec_hid_dim, batch_first=True)` |\n",
    "| fc_out          | 預測下一個 token 的 linear layer                   | `Linear(enc_hid_dim*2+dec_hid_dim+emb_dim, output_dim)`     |\n",
    "\n",
    "### Outputs\n",
    "\n",
    "| variable   | use                                                  | note                                          |\n",
    "| ---------- | ---------------------------------------------------- | --------------------------------------------- |\n",
    "| a          | attention vector                                     | `shape=[batch_size, src_len]`                 |\n",
    "| embedded   | input token 經過 embedding 得到的結果                | `shape=[batch_size, emb_dim]`                 |\n",
    "| weighted   | attention 和 encoder_outputs 乘積得到的 weighted sum | `shape=[batch_size, src_len]`                 |\n",
    "| rnn_input  | embedded 和 weighted 堆疊                            | `shape=[batch_size, enc_hid_dim*2 + emb_dim]` |\n",
    "| output     | DecoderGRU 的 hidden state $s_t$                     | `shape=[batch_size, dec_hid_dim]`             |\n",
    "| hidden     | DecoderGRU 的 hidden state $s_t$                     | `shape=[batch_size, dec_hid_dim]`             |\n",
    "| prediction | 預測下一個 token 是字典中哪一個 token 的機率分布     | `shape=[batch_size, output_dim]`              |\n",
    "\n",
    "> 1. forward 中很多向量都擴充了一個維度，那是代表 seq_len=1\n",
    "> 2. 因為 decoderGRU 只有單層、單時間點，所以 output 和 hidden 是一樣的東西－都是 $s_t$\n",
    "> 3. `torch.bmm` 是簡單的矩陣相乘\n",
    ">     1. 一定要 3 維矩陣\n",
    ">     2. 公式是 $b\\times n\\times m @ b\\times m\\times p = b\\times n\\times p$\n",
    ">     3. `bmm((10, 3, 4), (10, 4, 5)) = (10, 3, 5)`\n",
    ">     4. [documentation](https://pytorch.org/docs/stable/generated/torch.bmm.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a940208",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inp, hidden, encoder_outputs, mask):\n",
    "        # encoder_outputs = [batch_size, src_len, enc_hid_dim*2]\n",
    "        # hidden = [batch_size, dec_hid_dim]\n",
    "\n",
    "        inp = inp.unsqueeze(1)\n",
    "        # inp = [batch_size]\n",
    "        #     = [batch_size, 1]  (unsqueeze 1)\n",
    "\n",
    "        # embedded = [batch_size, 1, emb_dim]\n",
    "        embedded = self.dropout(self.embedding(inp))\n",
    "        \n",
    "        # a = [batch_size, src_len]\n",
    "        #   = [batch_size, 1, src_len]  (unsqueeze 1)\n",
    "        a = self.attention(hidden, encoder_outputs, mask)\n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        # weighted = [batch_size, 1, enc_hid_dim*2]\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "\n",
    "        # rnn_input = [batch_size, 1, emb_dim + enc_hid_dim*2]\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        \n",
    "        # hidden = [1, batch_size, dec_hid_dim]  (unsqueeze 0)\n",
    "        hidden = hidden.unsqueeze(0)\n",
    "        \n",
    "        # output = [batch_size, 1, dec_hid_dim]\n",
    "        # hidden = [1, batch_size, dec_hid_dim]\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        \n",
    "        # embedded = [batch_size, emb_dim]        (squeeze 0)\n",
    "        # output   = [batch_size, dec_hid_dim]    (squeeze 0)\n",
    "        # weighted = [batch_size, enc_hid_dim*2]  (squeeze 0)\n",
    "        # hidden = [batch_size, dec_hid_dim]      (squeeze 0)\n",
    "        embedded = embedded.squeeze(1)\n",
    "        output = output.squeeze(1)\n",
    "        weighted = weighted.squeeze(1)\n",
    "        hidden = hidden.squeeze(0)\n",
    "        \n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        predict_input = torch.cat((output, weighted, embedded), dim=1)\n",
    "\n",
    "        # prediction = [batch_size, output_dim]\n",
    "        prediction = self.fc_out(predict_input)\n",
    "\n",
    "        # a = [batch_size, src_len]  (squeeze 1)\n",
    "        a = a.squeeze(1)\n",
    "\n",
    "        return prediction, hidden, a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66bfac2",
   "metadata": {},
   "source": [
    "## Full Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2230e479",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "我用 `pl.LightningModule` 來封裝所有 seq2seq 模型的 training 和 validation (使用自己的 `_forward()` 函式)、以及 test step (使用內建的 `forward()` 函式)。輸入的 `config` 為網路中所有可以被調整的超參數，可以用於執行 `wandb sweep` (hyperparameter tuning)。\n",
    "\n",
    "### Training\n",
    "\n",
    "在 seq2seq 中，訓練時 (`_forward()`) 首先從 encoder 獲得兩種 final_hidden_states (分別是 outputs 和 hidden):\n",
    "\n",
    "1. outputs: 由每個時間點的 final_linear 輸出的 hidden_states 疊加而成，作為 attention 用途\n",
    "2. hidden: 由最後一個時間點的所有 hidden_states 組合而成，作為初始的 decoder_hidden_states\n",
    "\n",
    "再來就是 decoder 訓練的部分:\n",
    "\n",
    "- `preds` 用來儲存所有預測 $\\hat{y}$ 的結果\n",
    "- 將所有 (一個 batch) 要放入 decoder 的 input_tokens 都設為 `[BOS]`\n",
    "- 在 loop 裡面進行 decode:\n",
    "    - 往 decoder 丟入 input_token $y_t$ 和前一個 hidden_state $s_{t-1}$ 及 encoder_outputs $H$\n",
    "    - 獲得預測值 $\\hat{y}_{t+1}$ 和新的 hidden_state $s_t$\n",
    "    - 機率性使用 `teacher_force`:\n",
    "        - 使用: 下一次的 input_token 是 ground_truth\n",
    "        - 不使用: 下一次的 input_token 就是本次預測 $\\hat{y}_{t+1}$\n",
    "\n",
    "decode 的順序是從 1 開始，這是為了讓 `preds` 能夠跟 target 對稱，當我們要計算 loss 時，再把 target 和 `preds` 的第一個砍掉就好:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{trg} &= \\begin{bmatrix} &\\text{[BOS]}, &y_1, &y_2, &y_3, &\\text{[EOS]} \\end{bmatrix} \\\\\n",
    "\\text{preds} &= \\begin{bmatrix} &&&0, &\\hat{y}_1, &\\hat{y}_2, &\\hat{y}_3, &\\text{[EOS]} \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "### Inference\n",
    "\n",
    "在做 inference (`forward()`) 的時候，除了不會用 `teacher_force` 外，我們的 decode loop 會從 1 跑到自定義的 `max_len`，讓 decode 執行到 `max_len` 結束為止。我會在全部 batch 都預測完成後，再來切掉任何句子出現 `[EOS]` 之後的 tokens。\n",
    "\n",
    "``` python\n",
    "eos_pos = dict((preds == self.trg_tokenizer.token_to_id(\"[EOS]\")).nonzero().tolist())\n",
    "\n",
    "real_sentence = sentence[:eos_pos.get(idx)+1 if eos_pos.get(idx) else None]\n",
    "real_attention = attention[:eos_pos.get(idx)+1 if eos_pos.get(idx) else None, :src_len]\n",
    "```\n",
    "\n",
    "另外在 inference 會同時記錄 attention_matrix 作為 case study 用途。執行完所有的預測後，會對每一個要預測的句子回傳四個物件 (`test_outputs`):\n",
    "\n",
    "|variable|shape|desc|\n",
    "|-|-|-|\n",
    "|pred_sentence | [trg_len] | 預測的句子 tokens |\n",
    "|attn_matrix   | [trg_len, src_len] | pred_sentence 和 src_sentence 的專注力矩陣 |\n",
    "|src_sentence  | [src_len] | 原句 tokens |\n",
    "|trg_sentence  | [trg_len] | 目標句 tokens |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2c7e30a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Seq2SeqModel(pl.LightningModule):\n",
    "    def __init__(self, input_dim, output_dim, src_tokenizer, trg_tokenizer, config):\n",
    "        super().__init__()\n",
    "        self.src_pad_idx = src_tokenizer.token_to_id(\"[PAD]\")\n",
    "        self.trg_tokenizer = trg_tokenizer\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            input_dim,\n",
    "            config[\"enc_emb_dim\"],\n",
    "            config[\"enc_hid_dim\"],\n",
    "            config[\"dec_hid_dim\"],\n",
    "            config[\"enc_dropout\"],\n",
    "        )\n",
    "\n",
    "        attn = Attention(config[\"enc_hid_dim\"], config[\"dec_hid_dim\"])\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            output_dim,\n",
    "            config[\"dec_emb_dim\"],\n",
    "            config[\"enc_hid_dim\"],\n",
    "            config[\"dec_hid_dim\"],\n",
    "            config[\"dec_dropout\"],\n",
    "            attn,\n",
    "        )\n",
    "\n",
    "        self.lr = config[\"lr\"]\n",
    "        self.apply(self.init_weights)\n",
    "        self.save_hyperparameters()\n",
    "    \n",
    "    \n",
    "    def init_weights(self, m):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "            else:\n",
    "                nn.init.constant_(param.data, 0)\n",
    "    \n",
    "    \n",
    "    # Training\n",
    "    # Use only when training and validation\n",
    "    def _forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # teacher_forcing_ratio is probability to use teacher forcing\n",
    "        # e.g., if teacher_forcing_ratio is 0.5 we use teacher forcing 50% of the time\n",
    "\n",
    "        # src = list of Encoding([ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
    "        # trg = list of Encoding([ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
    "\n",
    "        # src_batch = [batch_size, src_len]\n",
    "        # src_mask  = [batch_size, src_len]\n",
    "        # src_len   = [batch_size]\n",
    "        src_batch = torch.tensor([e.ids for e in src], device=self.device)\n",
    "        src_mask = torch.tensor([e.attention_mask for e in src], device=self.device)\n",
    "        src_len = torch.sum(src_mask, axis=1)\n",
    "\n",
    "        # trg_batch = [batch_size, trg_len]\n",
    "        trg_batch = torch.tensor([e.ids for e in trg], device=self.device)\n",
    "\n",
    "        batch_size = src_batch.shape[0]\n",
    "        trg_len = trg_batch.shape[1]\n",
    "        trg_vocab_size = self.output_dim\n",
    "\n",
    "        # create a tensor for storing all decoder outputs\n",
    "        preds = torch.zeros(batch_size, trg_len, trg_vocab_size, device=self.device)\n",
    "\n",
    "        # encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        # hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src_batch, src_len)\n",
    "        \n",
    "        # first input to the decoder = [BOS] tokens\n",
    "        # inp = [batch_size]\n",
    "        inp = trg_batch[:, 0]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            # pred   = [batch_size, output_dim]\n",
    "            # hidden = [batch_size, dec_hid_dim]\n",
    "            pred, hidden, _ = self.decoder(inp, hidden, encoder_outputs, src_mask)\n",
    "\n",
    "            # store predictions in a tensor holding predictions for each token\n",
    "            preds[:, t, :] = pred\n",
    "            \n",
    "            # decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            # top1 = [batch_size]\n",
    "            # get the highest predicted token from our predictions\n",
    "            top1 = pred.argmax(1)\n",
    "\n",
    "            # inp = [batch_size]\n",
    "            # if teacher forcing, use actual next token as next input\n",
    "            # if not, use predicted token\n",
    "            inp = trg_batch[:, t] if teacher_force else top1\n",
    "        \n",
    "        return preds\n",
    "    \n",
    "    \n",
    "    # Inference\n",
    "    # * Let you use the pl model as a pytorch model.\n",
    "    # * \n",
    "    # * pl_model.eval()\n",
    "    # * pl_model(X)\n",
    "    # *\n",
    "    def forward(self, src, max_len=100):\n",
    "        src_batch = torch.tensor([e.ids for e in src], device=self.device)\n",
    "        src_mask = torch.tensor([e.attention_mask for e in src], device=self.device)\n",
    "        src_len = torch.sum(src_mask, axis=1)  # actual src_len without [PAD]\n",
    "        \n",
    "        batch_size = src_batch.shape[0]\n",
    "        src_size = src_batch.shape[1]  # src_len with [PAD]\n",
    "        trg_len = max_len\n",
    "        trg_vocab_size = self.output_dim\n",
    "        \n",
    "        preds = torch.zeros(batch_size, trg_len, trg_vocab_size, device=self.device)\n",
    "        encoder_outputs, hidden = self.encoder(src_batch, src_len)\n",
    "        \n",
    "        # create a tensor for storing all attention matrices\n",
    "        attns = torch.zeros(batch_size, trg_len, src_size, device=self.device)\n",
    "        \n",
    "        # first input to the decoder = [BOS] tokens\n",
    "        # inp = [batch_size]\n",
    "        inp = torch.tensor([self.trg_tokenizer.token_to_id(\"[BOS]\")], device=self.device).repeat(batch_size)\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            # attn = [batch_size, src_len]\n",
    "            pred, hidden, attn = self.decoder(inp, hidden, encoder_outputs, src_mask)\n",
    "            \n",
    "            preds[:, t, :] = pred\n",
    "            top1 = pred.argmax(1)\n",
    "            inp = top1\n",
    "            \n",
    "            # store attention sequences in a tensor holding attention value for each token\n",
    "            attns[:, t, :] = attn\n",
    "            \n",
    "        return preds, attns, src_len\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # both are lists of encodings\n",
    "        src, trg = batch\n",
    "        \n",
    "        # y    = [batch_size, trg_len]\n",
    "        # pred = [batch_size, trg_len, output_dim]\n",
    "        y = torch.tensor([e.ids for e in trg], device=self.device)\n",
    "        preds = self._forward(src, trg)\n",
    "        output_dim = preds.shape[-1]\n",
    "        \n",
    "        # y    = [batch_size * (trg_len-1)]\n",
    "        # pred = [batch_size * (trg_len-1), output_dim]\n",
    "        y = y[:, 1:].reshape(-1)\n",
    "        preds = preds[:, 1:, :].reshape(-1, output_dim)\n",
    "        \n",
    "        loss = F.cross_entropy(preds, y, ignore_index=src_pad_idx)\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        perplexity = torch.exp(loss)\n",
    "        self.log(\"train_ppl\", perplexity)\n",
    "        \n",
    "        if self.global_step % 100 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        src, trg = batch\n",
    "        y = torch.tensor([e.ids for e in trg], device=self.device)\n",
    "        preds = self._forward(src, trg)\n",
    "        \n",
    "        output_dim = preds.shape[-1]\n",
    "        y = y[:, 1:].reshape(-1)\n",
    "        preds = preds[:, 1:, :].reshape(-1, output_dim)\n",
    "        \n",
    "        loss = F.cross_entropy(preds, y, ignore_index=src_pad_idx)\n",
    "        self.log(\"valid_loss\", loss, sync_dist=True)\n",
    "        \n",
    "        perplexity = torch.exp(loss)\n",
    "        self.log(\"valid_ppl\", perplexity, sync_dist=True)\n",
    "        \n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        src, trg = batch\n",
    "        preds, attn_matrix, real_src_len = self(src)\n",
    "        \n",
    "        # attn_matrix = [batch_size, trg_len, src_len]\n",
    "        # preds       = [batch_size, trg_len, output_dim]\n",
    "        #             = [batch_size, trg_len]             (argmax 2)\n",
    "        preds = preds.argmax(2)\n",
    "        \n",
    "        # convert `preds` tensor to list of real sentences (tokens)\n",
    "        # meaning to cut the sentence by [EOS] and remove the [PAD] tokens\n",
    "        \n",
    "        # eos_pos = dict(sentence_idx: first_pad_position)\n",
    "        #\n",
    "        # e.g., {0: 32, 2: 55} \n",
    "        # Meaning that we have 32 tokens (include [EOS]) in the first predicted sentence\n",
    "        # and `max_len` tokens (no [EOS]) in the second predicted setence\n",
    "        # and 55 tokens (include [EOS]) in the third predicted sentence\n",
    "        eos_pos = dict(reversed((preds == self.trg_tokenizer.token_to_id(\"[EOS]\")).nonzero().tolist()))\n",
    "    \n",
    "        pred_sentences, attn_matrices = [], []\n",
    "        for idx, (sentence, attention, src_len) in enumerate(zip(preds, attn_matrix, real_src_len)):\n",
    "            \n",
    "            # sentence  = [trg_len_with_pad]\n",
    "            #           = [real_trg_len]\n",
    "            pred_sentences.append(sentence[:eos_pos.get(idx)+1 if eos_pos.get(idx) else None])\n",
    "            \n",
    "            # attention = [trg_len_with_pad, src_len_with_pad]\n",
    "            #           = [real_trg_len, real_src_len]\n",
    "            attn_matrices.append(attention[:eos_pos.get(idx)+1 if eos_pos.get(idx) else None, :src_len])\n",
    "        \n",
    "        # source sentences for displaying attention matrix \n",
    "        src = [[token for token in e.tokens if token != \"[PAD]\"] for e in src]\n",
    "        \n",
    "        # target sentences for calculating BLEU scores\n",
    "        trg = [[token for token in e.tokens if token != \"[PAD]\"] for e in trg]\n",
    "        \n",
    "        return pred_sentences, attn_matrices, src, trg\n",
    "        \n",
    "    \n",
    "    def test_epoch_end(self, test_outputs):\n",
    "        outputs = []\n",
    "        for (pred_sent_list, attn_list, src_list, trg_list) in test_outputs:\n",
    "            for pred_sent, attn, src, trg in list(zip(pred_sent_list, attn_list, src_list, trg_list)):\n",
    "                pred_sent = list(map(self.trg_tokenizer.id_to_token, pred_sent))\n",
    "                outputs.append((pred_sent, attn, src, trg))\n",
    "        \n",
    "        # outputs = list of predictions of testsets, each has a tuple of (pred_sentence, attn_matrix, src_sentence, trg_sentence)\n",
    "        # pred_sentence = [trg_len]\n",
    "        # attn_matrix   = [trg_len, src_len]\n",
    "        # src_sentence  = [src_len]\n",
    "        # trg_sentence  = [trg_len]\n",
    "        self.test_outputs = outputs\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "    \n",
    "    \n",
    "    def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37866f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = pl.loggers.WandbLogger()\n",
    "\n",
    "model = Seq2SeqModel(\n",
    "    input_dim,\n",
    "    output_dim,\n",
    "    dm.src_tokenizer,\n",
    "    dm.trg_tokenizer,\n",
    "    config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94433d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 84,620,032 trainable parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2SeqModel(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(32000, 300)\n",
       "    (rnn): GRU(300, 512, batch_first=True, bidirectional=True)\n",
       "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
       "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "    )\n",
       "    (embedding): Embedding(32000, 300)\n",
       "    (rnn): GRU(1324, 512, batch_first=True)\n",
       "    (fc_out): Linear(in_features=1836, out_features=32000, bias=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0b2bf9",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "db2b7293",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = Path(\"checkpoints\")\n",
    "ckpt_name = \"jp2ch-valid_loss=4.153-bleu=0.3916.ckpt\"\n",
    "\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(dirpath=ckpt_dir,  # path for saving checkpoints\n",
    "                                          filename=\"jp2ch-{epoch}-{valid_loss:.3f}\",\n",
    "                                          monitor=\"valid_loss\",\n",
    "                                          mode=\"min\",\n",
    "                                          save_top_k=3,\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "176b9c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    fast_dev_run=False,\n",
    "    logger=wandb_logger,\n",
    "    gpus=1,\n",
    "    max_epochs=5,\n",
    "    gradient_clip_val=1,\n",
    "    precision=config[\"precision\"],\n",
    "    callbacks=[checkpoint],\n",
    "    resume_from_checkpoint=ckpt_dir / ckpt_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7758a8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | encoder | Encoder | 12.6 M\n",
      "1 | decoder | Decoder | 72.0 M\n",
      "------------------------------------\n",
      "84.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "84.6 M    Total params\n",
      "338.480   Total estimated model params size (MB)\n",
      "Restored states from the checkpoint file at checkpoints/jp2ch-epoch=0-valid_loss=4.684.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d68ca1a58745cca087b6bc0dddfa77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae98352",
   "metadata": {},
   "source": [
    "# Testing (BLEU Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2529a90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqModel.load_from_checkpoint(ckpt_dir / ckpt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b79b9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38dbba7017e43e7a28c20108adeb3fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Testing'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a85069dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_corpus_bleu(preds: List[str], refs: List[List[str]], n_gram=4):\n",
    "    # arg example:\n",
    "    # preds: [\"机器人行业在环境问题上的措施\", \"松下生产科技公司也以环境先进企业为目标\"]\n",
    "    # refs: [[\"机器人在环境上的改变\", \"對於机器人在环境上的措施\"],  [\"松下科技公司的首要目标是解决环境问题\"]]\n",
    "    preds = list(map(list, preds))\n",
    "    refs = [[list(sen) for sen in ref] for ref in refs]\n",
    "    return torchmetrics.functional.nlp.bleu_score(preds, refs, n_gram=n_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b50d660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3916)\n"
     ]
    }
   ],
   "source": [
    "preds = [dm.trg_tokenizer.decode(list(map(dm.trg_tokenizer.token_to_id, output[0]))) for output in model.test_outputs]\n",
    "refs = [[dm.trg_tokenizer.decode(list(map(dm.trg_tokenizer.token_to_id, output[3])))] for output in model.test_outputs]\n",
    "\n",
    "bleu_score = calculate_corpus_bleu(preds, refs)\n",
    "print(bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad33246a",
   "metadata": {},
   "source": [
    "# Case Study and Attention Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4c7f6344",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.sans-serif'] = ['Noto Sans CJK TC']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def case_study(pred_token, src_token, trg_token, attn_matrix):\n",
    "    \n",
    "    src  = dm.src_tokenizer.decode(list(map(dm.src_tokenizer.token_to_id, src_token)))\n",
    "    trg  = dm.trg_tokenizer.decode(list(map(dm.trg_tokenizer.token_to_id, trg_token)))\n",
    "    pred = dm.trg_tokenizer.decode(list(map(dm.trg_tokenizer.token_to_id, pred_token)))\n",
    "    \n",
    "    print(f\"SOURCE: \\n{src}\\n {'-'*100}\")\n",
    "    print(f\"TARGET: \\n{trg}\\n {'-'*100}\")\n",
    "    print(f\"PREDICTION: \\n{pred}\\n {'-'*100}\")\n",
    "    \n",
    "    print(f\"BLEU SCORE: {calculate_corpus_bleu([trg], [[pred]])}\")\n",
    "    \n",
    "    plt.figure(figsize=(30, 30))\n",
    "    ax = sns.heatmap(attn_matrix, xticklabels=src_token, yticklabels=pred_token)\n",
    "    ax.xaxis.set_ticks_position('top')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f625d01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: \n",
      "2000 年前後からは行政も社会の持続可能性と環境教育を結び付けて取り組むようになった 。\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "TARGET: \n",
      "大约从 2000 年起结合社会持续发展的可能性和环境教育 , 还积极地开展了行政管理 。\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "PREDICTION: \n",
      "2000 年前后 ,行政管理 , ,持续社会的可持续性的可持续发展教育 。\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "BLEU SCORE: 0.31928369402885437\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x2160 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i=31\n",
    "case_study(model.test_outputs[i][0],\n",
    "           model.test_outputs[i][2],\n",
    "           model.test_outputs[i][3],\n",
    "           model.test_outputs[i][1].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dab545-002d-4ae6-8a48-ef989707ad04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
