{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ff90e0",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12e399c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import built-in Python libs\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Import data science libs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import deep learning libs\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Import weights & bias\n",
    "import wandb\n",
    "\n",
    "# Import data preprocessing libs\n",
    "from tokenizers import Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "%matplotlib inline\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d2834b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_path = Path.cwd().parent / \"utils\"\n",
    "sys.path.append(str(utils_path))\n",
    "from custom_tokenizer import load_jieba_tokenizer, load_janome_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1939508",
   "metadata": {},
   "source": [
    "# Job Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70d51c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = 1  # 0 - sentencepiece, 1 - language specific\n",
    "\n",
    "job_name = [\"transformer_baseline_sentencepiece_ch2jp\", \"transformer_baseline_language_specific_ch2jp\"]\n",
    "\n",
    "tokenizer_job = [\"sentencepiece\", \"language_specific\"]\n",
    "ch_tokenizer_job = [\"ch_tokenizer.json\", \"jieba_tokenizer.json\"]\n",
    "jp_tokenizer_job = [\"jp_tokenizer.json\", \"janome_tokenizer.json\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c730c61f",
   "metadata": {},
   "source": [
    "# Config and WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61e9d2f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"enc_layers\": 3,\n",
    "    \"dec_layers\": 3,\n",
    "    \"enc_heads\": 6,\n",
    "    \"dec_heads\": 6,\n",
    "    \"enc_pf_dim\": 512,\n",
    "    \"dec_pf_dim\": 512,\n",
    "    \"enc_dropout\": 0.1,\n",
    "    \"dec_dropout\": 0.1,\n",
    "    \"hid_dim\": 300,\n",
    "    \"lr\": 5e-4,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_workers\": 1,\n",
    "    \"precision\": 32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c783a568",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">dainty-yogurt-265</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/windsuzu/phonetic-translation\" target=\"_blank\">https://wandb.ai/windsuzu/phonetic-translation</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/windsuzu/phonetic-translation/runs/3serofq0\" target=\"_blank\">https://wandb.ai/windsuzu/phonetic-translation/runs/3serofq0</a><br/>\n",
       "                Run data is saved locally in <code>/home/windsuzu/phonetics-in-chinese-japanese-machine-translation/experiments/main/ch2jp/wandb/run-20210512_092021-3serofq0</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project=\"phonetic-translation\",\n",
    "    entity=\"windsuzu\",\n",
    "    group=\"experiments\",\n",
    "    job_type=job_name[job],\n",
    "    config=config,\n",
    "    reinit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750521ee",
   "metadata": {},
   "source": [
    "# Download Datasets, Tokenizers, DataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c35ccfc",
   "metadata": {},
   "source": [
    "## Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c558224",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_art = run.use_artifact(\"sampled_train:latest\")\n",
    "train_data_dir = train_data_art.download()\n",
    "\n",
    "dev_data_art = run.use_artifact(\"dev:latest\")\n",
    "dev_data_dir = dev_data_art.download()\n",
    "\n",
    "test_data_art = run.use_artifact(\"test:latest\")\n",
    "test_data_dir = test_data_art.download()\n",
    "\n",
    "data_dir = {\n",
    "    \"train\": train_data_dir,\n",
    "    \"dev\": dev_data_dir,\n",
    "    \"test\": test_data_dir,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80567a3",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e685d07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer_art = run.use_artifact(f\"{tokenizer_job[job]}:latest\")\n",
    "tokenizer_dir = tokenizer_art.download()\n",
    "\n",
    "src_tokenizer_dir = Path(tokenizer_dir) / ch_tokenizer_job[job]\n",
    "trg_tokenizer_dir = Path(tokenizer_dir) / jp_tokenizer_job[job]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43098206",
   "metadata": {},
   "source": [
    "## DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0118754a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SentencePieceDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir,\n",
    "        src_tokenizer_dir,\n",
    "        trg_tokenizer_dir,\n",
    "        batch_size=128,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "        job=0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.src_tokenizer_dir = src_tokenizer_dir\n",
    "        self.trg_tokenizer_dir = trg_tokenizer_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "        self.job = job\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.src_tokenizer = self._load_tokenizer(self.src_tokenizer_dir)\n",
    "        self.trg_tokenizer = self._load_tokenizer(self.trg_tokenizer_dir)\n",
    "\n",
    "        if stage == \"fit\":\n",
    "            self.train_set = self._data_preprocess(self.data_dir[\"train\"])\n",
    "            self.val_set = self._data_preprocess(self.data_dir[\"dev\"])\n",
    "\n",
    "        if stage == \"test\":\n",
    "            self.test_set = self._data_preprocess(self.data_dir[\"test\"])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_set,\n",
    "            self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=self._data_batching_fn,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_set,\n",
    "            self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=self._data_batching_fn,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_set,\n",
    "            self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=self._data_batching_fn,\n",
    "        )\n",
    "\n",
    "    def _read_data_array(self, data_dir):\n",
    "        with open(data_dir, encoding=\"utf8\") as f:\n",
    "            arr = f.readlines()\n",
    "        return arr\n",
    "\n",
    "    def _load_tokenizer(self, tokenizer_dir):\n",
    "        if self.job == 0:\n",
    "            return Tokenizer.from_file(str(tokenizer_dir))\n",
    "        else:\n",
    "            return (\n",
    "                load_jieba_tokenizer(tokenizer_dir)\n",
    "                if \"jieba\" in str(tokenizer_dir)\n",
    "                else load_janome_tokenizer(tokenizer_dir)\n",
    "            )\n",
    "\n",
    "    def _data_preprocess(self, data_dir):\n",
    "        src_txt = self._read_data_array(Path(data_dir) / \"ch.txt\")\n",
    "        trg_txt = self._read_data_array(Path(data_dir) / \"jp.txt\")\n",
    "        parallel_txt = np.array(list(zip(src_txt, trg_txt)))\n",
    "        return parallel_txt\n",
    "\n",
    "    def _data_batching_fn(self, data_batch):\n",
    "        data_batch = np.array(data_batch)  # shape=(batch_size, 2=src+trg)\n",
    "\n",
    "        src_batch = data_batch[:, 0]  # shape=(batch_size, )\n",
    "        trg_batch = data_batch[:, 1]  # shape=(batch_size, )\n",
    "\n",
    "        # src_batch=(batch_size, longest_sentence)\n",
    "        # trg_batch=(batch_size, longest_sentence)\n",
    "        src_batch = self.src_tokenizer.encode_batch(src_batch)\n",
    "        trg_batch = self.trg_tokenizer.encode_batch(trg_batch)\n",
    "\n",
    "        # We have to sort the batch by their non-padded lengths in descending order,\n",
    "        # because the descending order can help in `nn.utils.rnn.pack_padded_sequence()`,\n",
    "        # which it will help us ignoring the <pad> in training rnn.\n",
    "        # https://meetonfriday.com/posts/4d6a906a\n",
    "        src_batch, trg_batch = zip(\n",
    "            *sorted(\n",
    "                zip(src_batch, trg_batch),\n",
    "                key=lambda x: sum(x[0].attention_mask),\n",
    "                reverse=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return src_batch, trg_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a30dffe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dm = SentencePieceDataModule(\n",
    "    data_dir,\n",
    "    src_tokenizer_dir,\n",
    "    trg_tokenizer_dir,\n",
    "    config[\"batch_size\"],\n",
    "    config[\"num_workers\"],\n",
    "    job=job\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94a719d",
   "metadata": {},
   "source": [
    "### Test DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b94a15fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dm.setup(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "303d9c50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000 32000\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "input_dim = dm.src_tokenizer.get_vocab_size()\n",
    "output_dim = dm.trg_tokenizer.get_vocab_size()\n",
    "print(input_dim, output_dim)\n",
    "\n",
    "src_pad_idx = dm.src_tokenizer.token_to_id(\"[PAD]\")\n",
    "print(src_pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8334efea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 Encoding(num_tokens=103, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]) ['[BOS]', '_日本', '从', '世界', '各国', '进口', '酱', '油', '等的', '各种', '调', '料', '_,', '_但', '公', '知', '的是在', '制造', '过程中', '致癌性', '物质的', '氯', '丙', '醇', '类', '作为', '副', '产品', '生成', '_3', '_-', '_氯', '_-', '_1', '_,', '_2', '_-', '_丙', '二醇', '_(', '_3', '_-', '_M', 'CP', 'D', '_)', '_、', '_1', '_,', '_3', '_-', '_二氯', '_-', '_2', '_-', '_丙', '醇', '_(', '_1', '_,', '_3', '_-', '_D', 'CP', '_)', '_、', '_2', '_-', '_氯', '_-', '_1', '_,', '_3', '_-', '_丙', '二醇', '_(', '_2', '_-', '_M', 'CP', 'D', '_)', '_以及', '_2', '_,', '_3', '_-', '_二氯', '_-', '_1', '_丙', '醇', '_(', '_2', '_,', '_3', '_-', '_D', 'CP', '_)', '_。', '[EOS]']\n",
      "\n",
      "64 Encoding(num_tokens=115, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]) ['[BOS]', '_日本', 'へ', 'は世界', '各', '国', 'から', '醤', '油', 'などの', '種々の', '調', '味', '料', 'が', '輸入', 'されているが', '_,', '_製造', '過程', '中で', '発癌', '性', '物質の', 'クロロ', 'プロ', 'パ', 'ノール', '類が', '副', '産物', 'として', '_3', '_−', '_クロロ', '_−', '_1', '_,', '_2', '_−', '_プロ', 'パン', 'ジオ', 'ール', '_(', '_3', '_−', '_M', 'CP', 'D', '_)', '_,', '_1', '_,', '_3', '_−', '_ジクロロ', '_−', '_2', '_−', '_プロ', 'パ', 'ノール', '_(', '_1', '_,', '_3', '_−', '_D', 'CP', '_)', '_,', '_2', '_−', '_クロロ', '_−', '_1', '_,', '_3', '_−', '_プロ', 'パン', 'ジオ', 'ール', '_(', '_2', '_−', '_M', 'CP', 'D', '_)', '_および', '_2', '_,', '_3', '_−', '_ジクロロ', '_−', '_1', '_プロ', 'パ', 'ノール', '_(', '_2', '_,', '_3', '_−', '_D', 'CP', '_)', '_が', '生成される', 'ことが知られている', '_。', '[EOS]']\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for src, trg in dm.test_dataloader():\n",
    "    print(len(src), src[i], src[i].tokens)\n",
    "    print()\n",
    "    print(len(trg), trg[i], trg[i].tokens)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b14cf5",
   "metadata": {},
   "source": [
    "# Build Lightning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac7d02e",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef358e20",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](../../assets/transformer-encoder.png)\n",
    "\n",
    "若輸入有 5 個 token: $X = (x_1, x_2, x_3, x_4, x_5)$，那麼經過整個 `encoder` 就會得到 $Z = (z_1, z_2, z_3, z_4, z_5)$，這 5 個分別都是 `context vectors`，他們不像在 RNN 裡被叫做 `hidden state`，是因為他們已經讀過所有的 token 資訊，而不像在 RNN 裡，時間 `t` 時只讀過 $x_t$ 之前的資訊。\n",
    "\n",
    "---\n",
    "\n",
    "首先 token 一樣會經過一般的 `word embedding`，並透過另一個 `positional embedding` 來獲取 token 位置的資訊，這個 positional embedding 的大小等於句子的長度，若今天我們想解決的句子長度為 100，那麼 `positional embedding` 的大小 (`vocab_size`) 就會等於 100。\n",
    "\n",
    "在 `Attention is all you need` 裡面是使用固定的 embedding，而較新的 transformer 架構都會採用可以被訓練的 `positional embedding`；所以這邊我們使用後者。\n",
    "\n",
    "`word embedding` 會跟 `positional embedding` 相加，得到文字和位置的資訊。但在相加之前，`word embedding` 會先乘上一個 scaling factor $\\sqrt{d_\\text{model}}$ (`hid_dim`)，用來降低 `embedding` 的差異性 (variance)。\n",
    "\n",
    "合成後的 `embedding` 就會經過 $N$ 個 encoder layer 來取得 $Z$ (`context vectors`)，輸出給 decoder 做預測。\n",
    "\n",
    "---\n",
    "\n",
    "這邊的 `src_mask` 和在 RNN 的一樣，沒有 `<pad>` 的地方是 1，有 `<pad>` 的地方是 0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d91aeea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, max_len=250):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_len, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(hid_dim, n_heads, pf_dim, dropout) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.register_buffer(\"scale\", torch.sqrt(torch.FloatTensor([hid_dim])))\n",
    "\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        # src      = [batch_size, src_len]\n",
    "        # src_mask = [batch_size, 1, 1, src_len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).type_as(src)\n",
    "        # pos = [batch_size, src_len]\n",
    "        \n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        # src = [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        # src = [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4daa73f",
   "metadata": {},
   "source": [
    "## Encoder Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5943909f",
   "metadata": {},
   "source": [
    "在每個 encoder layer (不共享參數) 當中，分別發生了一些事件，可以細分成四個:\n",
    "\n",
    "1. 將 `src` 和 `src_mask` 丟入 multi-head attention layer 中，得到的結果 `_src` 做 dropout\n",
    "2. 和 `_src` 和 `src` 做 residual connection，然後丟到 layer normalization layer 得到新的 `src`\n",
    "3. 將新的 `src` 丟到 position-wise feedforward layer 得到 `_src` 並且在做一次 dropout\n",
    "4. `_src` 和 `src` 重複步驟二，做 residual connection 和 layer normalization，得到最終的 `src`\n",
    "\n",
    "---\n",
    "\n",
    "因為在 transformer 中的 attention 都是原句 (source sentence) 和自己本身計算出來的，而不是參考別的句子計算而成，所以又稱為 `self-attention`。\n",
    "\n",
    "---\n",
    "\n",
    "裡面執行兩次的 layer nomarlization，其目的是要將所有參數一般化為 mean=0 和 std=1，能夠讓擁有大量參數的網路更容易訓練。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf7b2143",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        # src     = [batch_size, src_len, hid_dim]\n",
    "        # src_mask = [batch_size, 1, 1, src_len]\n",
    "        \n",
    "        # self attention\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        \n",
    "        # dropout, residual connection, layer norm\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        # src = [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        \n",
    "        # positionwise feedforward\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "        \n",
    "        # dropout, residual connection, layer norm\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        # src = [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddda7b4",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125351b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"../../assets/transformer-attention.png\" width=500>\n",
    "\n",
    "Attention 可以被視為 `query`, `key`, `value`, 其中 query 和 key 作用 (dot-product) 得到 `attention vector`，再拿來和 value 相乘獲得 weighted sum（上圖左）。在 transformer 中會將 dot-product 後的結果先除以 $\\sqrt{d_k}$，$d_k$ 是 `head_dim` (`hid_dim // n_heads`)，原因是避免乘積越來越大，讓梯度越來越小。\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Transformer 進一步將 Q, K, V 透過 `hid_dim` 拆成 `h` 個 heads，不但能平行運算 `h` 個 head，還能夠讓每一個 head 學到不同的 attention concepts。最後再將 `h` 個 head 重新組合回 `hid_dim` 的 shape 即可。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{head}_i &= \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\\\\n",
    "\\text{MultiHeadAttention}(Q, K, V) &= \\text{Concat}(\\text{head}_1, \\cdots, \\text{head}_h)W^O\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "上面的 $W^Q, W^K, W^V$，分別是 `fc_q`, `fc_k`, `fc_v`，是將 Q, K, V 切成 `h` 個 head 前的 linear layer；而 $W^O$ 是最終輸出前的 linear layer `fc_o`。\n",
    "\n",
    "---\n",
    "\n",
    "整個 attention 運算過程:\n",
    "\n",
    "1. 透過 `fc_q`, `fc_k`, `fc_v` 計算 $QW^Q, KW^K, VW^V$ 得到 Q, K, V\n",
    "2. 將 Q, K, V 透過 `hid_dim` 拆成 `n_heads` 個，每個擁有維度 `head_dim`，並且 `permute` 成正確的順序，使他們可以相乘\n",
    "3. 計算 `energy` = Q * V / sqrt(`head_dim`), 其中 `head_dim = hid_dim // n_heads`\n",
    "4. 接著 mask 算好的 `energy`，移除對 `<pad>` 的 attention\n",
    "5. 對 `energy` 做 softmax 得到 `attention_vector`\n",
    "6. 對 `attention_vector` 做 dropout 後和 V 相乘獲得 `weighted_sum`\n",
    "7. 將 `n_heads` 個 `weighted_sum` 合併回一個\n",
    "8. 與 $W^O$ (fc_o) 相乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46674758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert hid_dim % n_heads == 0\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        \n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.register_buffer(\"scale\", torch.sqrt(torch.FloatTensor([hid_dim])))\n",
    "\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "\n",
    "        # query = [batch_size, query_len, hid_dim]\n",
    "        # key = [batch_size, key_len, hid_dim]\n",
    "        # value = [batch_size, value_len, hid_dim]\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        \n",
    "        # Q = [batch_size, query_len, hid_dim]\n",
    "        # K = [batch_size, key_len, hid_dim]\n",
    "        # V = [batch_size, value_len, hid_dim]\n",
    "        \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        # Q = [batch_size, n_heads, query_len, head_dim]\n",
    "        # K = [batch_size, n_heads, key_len, head_dim]\n",
    "        # V = [batch_size, n_heads, value_len, head_dim]\n",
    "        \n",
    "\n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        # energy = [batch_size, n_heads, query_len, key_len]\n",
    "        \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "        # attention = [batch_size, n_heads, query_len, key_len]\n",
    "        \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        # x = [batch_size, n_heads, query_len, head_dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        # x = [batch_size, query_len, n_heads, head_dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        # x = [batch_size, query_len, hid_dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        # x = [batch_size, query_len, hid_dim]\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f210ab",
   "metadata": {},
   "source": [
    "## Position-wise Feedforward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b15a200",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = [batch_size, seq_len, hid_dim]\n",
    "        \n",
    "        \n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        # x = [batch_size, seq_len, pf_dim]\n",
    "        \n",
    "        \n",
    "        x = self.fc_2(x)\n",
    "        # x = [batch_size, seq_len, hid_dim]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e873b2",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8d3733",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](../../assets/transformer-decoder.png)\n",
    "\n",
    "Decoder 將使用 source context vector (representation) $Z$，來預測下一個 token $\\hat{y}$。然後從答案 $y$ 計算 loss，用來更新所有網路參數權重。\n",
    "\n",
    "---\n",
    "\n",
    "Decoder 和 encoder 十分相似，只有些許地方不同。首先，多了一個 `masked multi-head attention layer`，在這個 layer 裡面，目標句子 (trg sentence) 會和自己做 `self-attention`，並且會加入更特別的 mask，防止 decoder 預先看到未來要預測的單字。\n",
    "\n",
    "第二，在第二層的 `multi-head attention layer` 是由 decoder context vector (representation) 當作 `query`，而 encoder context vector (representation) 當作 `key` 和 `value`。\n",
    "\n",
    "---\n",
    "\n",
    "Encoder 和 Decoder 裡面的層數不用一致。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "032e0d58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, output_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, max_len=250\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_len, hid_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [DecoderLayer(hid_dim, n_heads, pf_dim, dropout) for _ in range(n_layers)]\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.register_buffer(\"scale\", torch.sqrt(torch.FloatTensor([hid_dim])))\n",
    "        \n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        # trg = [batch_size, trg_len]\n",
    "        # enc_src = [batch_size, src_len, hid_dim]\n",
    "        # trg_mask = [batch_size, 1, trg_len, trg_len]\n",
    "        # src_mask = [batch_size, 1, 1, src_len]\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).type_as(trg)\n",
    "        # pos = [batch_size, trg_len]\n",
    "        \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "        # trg = [batch_size, trg_len, hid_dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        # trg = [batch_size, trg_len, hid_dim]\n",
    "        # attention = [batch_size, n heads, trg_len, src_len]\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        # output = [batch_size, trg_len, output_dim]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5046d1",
   "metadata": {},
   "source": [
    "## Decoder Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d32467",
   "metadata": {},
   "source": [
    "Decoder layer 的第一層 trg 和自己的 `self-attention`，也就是自己作為 Q, K, V 來計算 attention，裡面使用 `trg_mask` 來防止計算 attention 在 `<pad>` 或之後的答案上。\n",
    "\n",
    "---\n",
    "\n",
    "Decoder layer 的第二層是 trg 和 encoder context vector (representation) 的 `multi-head attention`，使用 trg 作為 `query`，而 encoded src 作為 `key` 和 `value`，這邊使用 `src_mask` 防止計算 attention 在 src 裡的 `<pad>` 上。\n",
    "\n",
    "---\n",
    "\n",
    "Decoder layer 的第三層是一個 position-wise feedforward layer，沒什麼特別的。\n",
    "\n",
    "---\n",
    "\n",
    "整個 decoder layer 的流程是:\n",
    "\n",
    "1. trg -> `self-attention` -> _trg -> `dropout` -> _trg\n",
    "2. `residual`(trg + _trg) -> `layer-norm` -> trg\n",
    "3. trg + src -> `multi-head attention` -> _trg -> `dropout` -> _trg\n",
    "4. `residual`(trg + _trg) -> `layer-norm` -> trg\n",
    "5. trg -> `positionwise-feedforward` -> _trg -> `dropout` -> _trg\n",
    "6. `residual`(trg + _trg) -> `layer-norm` -> trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "810a9df0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        \n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        # trg = [batch_size, trg_len, hid_dim]\n",
    "        # enc_src = [batch_size, src_len, hid_dim]\n",
    "        # trg_mask = [batch_size, 1, trg_len, trg_len]\n",
    "        # src_mask = [batch_size, 1, 1, src_len]\n",
    "        \n",
    "        # self attention\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        # dropout, residual connection and layer norm\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "         \n",
    "        # trg = [batch_size, trg_len, hid_dim]\n",
    "        # ====================================\n",
    "        \n",
    "        # encoder attention\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        # dropout, residual connection and layer norm\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        \n",
    "        # trg = [batch_size, trg_len, hid_dim]\n",
    "        # ====================================\n",
    "        \n",
    "        \n",
    "        # positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "        \n",
    "        # dropout, residual and layer norm\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        \n",
    "        # trg = [batch_size, trg_len, hid_dim]\n",
    "        # ====================================\n",
    "        \n",
    "        # attention = [batch_size, n_heads, trg_len, src_len]\n",
    "        \n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1421a489",
   "metadata": {},
   "source": [
    "## Full Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "961eb583",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Seq2SeqModel(pl.LightningModule):\n",
    "    def __init__(self, input_dim, output_dim, trg_tokenizer, config):\n",
    "        super().__init__()\n",
    "        self.trg_tokenizer = trg_tokenizer\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            input_dim,\n",
    "            config[\"hid_dim\"],\n",
    "            config[\"enc_layers\"],\n",
    "            config[\"enc_heads\"],\n",
    "            config[\"enc_pf_dim\"],\n",
    "            config[\"enc_dropout\"],\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            output_dim,\n",
    "            config[\"hid_dim\"],\n",
    "            config[\"dec_layers\"],\n",
    "            config[\"dec_heads\"],\n",
    "            config[\"dec_pf_dim\"],\n",
    "            config[\"dec_dropout\"],\n",
    "        )\n",
    "\n",
    "        self.lr = config[\"lr\"]\n",
    "        self.apply(self.initialize_weights)\n",
    "    \n",
    "    \n",
    "    def initialize_weights(self, m):\n",
    "        if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "            nn.init.xavier_uniform_(m.weight.data)\n",
    "    \n",
    "    \n",
    "    # Training\n",
    "    # Use only when training and validation\n",
    "    def _forward(self, src, trg):\n",
    "        # src = list of Encoding([ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
    "        # trg = list of Encoding([ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
    "\n",
    "        # src_batch = [batch_size, src_len]\n",
    "        # src_mask  = [batch_size, 1, 1, src_len]\n",
    "        src_batch = torch.tensor([e.ids for e in src], device=self.device)\n",
    "        src_mask = torch.tensor([e.attention_mask for e in src], device=self.device).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        # trg_batch = [batch_size, trg_len-1]\n",
    "        # trg_mask  = [batch_size, 1, trg_len-1, trg_len-1]\n",
    "        trg_batch = torch.tensor([e.ids[:-1] for e in trg], device=self.device)\n",
    "        trg_pad_mask = torch.tensor([e.attention_mask[:-1] for e in trg], device=self.device).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        trg_len = trg_batch.shape[1]\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        \n",
    "        # enc_src = [batch_size, src_len, hid_dim]\n",
    "        enc_src = self.encoder(src_batch, src_mask)\n",
    "        \n",
    "        # output = [batch_size, trg_len, output_dim]\n",
    "        # attention = [batch_size, n_heads, trg_len, src_len]\n",
    "        output, attention = self.decoder(trg_batch, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        return output, attention\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # both are lists of encodings\n",
    "        src, trg = batch\n",
    "        \n",
    "        y = torch.tensor([e.ids for e in trg], device=self.device)\n",
    "        preds, _ = self._forward(src, trg)\n",
    "        # y    = [batch_size, trg_len]\n",
    "        # pred = [batch_size, trg_len-1, output_dim]\n",
    "        \n",
    "        output_dim = preds.shape[-1]\n",
    "        \n",
    "        # y    = [batch_size * (trg_len-1)]\n",
    "        # pred = [batch_size * (trg_len-1), output_dim]\n",
    "        y = y[:, 1:].contiguous().view(-1)\n",
    "        preds = preds.contiguous().view(-1, output_dim)\n",
    "        \n",
    "        loss = F.cross_entropy(preds, y, ignore_index=self.trg_tokenizer.token_to_id(\"[PAD]\"))\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        perplexity = torch.exp(loss)\n",
    "        self.log(\"train_ppl\", perplexity)\n",
    "        \n",
    "        if self.global_step % 50 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        src, trg = batch\n",
    "        y = torch.tensor([e.ids for e in trg], device=self.device)\n",
    "        preds, _ = self._forward(src, trg)\n",
    "        \n",
    "        output_dim = preds.shape[-1]\n",
    "        y = y[:, 1:].contiguous().view(-1)\n",
    "        preds = preds.contiguous().view(-1, output_dim)\n",
    "        \n",
    "        loss = F.cross_entropy(preds, y, ignore_index=self.trg_tokenizer.token_to_id(\"[PAD]\"))\n",
    "        self.log(\"valid_loss\", loss, sync_dist=True)\n",
    "        \n",
    "        perplexity = torch.exp(loss)\n",
    "        self.log(\"valid_ppl\", perplexity, sync_dist=True)\n",
    "        \n",
    "    \n",
    "    # Inference\n",
    "    # * Let you use the pl model as a pytorch model.\n",
    "    # * \n",
    "    # * pl_model.eval()\n",
    "    # * pl_model(X)\n",
    "    # *\n",
    "    def forward(self, src, max_len=150):\n",
    "        # src_batch    = [batch_size, src_len]\n",
    "        # src_mask     = [batch_size, 1, 1, src_len]\n",
    "        # real_src_len = [batch_size] (src_len without <pad>)\n",
    "        src_batch = torch.tensor([e.ids for e in src], device=self.device)\n",
    "        src_mask = torch.tensor([e.attention_mask for e in src], device=self.device).unsqueeze(1).unsqueeze(2)\n",
    "        batch_size = src_batch.shape[0]\n",
    "        \n",
    "        # first input to the decoder = [BOS] tokens\n",
    "        # trg_batch = [batch_size, 1]\n",
    "        trg_batch = torch.tensor([self.trg_tokenizer.token_to_id(\"[BOS]\")], device=self.device).repeat(batch_size).unsqueeze(1)\n",
    "        \n",
    "        # enc_src = [batch_size, src_len, hid_dim]\n",
    "        enc_src = self.encoder(src_batch, src_mask)\n",
    "        \n",
    "        for _ in range(1, max_len):\n",
    "            trg_pad_mask = (trg_batch != self.trg_tokenizer.token_to_id(\"[PAD]\")).unsqueeze(1).unsqueeze(2)\n",
    "            trg_sub_mask = torch.tril(torch.ones((trg_batch.shape[1], trg_batch.shape[1]), device = self.device)).bool()\n",
    "            trg_mask = trg_pad_mask & trg_sub_mask\n",
    "            # trg_mask = [batch_size, 1, trg_len, trg_len]\n",
    "            \n",
    "            # output = [batch_size, trg_len, output_dim]\n",
    "            # attention = [batch_size, n_heads, trg_len, src_len]\n",
    "            output, attention = self.decoder(trg_batch, enc_src, trg_mask, src_mask)\n",
    "            pred = output.argmax(2)[:, -1].unsqueeze(1)\n",
    "            \n",
    "            trg_batch = torch.cat([trg_batch, pred], dim=1)\n",
    "\n",
    "        real_src_len = torch.sum(src_mask, axis=3).view(-1)\n",
    "\n",
    "        return trg_batch, attention, real_src_len      \n",
    "\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        src, trg = batch\n",
    "        \n",
    "        output, attn_matrix = self._forward(src, trg)\n",
    "        preds = output.argmax(2)\n",
    "        src_mask = torch.tensor([e.attention_mask for e in src], device=self.device)\n",
    "        real_src_len = torch.sum(src_mask, axis=1).view(-1)\n",
    "    \n",
    "        # attn_matrix  = [batch_size, n_heads, trg_len, src_len]\n",
    "        # preds        = [batch_size, trg_len]\n",
    "        # real_src_len = [batch_size]\n",
    "        \n",
    "        # convert `preds` tensor to list of real sentences (tokens)\n",
    "        # meaning to cut the sentence by [EOS] and remove the [PAD] tokens\n",
    "        \n",
    "        # eos_pos = dict(sentence_idx: first_pad_position)\n",
    "        #\n",
    "        # e.g., {0: 32, 2: 55} \n",
    "        # Meaning that we have 32 tokens (include [EOS]) in the first predicted sentence\n",
    "        # and `max_len` tokens (no [EOS]) in the second predicted setence\n",
    "        # and 55 tokens (include [EOS]) in the third predicted sentence\n",
    "        eos_pos = dict(reversed((preds == self.trg_tokenizer.token_to_id(\"[EOS]\")).nonzero().tolist()))\n",
    "    \n",
    "        pred_sentences, attn_matrices = [], []\n",
    "        for idx, (sentence, attention, src_len) in enumerate(zip(preds, attn_matrix, real_src_len)):\n",
    "            \n",
    "            # sentence  = [trg_len_with_pad]\n",
    "            #           = [real_trg_len]\n",
    "            pred_sentences.append(sentence[:eos_pos.get(idx)+1 if eos_pos.get(idx) else None])\n",
    "            \n",
    "            # attention = [n_heads, trg_len_with_pad, src_len_with_pad]\n",
    "            #           = [n_heads, real_trg_len, real_src_len]\n",
    "            attn_matrices.append(attention[:, :eos_pos.get(idx)+1 if eos_pos.get(idx) else None, :src_len])\n",
    "        \n",
    "        # source sentences for displaying attention matrix \n",
    "        src = [[token for token in e.tokens if token != \"[PAD]\"] for e in src]\n",
    "        \n",
    "        # target sentences for calculating BLEU scores\n",
    "        trg = [[token for token in e.tokens if token != \"[PAD]\"] for e in trg]\n",
    "        \n",
    "        return pred_sentences, attn_matrices, src, trg\n",
    "        \n",
    "    \n",
    "    def test_epoch_end(self, test_outputs):\n",
    "        outputs = []\n",
    "        for (pred_sent_list, attn_list, src_list, trg_list) in test_outputs:\n",
    "            for pred_sent, attn, src, trg in list(zip(pred_sent_list, attn_list, src_list, trg_list)):\n",
    "                pred_sent = list(map(self.trg_tokenizer.id_to_token, pred_sent))\n",
    "                outputs.append((pred_sent, attn, src, trg))\n",
    "        \n",
    "        # outputs = list of predictions of testsets, each has a tuple of (pred_sentence, attn_matrix, src_sentence, trg_sentence)\n",
    "        # pred_sentence = [trg_len]\n",
    "        # attn_matrix   = [n_heads, trg_len, src_len]\n",
    "        # src_sentence  = [src_len]\n",
    "        # trg_sentence  = [trg_len]\n",
    "        self.test_outputs = outputs\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "    \n",
    "    \n",
    "    def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d00f81a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb_logger = pl.loggers.WandbLogger()\n",
    "\n",
    "model = Seq2SeqModel(\n",
    "    input_dim,\n",
    "    output_dim,\n",
    "    dm.trg_tokenizer,\n",
    "    config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8bb3cb3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 34,089,872 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acc6526",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "992d2cb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/windsuzu/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: Checkpoint directory checkpoints exists and is not empty.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = Path(\"checkpoints\")\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(dirpath=ckpt_dir,  # path for saving checkpoints\n",
    "                                          filename=f\"{job_name[job]}-\" + \"{epoch}-{valid_loss:.3f}\",\n",
    "                                          monitor=\"valid_loss\",\n",
    "                                          mode=\"min\",\n",
    "                                          save_weights_only=True,\n",
    "                                          save_top_k=5,\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39c46e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    fast_dev_run=False,\n",
    "    logger=wandb_logger,\n",
    "    gpus=1,\n",
    "    max_epochs=7,\n",
    "    gradient_clip_val=1,\n",
    "    precision=config[\"precision\"],\n",
    "    callbacks=[checkpoint],\n",
    "#     resume_from_checkpoint=ckpt_dir / \"transformer_baseline_language_specific_ch2jp-epoch=4-valid_loss=4.131.ckpt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c8480e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | encoder | Encoder | 11.7 M\n",
      "1 | decoder | Decoder | 22.4 M\n",
      "------------------------------------\n",
      "34.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "34.1 M    Total params\n",
      "136.359   Total estimated model params size (MB)\n",
      "/home/windsuzu/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/windsuzu/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0292512bde4bfbba6750891e672972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd799ac",
   "metadata": {},
   "source": [
    "# Testing (BLEU Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee51cda3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(ckpt_dir/\"transformer_baseline_sentencepiece_ch2jp-epoch=3-valid_loss=5.206.ckpt\")[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bbcbbc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6676aaf78b9408cae92199475413427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Testing'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aec2644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_corpus_bleu(preds: List[str], refs: List[List[str]], n_gram=4):\n",
    "    # arg example:\n",
    "    # preds: [\"机器人行业在环境问题上的措施\", \"松下生产科技公司也以环境先进企业为目标\"]\n",
    "    # refs: [[\"机器人在环境上的改变\", \"對於机器人在环境上的措施\"],  [\"松下科技公司的首要目标是解决环境问题\"]]\n",
    "    preds = list(map(list, preds))\n",
    "    refs = [[list(sen) for sen in ref] for ref in refs]\n",
    "    return torchmetrics.functional.nlp.bleu_score(preds, refs, n_gram=n_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1bcbdb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2036)\n"
     ]
    }
   ],
   "source": [
    "preds = [dm.trg_tokenizer.decode(list(map(dm.trg_tokenizer.token_to_id, output[0]))) for output in model.test_outputs]\n",
    "refs = [[dm.trg_tokenizer.decode(list(map(dm.trg_tokenizer.token_to_id, output[3])))] for output in model.test_outputs]\n",
    "\n",
    "bleu_score = calculate_corpus_bleu(preds, refs, n_gram=4)\n",
    "print(bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60f9139",
   "metadata": {},
   "source": [
    "# Case Study and Attention Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ac16d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.sans-serif'] = ['Noto Sans CJK TC']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def case_study(pred_token, src_token, trg_token, attn_matrix):\n",
    "    \n",
    "    src  = dm.src_tokenizer.decode(list(map(dm.src_tokenizer.token_to_id, src_token)))\n",
    "    trg  = dm.trg_tokenizer.decode(list(map(dm.trg_tokenizer.token_to_id, trg_token)))\n",
    "    pred = dm.trg_tokenizer.decode(list(map(dm.trg_tokenizer.token_to_id, pred_token)))\n",
    "    \n",
    "    print(f\"SOURCE: \\n{src}\\n {'-'*100}\")\n",
    "    print(f\"TARGET: \\n{trg}\\n {'-'*100}\")\n",
    "    print(f\"PREDICTION: \\n{pred}\\n {'-'*100}\")\n",
    "    \n",
    "    print(f\"BLEU SCORE: {calculate_corpus_bleu([trg], [[pred]])}\")\n",
    "    \n",
    "#     plt.figure(figsize=(30, 30))\n",
    "#     for i in range(6):\n",
    "#         plt.subplot(3, 2, i+1)\n",
    "#         ax = sns.heatmap(attn_matrix[i], xticklabels=src_token, yticklabels=pred_token)\n",
    "#         ax.xaxis.set_ticks_position('top')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60c6f119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: \n",
      "日本从世界各国进口酱油等的各种调料,但公知的是在制造过程中致癌性物质的氯丙醇类作为副产品生成3-氯-1,2-丙二醇(3-MCPD)、1,3-二氯-2-丙醇(1,3-DCP)、2-氯-1,3-丙二醇(2-MCPD)以及2,3-二氯-1丙醇(2,3-DCP)。\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "TARGET: \n",
      "日本へは世界各国から醤油などの種々の調味料が輸入されているが,製造過程中で発癌性物質のクロロプロパノール類が副産物として3−クロロ−1,2−プロパンジオール(3−MCPD),1,3−ジクロロ−2−プロパノール(1,3−DCP),2−クロロ−1,3−プロパンジオール(2−MCPD)および2,3−ジクロロ−1プロパノール(2,3−DCP)が生成されることが知られている。\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "PREDICTION: \n",
      "日本におけるの,各国のののの各種の鉄−(2−れ,いるが,2過程−の−−−−2エタン−11−1−1−−1,−塩素−3−−SO−1−−塩素−塩素−2塩素2−2)SO−塩素−DCP)エタン−2−−2−塩素−DCP−−SO−DCP−−−メチル−塩素−DCP塩素DCP−)SO−塩素−DCP)−3)れる。で3れたいる。\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "BLEU SCORE: 0.10589826107025146\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "case_study(model.test_outputs[i][0],\n",
    "           model.test_outputs[i][2],\n",
    "           model.test_outputs[i][3],\n",
    "           model.test_outputs[i][1].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5f84d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "910af126f78e4f70975a50f5d0344a29878143e0b01cc32c99ca6cf65dbefcc1"
   }
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
